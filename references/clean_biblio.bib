@comment{{This file has been generated by bib2bib 1.98}}

@comment{{Command line: bib2bib --remove archiveprefix}}

@comment{{This file has been generated by bib2bib 1.98}}

@comment{{Command line: bib2bib --remove doi}}

@comment{{This file has been generated by bib2bib 1.98}}

@comment{{Command line: bib2bib --remove url}}

@comment{{This file has been generated by bib2bib 1.98}}

@comment{{Command line: bib2bib --remove http}}

@comment{{This file has been generated by bib2bib 1.98}}

@comment{{Command line: bib2bib --remove pdf biblio.bib}}

@incollection{Salimbeni17,
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  booktitle = {Advances in Neural Information Processing Systems 30},
  citeulike-article-id = {14629538},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  keywords = {deep\_learning},
  pages = {4588--4599},
  posted-at = {2018-08-27 11:09:16},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Doubly Stochastic Variational Inference for Deep Gaussian Processes}},
  year = {2017}
}

@inproceedings{Tran19,
  title = {{Calibrating Deep Convolutional Gaussian Processes}},
  author = {Tran, Gia-Lac and Bonilla, Edwin V. and Cunningham, John and Michiardi, Pietro and Filippone, Maurizio},
  booktitle = {Proceedings of Machine Learning Research},
  pages = {1554--1563},
  year = {2019},
  editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = {89},
  series = {Proceedings of Machine Learning Research},
  address = {},
  month = {16--18 Apr},
  publisher = {PMLR},
  abstract = {The wide adoption of Convolutional Neural Networks CNNs in applications where decision-making under uncertainty is fundamental, has brought a great deal of attention to the ability of these models to accurately quantify the uncertainty in their predictions. Previous work on combining CNNs with Gaussian processes GPs has been developed under the assumption that the predictive probabilities of these models are well-calibrated. In this paper we show that, in fact, current combinations of CNNs and GPs are miscalibrated.  We proposes a novel combination that considerably outperforms previous approaches on this aspect, while achieving state-of-the-art performance on image classification tasks.}
}

@misc{Laumann18,
  abstract = {{We propose a Bayesian convolutional neural network built upon Bayes by
Backprop and elaborate how this known method can serve as the fundamental
construct of our novel, reliable variational inference method for convolutional
neural networks. First, we show how Bayes by Backprop can be applied to
convolutional layers where weights in filters have probability distributions
instead of point-estimates; and second, how our proposed framework leads with
various network architectures to performances comparable to convolutional
neural networks with point-estimates weights. In the past, Bayes by Backprop
has been successfully utilised in feedforward and recurrent neural networks,
but not in convolutional ones. This work symbolises the extension of the group
of Bayesian neural networks which encompasses all three aforementioned types of
network architectures now.}},
  author = {Laumann, Felix and Shridhar, Kumar and Maurin, Adrian L.},
  citeulike-article-id = {14629536},
  citeulike-linkout-0 = {http://arxiv.org/abs/1806.05978},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1806.05978},
  day = {27},
  eprint = {1806.05978},
  keywords = {deep\_learning},
  month = jun,
  note = {arXiv:1806.05978},
  posted-at = {2018-08-27 10:58:35},
  priority = {2},
  title = {{Bayesian Convolutional Neural Networks}},
  year = {2018}
}

@article{Domingues18,
  author = {Domingues, Remi and Michiardi, Pietro and Zouaoui, Jihane and Filippone, Maurizio},
  citeulike-article-id = {14629535},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10994-018-5723-3},
  citeulike-linkout-1 = {https://doi.org/10.1007/s10994-018-5723-3},
  journal = {Machine Learning},
  keywords = {deep\_learning},
  number = {8-10},
  pages = {1363--1383},
  posted-at = {2018-08-27 10:56:55},
  priority = {2},
  title = {Deep {G}aussian Process autoencoders for novelty detection},
  volume = {107},
  year = {2018}
}

@misc{Kumar18,
  abstract = {{Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative
to standard parametric deep learning models. A DGP is formed by stacking
multiple GPs resulting in a well-regularized composition of functions. The
Bayesian framework that equips the model with attractive properties, such as
implicit capacity control and predictive uncertainty, makes it at the same time
challenging to combine with a convolutional structure. This has hindered the
application of DGPs in computer vision tasks, an area where deep parametric
models (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such
as radial basis functions (RBFs) are insufficient for handling pixel
variability in raw images. In this paper, we build on the recent convolutional
GP to develop Convolutional DGP (CDGP) models which effectively capture image
level features through the use of convolution kernels, therefore opening up the
way for applying DGPs to computer vision tasks. Our model learns local spatial
influence and outperforms strong GP based baselines on multi-class image
classification. We also consider various constructions of convolution kernel
over the image patches, analyze the computational trade-offs and provide an
efficient framework for convolutional DGP models. The experimental results on
image data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate
the effectiveness of the proposed approaches.}},
  author = {Kumar, Vinayak and Singh, Vaibhav and Srijith, P. K. and Damianou, Andreas},
  citeulike-article-id = {14629534},
  citeulike-linkout-0 = {http://arxiv.org/abs/1806.01655},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1806.01655},
  day = {5},
  eprint = {1806.01655},
  keywords = {deep\_learning, gaussian\_processes},
  month = jun,
  note = {arXiv:1806.01655},
  posted-at = {2018-08-27 10:54:32},
  priority = {2},
  title = {{Deep Gaussian Processes with Convolutional Kernels}},
  year = {2018}
}

@inproceedings{Garriga-Alonso18,
  title = {{Deep Convolutional Networks as shallow Gaussian Processes}},
  author = {Adrià Garriga-Alonso and Carl Edward Rasmussen and Laurence Aitchison},
  booktitle = {International Conference on Learning Representations},
  year = {2019}
}

@inproceedings{Hoffman17,
  abstract = {{Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC's additional computational overhead proves to be significant, but not prohibitive.}},
  address = {International Convention Centre, Sydney, Australia},
  author = {Hoffman, Matthew D.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  citeulike-article-id = {14629530},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v70/hoffman17a.html},
  editor = {Precup, Doina and Teh, Yee W.},
  keywords = {deep\_learning},
  month = aug,
  pages = {1510--1519},
  posted-at = {2018-08-27 10:45:40},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Deep Latent {G}aussian Models with {M}arkov Chain {M}onte {C}arlo},
  volume = {70},
  year = {2017}
}

@incollection{Havasi18,
  title = {{Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo}},
  author = {Havasi, Marton and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Murillo-Fuentes, Juan Jos\'{e}},
  booktitle = {Advances in Neural Information Processing Systems 31},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages = {7506--7516},
  year = {2018},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{Matthews18,
  title = {Gaussian Process Behaviour in Wide Deep Neural Networks},
  author = {Alexander G. de G. Matthews and Jiri Hron and Mark Rowland and Richard E. Turner and Zoubin Ghahramani},
  booktitle = {International Conference on Learning Representations},
  year = {2018}
}

@inproceedings{lee18,
  author = {Lee, Jaehoon and Sohl-dickstein, Jascha and Pennington, Jeffrey and Novak, Roman and Schoenholz, Sam and Bahri, Yasaman},
  booktitle = {International Conference on Learning Representations},
  citeulike-article-id = {14629527},
  citeulike-linkout-0 = {https://openreview.net/forum?id=B1EA-M-0Z},
  keywords = {deep\_learning},
  posted-at = {2018-08-27 10:37:39},
  priority = {2},
  title = {{Deep Neural Networks as Gaussian Processes}},
  year = {2018}
}

@inproceedings{DondelingerAISTATS13,
  abstract = {{Parameter inference in mechanistic models based on systems of coupled differential equations is a topical yet computationally challenging problem, due to the need to follow each parameter adaptation with a numerical integration of the differential equations. Techniques based on gradient matching, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differential equations, offer a computationally appealing shortcut to the inference problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes due to Calderhead et al. (2008), and shows how inference in this model can be substantially improved by consistently sampling from the joint distribution of the ODE parameters and GP hyperparameters. We demonstrate the efficiency of our adaptive gradient matching technique on three benchmark systems, and perform a detailed comparison with the method in Calderhead et al. (2008) and the explicit ODE integration approach, both in terms of parameter inference accuracy and in terms of computational efficiency.}},
  address = {Scottsdale, Arizona, USA},
  author = {Dondelinger, Frank and Husmeier, Dirk and Rogers, Simon and Filippone, Maurizio},
  booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  citeulike-article-id = {14597761},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v31/dondelinger13a.html},
  editor = {Carvalho, Carlos M. and Ravikumar, Pradeep},
  keywords = {gaussian\_processes},
  month = apr,
  pages = {216--228},
  posted-at = {2018-06-01 14:03:14},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ODE parameter inference using adaptive gradient matching with Gaussian processes}},
  volume = {31},
  year = {2013}
}

@incollection{VanDerWilk17,
  author = {van der Wilk, Mark and Rasmussen, Carl E. and Hensman, James},
  booktitle = {Advances in Neural Information Processing Systems 30},
  citeulike-article-id = {14593639},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/6877-convolutional-gaussian-processes.pdf},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  keywords = {gaussian\_processes},
  pages = {2849--2858},
  posted-at = {2018-05-24 09:59:46},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Convolutional Gaussian Processes}},
  year = {2017}
}

@incollection{Gal17,
  author = {Gal, Yarin and Hron, Jiri and Kendall, Alex},
  booktitle = {Advances in Neural Information Processing Systems 30},
  citeulike-article-id = {14588542},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/6949-concrete-dropout.pdf},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  keywords = {deep\_nets},
  pages = {3581--3590},
  posted-at = {2018-05-18 17:03:54},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Concrete Dropout}},
  year = {2017}
}

@incollection{Kendall17,
  author = {Kendall, Alex and Gal, Yarin},
  booktitle = {Advances in Neural Information Processing Systems 30},
  citeulike-article-id = {14587812},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision.pdf},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  keywords = {deep\_nets},
  pages = {5574--5584},
  posted-at = {2018-05-16 22:49:17},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}},
  year = {2017}
}

@misc{Kingma14b,
  abstract = {{We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  citeulike-article-id = {13539881},
  citeulike-linkout-0 = {http://arxiv.org/abs/1412.6980},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1412.6980},
  day = {30},
  eprint = {1412.6980},
  keywords = {optimization},
  month = jan,
  note = {arXiv:1412.6980},
  posted-at = {2018-05-16 14:30:29},
  priority = {2},
  title = {{Adam: A Method for Stochastic Optimization}},
  year = {2017}
}

@incollection{Lakshminarayanan17,
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle = {Advances in Neural Information Processing Systems 30},
  citeulike-article-id = {14587307},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  keywords = {deep\_nets},
  pages = {6402--6413},
  posted-at = {2018-05-15 10:16:55},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
  year = {2017}
}

@article{Rifkin03,
  author = {Rifkin, Ryan and Yeo, Gene and Poggio, Tomaso and Others},
  citeulike-article-id = {14578549},
  journal = {Nato Science Series Sub Series III Computer and Systems Sciences},
  keywords = {classification},
  pages = {131--154},
  posted-at = {2018-04-30 10:46:34},
  priority = {2},
  publisher = {IOS PRESS},
  title = {{Regularized least-squares classification}},
  volume = {190},
  year = {2003}
}

@article{Suykens99,
  address = {Hingham, MA, USA},
  author = {Suykens, J. A. K. and Vandewalle, J.},
  citeulike-article-id = {14578545},
  citeulike-linkout-0 = {http://dx.doi.org/10.1023/A:1018628609742},
  citeulike-linkout-1 = {https://doi.org/10.1023/A:1018628609742},
  journal = {Neural Process. Lett.},
  keywords = {basis, classification, function, kernel, least, linear, machines, radial, squares, support, vector},
  month = jun,
  number = {3},
  pages = {293--300},
  posted-at = {2018-04-30 10:37:21},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{Least Squares Support Vector Machine Classifiers}},
  volume = {9},
  year = {1999}
}

@inproceedings{Naeini15,
  author = {Naeini, Mahdi P. and Cooper, Gregory F. and Hauskrecht, Milos},
  booktitle = {{AAAI}},
  citeulike-article-id = {14578544},
  keywords = {classification},
  pages = {2901--2907},
  posted-at = {2018-04-30 10:30:03},
  priority = {2},
  publisher = {AAAI Press},
  title = {Obtaining Well Calibrated Probabilities Using {B}ayesian Binning},
  year = {2015}
}

@inproceedings{Zadrozny02,
  address = {New York, NY, USA},
  author = {Zadrozny, Bianca and Elkan, Charles},
  booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  citeulike-article-id = {14578542},
  citeulike-linkout-0 = {http://dx.doi.org/10.1145/775047.775151},
  citeulike-linkout-1 = {http://doi.acm.org/10.1145/775047.775151},
  keywords = {classification},
  location = {Edmonton, Alberta, Canada},
  pages = {694--699},
  posted-at = {2018-04-30 10:27:06},
  priority = {2},
  publisher = {ACM},
  series = {KDD '02},
  title = {{Transforming Classifier Scores into Accurate Multiclass Probability Estimates}},
  year = {2002}
}

@inproceedings{Niculescu-Mizil05,
  address = {New York, NY, USA},
  author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
  booktitle = {Proceedings of the 22Nd International Conference on Machine Learning},
  citeulike-article-id = {14573316},
  citeulike-linkout-0 = {http://dx.doi.org/10.1145/1102351.1102430},
  citeulike-linkout-1 = {http://doi.acm.org/10.1145/1102351.1102430},
  keywords = {neural\_networks},
  location = {Bonn, Germany},
  pages = {625--632},
  posted-at = {2018-04-21 21:56:54},
  priority = {2},
  publisher = {ACM},
  series = {ICML '05},
  title = {{Predicting Good Probabilities with Supervised Learning}},
  year = {2005}
}

@inproceedings{Kull17,
  abstract = {{For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior probability. Isotonic calibration is a powerful non-parametric method that is however prone to overfitting on smaller datasets; hence a parametric method based on the logistic curve is commonly used. While logistic calibration is designed for normally distributed per-class scores, we demonstrate experimentally that many classifiers including Naive Bayes and Adaboost suffer from a particular distortion where these score distributions are heavily skewed. In such cases logistic calibration can easily yield probability estimates that are worse than the original scores. Moreover, the logistic curve family does not include the identity function, and hence logistic calibration can easily uncalibrate a perfectly calibrated classifier. In this paper we solve all these problems with a richer class of calibration maps based on the beta distribution. We derive the method from first principles and show that fitting it is as easy as fitting a logistic curve. Extensive experiments show that beta calibration is superior to logistic calibration for Naive Bayes and Adaboost.}},
  address = {Fort Lauderdale, FL, USA},
  author = {Kull, Meelis and Filho, Telmo S. and Flach, Peter},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  citeulike-article-id = {14560694},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v54/kull17a.html},
  editor = {Singh, Aarti and Zhu, Jerry},
  keywords = {classification},
  month = apr,
  pages = {623--631},
  posted-at = {2018-04-03 12:41:28},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers}},
  volume = {54},
  year = {2017}
}

@incollection{Rudi15,
  author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
  booktitle = {Advances in Neural Information Processing Systems 28},
  citeulike-article-id = {14560689},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/5936-less-is-more-nystrom-computational-regularization.pdf},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  keywords = {kernel\_methods},
  pages = {1657--1665},
  posted-at = {2018-04-03 12:33:16},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Less is More: Nystr\"{o}m Computational Regularization}},
  year = {2015}
}

@article{Platt99b,
  author = {Platt, J.},
  citeulike-article-id = {14560685},
  journal = {Advances in Large Margin Classifiers},
  keywords = {imported, kernel\_methods},
  number = {3},
  posted-at = {2018-04-03 12:23:51},
  priority = {2},
  publisher = {Alexander J. Smola, Peter Bartlett, MA, MIT Press},
  title = {{Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods}},
  volume = {10},
  year = {1999}
}

@inproceedings{Guo17,
  abstract = {{Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}},
  address = {International Convention Centre, Sydney, Australia},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  citeulike-article-id = {14560683},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v70/guo17a.html},
  editor = {Precup, Doina and Teh, Yee W.},
  keywords = {deep\_learning},
  month = aug,
  pages = {1321--1330},
  posted-at = {2018-04-03 12:21:22},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{On Calibration of Modern Neural Networks}},
  volume = {70},
  year = {2017}
}

@incollection{Flach16,
  abstract = {{Classifier calibration is concerned with the scale on which a classifier's scores are expressed. While a classifier ultimately maps instances to discrete classes, it is often beneficial to decompose this mapping into a scoring classifier which outputs one or more real-valued numbers and a decision rule which converts these numbers into predicted classes. For example, a linear classifier might output a positive or negative score whose magnitude is proportional to the distance between the instance and the decision boundary, in which case the decision rule would be a simple threshold on that score. The advantage of calibrating these scores to a known, domain-independent scale is that the decision rule then also takes a domain-independent form and does not have to be learned. The best-known example of this occurs when the classifier's scores approximate, in a precise sense, the posterior probability over the classes; the main advantage of this is that the optimal decision rule is to predict the class that minimizes expected cost averaged over all possible true classes.The main methods to obtain calibrated scores are logistic calibration, which is a parametric method that assumes that the distances on either side of the decision boundary are normally distributed and a nonparametric alternative that is variously known as isotonic regression, the pool adjacent violators (PAV) method or the ROC convex hull (ROCCH) method.}},
  address = {Boston, MA},
  author = {Flach, Peter A.},
  booktitle = {Encyclopedia of Machine Learning and Data Mining},
  citeulike-article-id = {14560236},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-1-4899-7502-7\_900-1},
  citeulike-linkout-1 = {https://doi.org/10.1007/978-1-4899-7502-7\_900-1},
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  keywords = {classification},
  pages = {1--8},
  posted-at = {2018-04-03 10:56:25},
  priority = {2},
  publisher = {Springer US},
  title = {{Classifier Calibration}},
  year = {2016}
}

@inproceedings{Hensman15b,
  abstract = {{Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, out-performing the state of the art on benchmark datasets. Importantly, the variational formulation an be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.}},
  address = {San Diego, California, USA},
  author = {Hensman, James and Matthews, Alexander and Ghahramani, Zoubin},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  citeulike-article-id = {14560233},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v38/hensman15.html},
  editor = {Lebanon, Guy and Vishwanathan, S. V. N.},
  keywords = {gaussian\_processes},
  month = may,
  pages = {351--360},
  posted-at = {2018-04-03 10:55:17},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Scalable Variational Gaussian Process Classification}},
  volume = {38},
  year = {2015}
}

@incollection{Rudi17,
  author = {Rudi, Alessandro and Carratino, Luigi and Rosasco, Lorenzo},
  booktitle = {Advances in Neural Information Processing Systems 30},
  citeulike-article-id = {14560230},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  keywords = {kernel\_methods},
  pages = {3888--3898},
  posted-at = {2018-04-03 10:50:42},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{FALKON}: {A}n {O}ptimal {L}arge {S}cale {K}ernel {M}ethod},
  year = {2017}
}

@article{Trabesinger17,
  author = {Trabesinger, Andreas},
  citeulike-article-id = {14557856},
  journal = {Nature},
  keywords = {computing},
  number = {S1},
  posted-at = {2018-03-28 10:01:40},
  priority = {2},
  title = {{Quantum computing: towards reality}},
  volume = {543},
  year = {2017}
}

@article{Zhu17,
  address = {Secaucus, NJ, USA},
  author = {Zhu, Qi and Wu, Bo and Shen, Xipeng and Shen, Kai and Shen, Li and Wang, Zhiying},
  citeulike-article-id = {14557027},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s11704-016-5468-8},
  citeulike-linkout-1 = {https://doi.org/10.1007/s11704-016-5468-8},
  journal = {Front. Comput. Sci.},
  keywords = {analysis, co-run, degradation, distributed\_computing, fused, gpgpu, performance, processor, program, transformation},
  month = feb,
  number = {1},
  pages = {130--146},
  posted-at = {2018-03-27 10:37:15},
  priority = {2},
  publisher = {Springer-Verlag New York, Inc.},
  title = {{Understanding Co-run Performance on CPU-GPU Integrated Processors: Observations, Insights, Directions}},
  volume = {11},
  year = {2017}
}

@inproceedings{Li14,
  address = {Berkeley, CA, USA},
  author = {Li, Mu and Andersen, David G. and Park, Jun W. and Smola, Alexander J. and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J. and Su, Bor-Yiing},
  booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
  citeulike-article-id = {14556997},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=2685048.2685095},
  keywords = {distributed\_computing},
  location = {Broomfield, CO},
  pages = {583--598},
  posted-at = {2018-03-27 09:58:51},
  priority = {2},
  publisher = {USENIX Association},
  series = {OSDI'14},
  title = {{Scaling Distributed Machine Learning with the Parameter Server}},
  year = {2014}
}

@inproceedings{Chilimbi14,
  address = {Broomfield, CO},
  author = {Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
  booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  citeulike-article-id = {14556996},
  citeulike-linkout-0 = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi},
  keywords = {distributed\_computing},
  pages = {571--582},
  posted-at = {2018-03-27 09:57:59},
  priority = {2},
  publisher = {{USENIX} Association},
  title = {{Project Adam: Building an Efficient and Scalable Deep Learning Training System}},
  year = {2014}
}

@inproceedings{XiongACMMM16,
  author = {Xiong, Xiaoyu and Filippone, Maurizio and Vinciarelli, Alessandro},
  booktitle = {ACM Multimedia},
  citeulike-article-id = {14556403},
  keywords = {gaussian\_processes},
  posted-at = {2018-03-26 14:20:26},
  priority = {2},
  title = {Looking Good With Flickr Faves: {G}aussian Processes for Finding Difference Makers in Personality Impressions},
  year = {2016}
}

@article{KimIEEETAFFC14,
  author = {Kim, Samuel and Valente, Fabio and Filippone, Maurizio and Vinciarelli, Alessandro},
  citeulike-article-id = {14556401},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TAFFC.2014.2324564},
  journal = {IEEE Transactions on Affective Computing},
  keywords = {gaussian\_processes},
  number = {2},
  pages = {187--200},
  posted-at = {2018-03-26 14:19:07},
  priority = {2},
  title = {Predicting Continuous Conflict Perception with {B}ayesian {G}aussian Processes},
  volume = {5},
  year = {2014}
}

@incollection{Hensman15,
  author = {Hensman, James and Matthews, Alexander G. and Filippone, Maurizio and Ghahramani, Zoubin},
  booktitle = {Advances in Neural Information Processing Systems 28},
  citeulike-article-id = {14556394},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/5875-mcmc-for-variationally-sparse-gaussian-processes.pdf},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  keywords = {gaussian\_processes},
  pages = {1648--1656},
  posted-at = {2018-03-26 14:06:05},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{MCMC for Variationally Sparse Gaussian Processes}},
  year = {2015}
}

@inproceedings{Bottou10,
  abstract = {{During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.}},
  address = {Heidelberg},
  author = {Bottou, L{\'{e}}on},
  booktitle = {Proceedings of COMPSTAT'2010},
  citeulike-article-id = {14556391},
  editor = {Lechevallier, Yves and Saporta, Gilbert},
  keywords = {optimization},
  pages = {177--186},
  posted-at = {2018-03-26 14:01:13},
  priority = {2},
  publisher = {Physica-Verlag HD},
  title = {{Large-Scale Machine Learning with Stochastic Gradient Descent}},
  year = {2010}
}

@article{Taylor12b,
  abstract = {{The fifth phase of the Coupled Model Intercomparison Project (CMIP5) will produce a state-of-the- art multimodel dataset designed to advance our knowledge of climate variability and climate change. Researchers worldwide are analyzing the model output and will produce results likely to underlie the forthcoming Fifth Assessment Report by the Intergovernmental Panel on Climate Change. Unprecedented in scale and attracting interest from all major climate modeling groups, CMIP5 includes  ” long term” simulations of twentieth-century climate and projections for the twenty-first century and beyond. Conventional atmosphere–ocean global climate models and Earth system models of intermediate complexity are for the first time being joined by more recently developed Earth system models under an experiment design that allows both types of models to be compared to observations on an equal footing. Besides the longterm experiments, CMIP5 calls for an entirely new suite of  ” near term” simulations focusing on recent decades and the future to year 2035. These  ” decadal predictions” are initialized based on observations and will be used to explore the predictability of climate and to assess the forecast system's predictive skill. The CMIP5 experiment design also allows for participation of stand-alone atmospheric models and includes a variety of idealized experiments that will improve understanding of the range of model responses found in the more complex and realistic simulations. An exceptionally comprehensive set of model output is being collected and made freely available to researchers through an integrated but distributed data archive. For researchers unfamiliar with climate models, the limitations of the models and experiment design are described.}},
  author = {Taylor, Karl E. and Stouffer, Ronald J. and Meehl, Gerald A.},
  citeulike-article-id = {14556007},
  citeulike-linkout-0 = {http://dx.doi.org/10.1175/BAMS-D-11-00094.1},
  citeulike-linkout-1 = {https://doi.org/10.1175/BAMS-D-11-00094.1},
  journal = {Bulletin of the American Meteorological Society},
  keywords = {climate},
  number = {4},
  pages = {485--498},
  posted-at = {2018-03-25 11:37:18},
  priority = {2},
  title = {An Overview of {CMIP5} and the Experiment Design},
  volume = {93},
  year = {2012}
}

@incollection{Kingma15,
  author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
  booktitle = {Advances in Neural Information Processing Systems 28},
  citeulike-article-id = {14556006},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  keywords = {variational\_inference},
  pages = {2575--2583},
  posted-at = {2018-03-25 11:32:03},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Variational Dropout and the Local Reparameterization Trick}},
  year = {2015}
}

@inproceedings{Sriperumbudur15,
  address = {Cambridge, MA, USA},
  author = {Sriperumbudur, Bharath K. and Szab\'{o}, Zolt\'{a}n},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
  citeulike-article-id = {14556005},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=2969239.2969367},
  keywords = {kernels},
  location = {Montreal, Canada},
  pages = {1144--1152},
  posted-at = {2018-03-25 11:30:08},
  priority = {2},
  publisher = {MIT Press},
  series = {NIPS'15},
  title = {{Optimal Rates for Random Fourier Features}},
  year = {2015}
}

@inproceedings{Sutherland15,
  address = {Arlington, Virginia, United States},
  author = {Sutherland, Dougal J. and Schneider, Jeff},
  booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
  citeulike-article-id = {14556004},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=3020847.3020936},
  keywords = {kernels},
  location = {Amsterdam, Netherlands},
  pages = {862--871},
  posted-at = {2018-03-25 11:27:44},
  priority = {2},
  publisher = {AUAI Press},
  series = {UAI'15},
  title = {{On the Error of Random Fourier Features}},
  year = {2015}
}

@inproceedings{Avron17,
  abstract = {{Random Fourier features is one of the most popular techniques for scaling up kernel methods, such as kernel ridge regression. However, despite impressive empirical results, the statistical properties of random Fourier features are still not well understood. In this paper we take steps toward filling this gap. Specifically, we approach random Fourier features from a spectral matrix approximation point of view, give tight bounds on the number of Fourier features required to achieve a spectral approximation, and show how spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression.}},
  address = {International Convention Centre, Sydney, Australia},
  author = {Avron, Haim and Kapralov, Michael and Musco, Cameron and Musco, Christopher and Velingker, Ameya and Zandieh, Amir},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  citeulike-article-id = {14556003},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v70/avron17a.html},
  editor = {Precup, Doina and Teh, Yee W.},
  keywords = {kernels},
  month = aug,
  pages = {253--262},
  posted-at = {2018-03-25 11:25:33},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Random {F}ourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees},
  volume = {70},
  year = {2017}
}

@inproceedings{Choromanski18,
  author = {Choromanski, Krzysztof and Rowland, Mark and Sarlos, Tamas and Sindhwani, Vikas and Turner, Richard and Weller, Adrian},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS'18)},
  citeulike-article-id = {14556002},
  keywords = {kernels},
  posted-at = {2018-03-25 11:23:18},
  priority = {2},
  title = {{The Geometry of Random Features}},
  year = {2017}
}

@article{Shabat18,
  author = {Shabat, Gil and Shmueli, Yaniv and Aizenbud, Yariv and Averbuch, Amir},
  citeulike-article-id = {14556001},
  citeulike-linkout-0 = {http://dx.doi.org/https://doi.org/10.1016/j.acha.2016.04.006},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S1063520316300069},
  journal = {Applied and Computational Harmonic Analysis},
  keywords = {algebra, algorithms, decomposition, factorizations, lu, matrices, matrix, random, randomized},
  number = {2},
  pages = {246--272},
  posted-at = {2018-03-25 11:16:46},
  priority = {2},
  title = {{Randomized LU decomposition}},
  volume = {44},
  year = {2018}
}

@article{Mahoney11,
  address = {Hanover, MA, USA},
  author = {Mahoney, Michael W.},
  citeulike-article-id = {14556000},
  citeulike-linkout-0 = {http://dx.doi.org/10.1561/2200000035},
  journal = {Found. Trends Mach. Learn.},
  keywords = {algebra},
  month = feb,
  number = {2},
  pages = {123--224},
  posted-at = {2018-03-25 11:15:26},
  priority = {2},
  publisher = {Now Publishers Inc.},
  title = {{Randomized Algorithms for Matrices and Data}},
  volume = {3},
  year = {2011}
}

@incollection{Nokland16,
  author = {N{\o} kland, Arild},
  booktitle = {Advances in Neural Information Processing Systems 29},
  citeulike-article-id = {14555999},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/6441-direct-feedback-alignment-provides-learning-in-deep-neural-networks.pdf},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  keywords = {neural\_nets},
  pages = {1037--1045},
  posted-at = {2018-03-25 11:11:44},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Direct Feedback Alignment Provides Learning in Deep Neural Networks}},
  year = {2016}
}

@article{Hornik91,
  author = {Hornik, Kurt},
  citeulike-article-id = {14555998},
  citeulike-linkout-0 = {http://dx.doi.org/https://doi.org/10.1016/0893-6080(91)90009-T},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
  journal = {Neural Networks},
  keywords = {activation, approximation, capabilities, environment, feedforward, function, input, measure, multilayer, networks, neural\_nets, smooth, sobolev, spaces, uniform, universal},
  number = {2},
  pages = {251--257},
  posted-at = {2018-03-25 11:08:36},
  priority = {2},
  title = {{Approximation capabilities of multilayer feedforward networks}},
  volume = {4},
  year = {1991}
}

@article{Micchelli06,
  author = {Micchelli, Charles A. and Xu, Yuesheng and Zhang, Haizhang},
  citeulike-article-id = {14555997},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=1248547.1248642},
  journal = {Journal of Machine Learning Research},
  keywords = {kernels},
  month = dec,
  pages = {2651--2667},
  posted-at = {2018-03-25 10:53:29},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Universal Kernels}},
  volume = {7},
  year = {2006}
}

@article{Cybenko89,
  abstract = {{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}},
  author = {Cybenko, G.},
  citeulike-article-id = {14555996},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF02551274},
  citeulike-linkout-1 = {https://doi.org/10.1007/BF02551274},
  day = {01},
  journal = {Mathematics of Control, Signals and Systems},
  keywords = {neural\_networks},
  month = dec,
  number = {4},
  pages = {303--314},
  posted-at = {2018-03-25 10:51:50},
  priority = {2},
  title = {{Approximation by superpositions of a sigmoidal function}},
  volume = {2},
  year = {1989}
}

@misc{Jouppi17,
  abstract = {{Many architects believe that major improvements in cost-energy-performance
must now come from domain-specific hardware. This paper evaluates a custom
ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since
2015 that accelerates the inference phase of neural networks (NN). The heart of
the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak
throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed
on-chip memory. The TPU's deterministic execution model is a better match to
the 99th-percentile response-time requirement of our NN applications than are
the time-varying optimizations of CPUs and GPUs (caches, out-of-order
execution, multithreading, multiprocessing, prefetching, ...) that help average
throughput more than guaranteed latency. The lack of such features helps
explain why, despite having myriad MACs and a big memory, the TPU is relatively
small and low power. We compare the TPU to a server-class Intel Haswell CPU and
an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters.
Our workload, written in the high-level TensorFlow framework, uses production
NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters'
NN inference demand. Despite low utilization for some applications, the TPU is
on average about 15X - 30X faster than its contemporary GPU or CPU, with
TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the
TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and
200X the CPU.}},
  author = {{Jouppi et al}},
  citeulike-article-id = {14555995},
  citeulike-linkout-0 = {http://arxiv.org/abs/1704.04760},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1704.04760},
  day = {16},
  eprint = {1704.04760},
  keywords = {hardware},
  month = apr,
  note = {arXiv:1704.04760},
  posted-at = {2018-03-25 10:46:32},
  priority = {2},
  title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
  year = {2017}
}

@incollection{Salzmann10,
  author = {Salzmann, Mathieu and Urtasun, Raquel},
  booktitle = {Advances in Neural Information Processing Systems 23},
  citeulike-article-id = {14530784},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/4179-implicitly-constrained-gaussian-process-regression-for-monocular-non-rigid-pose-estimation.pdf},
  editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
  keywords = {gaussian\_processes},
  pages = {2065--2073},
  posted-at = {2018-02-07 11:37:17},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation}},
  year = {2010}
}

@article{DaVeiga12,
  author = {Veiga, S\'{e}bastien D. and Marrel, Amandine},
  citeulike-article-id = {14530736},
  citeulike-linkout-0 = {http://eudml.org/doc/250989},
  journal = {Annales de la facult\'{e} des sciences de Toulouse Math\'{e}matiques},
  keywords = {gaussian\_processes},
  month = apr,
  number = {3},
  pages = {529--555},
  posted-at = {2018-02-07 10:03:13},
  priority = {2},
  publisher = {Universit\'{e} Paul Sabatier, Toulouse},
  title = {{Gaussian process modeling with inequality constraints}},
  volume = {21},
  year = {2012}
}

@inproceedings{Riihimaki10,
  abstract = {{A method for using monotonicity information in multivariate Gaussian process regression and classification is proposed. Monotonicity information is introduced with virtual derivative observations, and the resulting posterior is approximated with expectation propagation. Behaviour of the method is illustrated with artificial regression examples, and the method is used in a real world health care classification problem to include monotonicity information with respect to one of the covariates.}},
  address = {Chia Laguna Resort, Sardinia, Italy},
  author = {Riihim\"{a}ki, Jaakko and Vehtari, Aki},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  citeulike-article-id = {14530700},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v9/riihimaki10a.html},
  editor = {Teh, Yee W. and Titterington, Mike},
  keywords = {gaussian\_processes},
  month = may,
  pages = {645--652},
  posted-at = {2018-02-07 09:11:43},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Gaussian processes with monotonicity information}},
  volume = {9},
  year = {2010}
}

@incollection{Gorbach17,
  author = {Gorbach, Nico S. and Bauer, Stefan and Buhmann, Joachim M.},
  booktitle = {Advances in Neural Information Processing Systems 30},
  citeulike-article-id = {14525472},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  keywords = {ode},
  pages = {4809--4818},
  posted-at = {2018-01-28 22:21:19},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Scalable Variational Inference for Dynamical Systems}},
  year = {2017}
}

@article{Macdonald15b,
  abstract = {{Parameter inference in mathematical models of biological pathways, expressed as coupled ordinary differential equations (ODEs), is a challenging problem in contemporary systems biology. Conventional methods involve repeatedly solving the ODEs by numerical integration, which is computationally onerous and does not scale up to complex systems. Aimed at reducing the computational costs, new concepts based on gradient matching have recently been proposed in the computational statistics and machine learning literature. In a preliminary smoothing step, the time series data are interpolated; then, in a second step, the parameters of the ODEs are optimised so as to minimise some metric measuring the difference between the slopes of the tangents to the interpolants, and the time derivatives from the ODEs. In this way, the ODEs never have to be solved explicitly. This review provides a concise methodological overview of the current state-of-the-art methods for gradient matching in ODEs, followed by an empirical comparative evaluation based on a set of widely used and representative benchmark data.}},
  author = {Macdonald, Benn and Husmeier, Dirk},
  citeulike-article-id = {14525471},
  citeulike-linkout-0 = {http://dx.doi.org/10.3389/fbioe.2015.00180},
  citeulike-linkout-1 = {https://www.frontiersin.org/article/10.3389/fbioe.2015.00180},
  journal = {Frontiers in Bioengineering and Biotechnology},
  keywords = {ode},
  pages = {180},
  posted-at = {2018-01-28 22:14:37},
  priority = {2},
  title = {{Gradient Matching Methods for Computational Inference in Mechanistic Models for Systems Biology: A Review and Comparative Analysis}},
  volume = {3},
  year = {2015}
}

@article{Niu17,
  abstract = {{Inference in mechanistic models of non-linear differential equations is a challenging problem in current computational statistics. Due to the high computational costs of numerically solving the differential equations in every step of an iterative parameter adaptation scheme, approximate methods based on gradient matching have become popular. However, these methods critically depend on the smoothing scheme for function interpolation. The present article adapts an idea from manifold learning and demonstrates that a time warping approach aiming to homogenize intrinsic length scales can lead to a significant improvement in parameter estimation accuracy. We demonstrate the effectiveness of this scheme on noisy data from two dynamical systems with periodic limit cycle, a biopathway, and an application from soft-tissue mechanics. Our study also provides a comparative evaluation on a wide range of signal-to-noise ratios.}},
  author = {Niu, Mu and Macdonald, Benn and Rogers, Simon and Filippone, Maurizio and Husmeier, Dirk},
  citeulike-article-id = {14525470},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s00180-017-0753-z},
  citeulike-linkout-1 = {https://doi.org/10.1007/s00180-017-0753-z},
  day = {09},
  journal = {Computational Statistics},
  keywords = {ode},
  month = aug,
  posted-at = {2018-01-28 22:11:04},
  priority = {2},
  title = {{Statistical inference in mechanistic models: time warping for improved gradient matching}},
  year = {2017}
}

@inproceedings{Macdonald15,
  abstract = {{Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by.}},
  address = {Lille, France},
  author = {Macdonald, Benn and Higham, Catherine and Husmeier, Dirk},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  citeulike-article-id = {14525469},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v37/macdonald15.html},
  editor = {Bach, Francis and Blei, David},
  keywords = {ode},
  month = jul,
  pages = {1539--1547},
  posted-at = {2018-01-28 22:08:49},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Controversy in mechanistic modelling with Gaussian processes}},
  volume = {37},
  year = {2015}
}

@article{Gonzales14,
  author = {Gonz\'{a}lez, Javier and Vuja\v{c}i\'{c}, Ivan and Wit, Ernst},
  citeulike-article-id = {14525468},
  citeulike-linkout-0 = {http://dx.doi.org/https://doi.org/10.1016/j.patrec.2014.02.019},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S0167865514000695},
  journal = {Pattern Recognition Letters},
  keywords = {differential, equations, gene, hilbert, kernel, network, ode, of, operator, ordinary, regulatory, reproducing, space, system},
  pages = {26--32},
  posted-at = {2018-01-28 22:07:20},
  priority = {2},
  title = {Reproducing kernel {H}ilbert space based estimation of systems of ordinary differential equations},
  volume = {45},
  year = {2014}
}

@inproceedings{Barber14,
  abstract = {{Bayesian parameter estimation in coupled ordinary differential equations (ODEs) is challenging due to the high computational cost of numerical integration. In gradient matching a separate data model is introduced with the property that its gradient can be calculated easily. Parameter estimation is achieved by requiring consistency between the gradients computed from the data model and those specified by the ODE. We propose a Gaussian process model that directly links state derivative information with system observations, simplifying previous approaches and providing a natural generative model.}},
  address = {Bejing, China},
  author = {Barber, David and Wang, Yali},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  citeulike-article-id = {14525466},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v32/barber14.html},
  editor = {Xing, Eric P. and Jebara, Tony},
  keywords = {ode},
  month = jun,
  number = {2},
  pages = {1485--1493},
  posted-at = {2018-01-28 22:05:20},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations}},
  volume = {32},
  year = {2014}
}

@article{Liang08,
  author = {Liang, Hua and Wu, Hulin},
  citeulike-article-id = {14525465},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/016214508000000797},
  citeulike-linkout-1 = {https://doi.org/10.1198/016214508000000797},
  eprint = {https://doi.org/10.1198/016214508000000797},
  journal = {Journal of the American Statistical Association},
  keywords = {ode},
  note = {PMID: 19956350},
  number = {484},
  pages = {1570--1583},
  posted-at = {2018-01-28 22:03:47},
  priority = {2},
  publisher = {Taylor \& Francis},
  title = {{Parameter Estimation for Differential Equation Models Using a Framework of Measurement Error in Regression Models}},
  volume = {103},
  year = {2008}
}

@inproceedings{Niu16,
  abstract = {{Parameter inference in mechanistic models of coupled differential equations is a topical and challenging problem. We propose a new method based on kernel ridge regression and gradient matching, and an objective function that simultaneously encourages goodness of fit and penalises inconsistencies with the differential equations. Fast minimisation is achieved by exploiting partial convexity inherent in this function, and setting up an iterative algorithm in the vein of the EM algorithm. An evaluation of the proposed method on various benchmark data suggests that it compares favourably with state-of-the-art alternatives.}},
  address = {New York, New York, USA},
  author = {Niu, Mu and Rogers, Simon and Filippone, Maurizio and Husmeier, Dirk},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  citeulike-article-id = {14525463},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v48/niu16.html},
  editor = {Balcan, Maria F. and Weinberger, Kilian Q.},
  keywords = {ode},
  month = jun,
  pages = {1699--1707},
  posted-at = {2018-01-28 22:00:10},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Fast Parameter Inference in Nonlinear Dynamical Systems using Iterative Gradient Matching}},
  volume = {48},
  year = {2016}
}

@article{Varah82,
  author = {Varah, J. M.},
  citeulike-article-id = {14525462},
  citeulike-linkout-0 = {http://dx.doi.org/10.1137/0903003},
  citeulike-linkout-1 = {https://doi.org/10.1137/0903003},
  eprint = {https://doi.org/10.1137/0903003},
  journal = {SIAM Journal on Scientific and Statistical Computing},
  keywords = {ode},
  number = {1},
  pages = {28--46},
  posted-at = {2018-01-28 21:57:13},
  priority = {2},
  title = {{A Spline Least Squares Method for Numerical Parameter Estimation in Differential Equations}},
  volume = {3},
  year = {1982}
}

@article{Ramsay07,
  abstract = {{We propose a new method for estimating parameters in models that are defined by a system of non-linear differential equations. Such equations represent changes in system outputs by linking the behaviour of derivatives of a process to the behaviour of the process itself. Current methods for estimating parameters in differential equations from noisy data are computationally intensive and often poorly suited to the realization of statistical objectives such as inference and interval estimation. The paper describes a new method that uses noisy measurements on a subset of variables to estimate the parameters defining a system of non-linear differential equations. The approach is based on a modification of data smoothing methods along with a generalization of profiled estimation. We derive estimates and confidence intervals, and show that these have low bias and good coverage properties respectively for data that are simulated from models in chemical engineering and neurobiology. The performance of the method is demonstrated by using real world data from chemistry and from the progress of the autoimmune disease lupus. Copyright 2007 Royal Statistical Society.}},
  author = {Ramsay, J. O. and Hooker, G. and Campbell, D. and Cao, J.},
  citeulike-article-id = {14525461},
  citeulike-linkout-0 = {https://EconPapers.repec.org/RePEc:bla:jorssb:v:69:y:2007:i:5:p:741-796},
  journal = {Journal of the Royal Statistical Society Series B},
  keywords = {ode},
  number = {5},
  pages = {741--796},
  posted-at = {2018-01-28 21:48:08},
  priority = {2},
  title = {{Parameter estimation for differential equations: a generalized smoothing approach}},
  volume = {69},
  year = {2007}
}

@article{Campbell12,
  abstract = {{Differential equations are used in modeling diverse system behaviors in a wide variety of sciences. Methods for estimating the differential equation parameters traditionally depend on the inclusion of initial system states and numerically solving the equations. This paper presents Smooth Functional Tempering, a new population Markov Chain Monte Carlo approach for posterior estimation of parameters. The proposed method borrows insights from parallel tempering and model based smoothing to define a sequence of approximations to the posterior. The tempered approximations depend on relaxations of the solution to the differential equation model, reducing the need for estimating the initial system states and obtaining a numerical differential equation solution. Rather than tempering via approximations to the posterior that are more heavily rooted in the prior, this new method tempers towards data features. Using our proposed approach, we observed faster convergence and robustness to both initial values and prior distributions that do not reflect the features of the data. Two variations of the method are proposed and their performance is examined through simulation studies and a real application to the chemical reaction dynamics of producing nylon.}},
  author = {Campbell, David and Steele, Russell J.},
  citeulike-article-id = {14525460},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s11222-011-9234-3},
  citeulike-linkout-1 = {https://doi.org/10.1007/s11222-011-9234-3},
  day = {01},
  journal = {Statistics and Computing},
  keywords = {ode},
  month = mar,
  number = {2},
  pages = {429--443},
  posted-at = {2018-01-28 21:45:52},
  priority = {2},
  title = {{Smooth functional tempering for nonlinear differential equation models}},
  volume = {22},
  year = {2012}
}

@incollection{Calderhead08,
  author = {Calderhead, Ben and Girolami, Mark and Lawrence, Neil D.},
  booktitle = {Advances in Neural Information Processing Systems 21},
  citeulike-article-id = {14525459},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/3497-accelerating-bayesian-inference-over-nonlinear-differential-equations-with-gaussian-processes.pdf},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  keywords = {gaussian\_processes},
  pages = {217--224},
  posted-at = {2018-01-28 21:44:04},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes}},
  year = {2009}
}

@inproceedings{Krauth17,
  author = {Krauth, Karl and Bonilla, Edwin V. and Cutajar, Kurt and Filippone, Maurizio},
  booktitle = {Thirty-Third Conference on Uncertainty in Artificial Intelligence, {UAI} 2017, August 11-15, 2017, Sydney, Australia},
  citeulike-article-id = {14459274},
  keywords = {gaussian\_processes},
  posted-at = {2017-10-16 13:35:35},
  priority = {2},
  title = {{AutoGP}: {E}xploring the Capabilities and Limitations of {G}aussian Process Models},
  year = {2017}
}

@article{LorenziNeuroImage17,
  author = {Lorenzi, Marco and Filippone, Maurizio and Frisoni, Giovanni B. and Alexander, Daniel C. and Ourselin, Sebastien},
  citeulike-article-id = {14459273},
  journal = {NeuroImage},
  keywords = {gaussian\_processes},
  note = {to appear},
  posted-at = {2017-10-16 13:29:45},
  priority = {2},
  title = {Probabilistic disease progression modeling to characterize diagnostic uncertainty: staging and prediction in {A}lzheimer's disease},
  year = {2017}
}

@article{Rondina16,
  author = {Rondina, Jane M. and Filippone, Maurizio and Girolami, Mark and Ward, Nick S.},
  citeulike-article-id = {14459270},
  citeulike-linkout-0 = {http://dx.doi.org/https://doi.org/10.1016/j.nicl.2016.07.014},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S2213158216301346},
  journal = {NeuroImage: Clinical},
  keywords = {gaussian\_processes, stroke},
  number = {Supplement C},
  pages = {372--380},
  posted-at = {2017-10-16 13:27:21},
  priority = {2},
  title = {{Decoding post-stroke motor function from structural brain imaging}},
  volume = {12},
  year = {2016}
}

@article{LeCun15,
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  citeulike-article-id = {14448277},
  journal = {Nature},
  keywords = {deep\_learning},
  number = {7553},
  pages = {436--444},
  posted-at = {2017-10-10 14:38:47},
  priority = {2},
  publisher = {Nature Publishing Group},
  title = {{Deep learning}},
  volume = {521},
  year = {2015}
}

@article{Hilbert11,
  abstract = {We estimated the world{\\textquoteright}s technological capacity to store, communicate, and compute information, tracking 60 analog and digital technologies during the period from 1986 to 2007. In 2007, humankind was able to store 2.9 {\\texttimes} 1020 optimally compressed bytes, communicate almost 2 {\\texttimes} 1021 bytes, and carry out 6.4 {\\texttimes} 1018 instructions per second on general-purpose computers. General-purpose computing capacity grew at an annual rate of 58\%. The world{\\textquoteright}s capacity for bidirectional telecommunication grew at 28\% per year, closely followed by the increase in globally stored information (23\%). Humankind{\\textquoteright}s capacity for unidirectional information diffusion through broadcasting channels has experienced comparatively modest annual growth (6\%). Telecommunication has been dominated by digital technologies since 1990 (99.9\% in digital format in 2007), and the majority of our technological memory has been in digital format since the early 2000s (94\\ digital in 2007).},
  author = {Hilbert, Martin and L\'{o}pez, Priscila},
  citeulike-article-id = {14448273},
  citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.1200970},
  citeulike-linkout-1 = {http://science.sciencemag.org/content/332/6025/60},
  journal = {Science},
  keywords = {computing},
  number = {6025},
  pages = {60--65},
  posted-at = {2017-10-10 14:37:03},
  priority = {2},
  publisher = {American Association for the Advancement of Science},
  title = {{The world's technological capacity to store, communicate, and compute information}},
  volume = {332},
  year = {2011}
}

@article{Gonzalez15,
  author = {Gonzalez, Joseph E. and Bailis, Peter and Jordan, Michael I. and Franklin, Michael J. and Hellerstein, Joseph M. and Ghodsi, Ali and Stoica, Ion},
  citeulike-article-id = {14448271},
  citeulike-linkout-0 = {http://arxiv.org/abs/1510.07092},
  eprint = {1510.07092},
  journal = {CoRR},
  keywords = {distributed},
  posted-at = {2017-10-10 14:36:38},
  priority = {2},
  title = {{Asynchronous Complex Analytics in a Distributed Dataflow Architecture}},
  volume = {abs/1510.07092},
  year = {2015}
}

@article{Burda15,
  author = {Burda, Yuri and Grosse, Roger B. and Salakhutdinov, Ruslan},
  citeulike-article-id = {14448270},
  citeulike-linkout-0 = {http://arxiv.org/abs/1509.00519},
  eprint = {1509.00519},
  journal = {CoRR},
  keywords = {deep\_learning},
  posted-at = {2017-10-10 14:36:11},
  priority = {2},
  title = {{Importance Weighted Autoencoders}},
  volume = {abs/1509.00519},
  year = {2015}
}

@inproceedings{Huang14,
  author = {Huang, Po{-}Sen and Avron, Haim and Sainath, Tara N. and Sindhwani, Vikas and Ramabhadran, Bhuvana},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing},
  citeulike-article-id = {14448269},
  keywords = {kernels},
  posted-at = {2017-10-10 14:35:18},
  priority = {2},
  title = {Kernel methods match Deep Neural Networks on {TIMIT}},
  year = {2014}
}

@article{Lu14,
  author = {Lu, Zhiyun and May, Avner and Liu, Kuan and Garakani, Alireza B. and Guo, Dong and Bellet, Aur{\'{e}}lien and Fan, Linxi and Collins, Michael and Kingsbury, Brian and Picheny, Michael and Sha, Fei},
  citeulike-article-id = {14448268},
  journal = {CoRR},
  keywords = {kernels},
  posted-at = {2017-10-10 14:35:06},
  priority = {2},
  title = {{How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets}},
  volume = {abs/1411.4000},
  year = {2014}
}

@inproceedings{Collobert08,
  address = {New York, NY, USA},
  author = {Collobert, Ronan and Weston, Jason},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  citeulike-article-id = {14448266},
  citeulike-linkout-0 = {http://dx.doi.org/10.1145/1390156.1390177},
  citeulike-linkout-1 = {http://doi.acm.org/10.1145/1390156.1390177},
  keywords = {deep\_learning},
  location = {Helsinki, Finland},
  pages = {160--167},
  posted-at = {2017-10-10 14:33:19},
  priority = {2},
  publisher = {ACM},
  series = {ICML '08},
  title = {{A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning}},
  year = {2008}
}

@article{Hinton12,
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Others},
  citeulike-article-id = {14448264},
  journal = {IEEE Signal Processing Magazine},
  keywords = {deep\_learning},
  number = {6},
  pages = {82--97},
  posted-at = {2017-10-10 14:31:53},
  priority = {2},
  publisher = {IEEE},
  title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
  volume = {29},
  year = {2012}
}

@article{Saade15,
  author = {Saade, Alaa and Caltagirone, Francesco and Carron, Igor and Daudet, Laurent and Dr{\'{e}}meau, Ang{\'{e}}lique and Gigan, Sylvain and Krzakala, Florent},
  citeulike-article-id = {14448263},
  citeulike-linkout-0 = {http://arxiv.org/abs/1510.06664},
  eprint = {1510.06664},
  journal = {CoRR},
  keywords = {kernels},
  posted-at = {2017-10-10 14:28:51},
  priority = {2},
  title = {{Random Projections through multiple optical scattering: Approximating kernels at the speed of light}},
  volume = {abs/1510.06664},
  year = {2015}
}

@article{Hennig15,
  abstract = {{We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data have led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimizers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.}},
  author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
  citeulike-article-id = {14448261},
  citeulike-linkout-0 = {http://dx.doi.org/10.1098/rspa.2015.0142},
  citeulike-linkout-1 = {http://rspa.royalsocietypublishing.org/content/471/2179/20150142},
  eprint = {http://rspa.royalsocietypublishing.org/content/471/2179/20150142.full.pdf},
  journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  keywords = {probabilistic\_numerics},
  number = {2179},
  posted-at = {2017-10-10 14:25:21},
  priority = {2},
  publisher = {The Royal Society},
  title = {{Probabilistic numerics and uncertainty in computations}},
  volume = {471},
  year = {2015}
}

@misc{Bradshaw17,
  abstract = {{Deep neural networks (DNNs) have excellent representative power and are state
of the art classifiers on many tasks. However, they often do not capture their
own uncertainties well making them less robust in the real world as they
overconfidently extrapolate and do not notice domain shift. Gaussian processes
(GPs) with RBF kernels on the other hand have better calibrated uncertainties
and do not overconfidently extrapolate far from data in their training set.
However, GPs have poor representational power and do not perform as well as
DNNs on complex domains. In this paper we show that GP hybrid deep networks,
GPDNNs, (GPs on top of DNNs and trained end-to-end) inherit the nice properties
of both GPs and DNNs and are much more robust to adversarial examples. When
extrapolating to adversarial examples and testing in domain shift settings,
GPDNNs frequently output high entropy class probabilities corresponding to
essentially "don't know". GPDNNs are therefore promising as deep architectures
that know when they don't know.}},
  author = {Bradshaw, John and Alexander and Ghahramani, Zoubin},
  citeulike-article-id = {14391642},
  citeulike-linkout-0 = {http://arxiv.org/abs/1707.02476},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1707.02476},
  day = {8},
  eprint = {1707.02476},
  keywords = {deep\_learning, gaussian\_processes},
  month = jul,
  note = {arXiv:1707.02476},
  posted-at = {2017-10-05 09:42:33},
  priority = {2},
  title = {{Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks}},
  year = {2017}
}

@inproceedings{Wilson16b,
  abstract = {{We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.}},
  address = {Cadiz, Spain},
  author = {Wilson, Andrew G. and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  citeulike-article-id = {14445348},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v51/wilson16.html},
  editor = {Gretton, Arthur and Robert, Christian C.},
  keywords = {gaussian\_processes},
  month = may,
  pages = {370--378},
  posted-at = {2017-10-04 14:56:46},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Deep Kernel Learning}},
  volume = {51},
  year = {2016}
}

@incollection{Cho09,
  author = {Cho, Youngmin and Saul, Lawrence K.},
  booktitle = {Advances in Neural Information Processing Systems 22},
  citeulike-article-id = {14445323},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  keywords = {deep\_learning, kernels},
  pages = {342--350},
  posted-at = {2017-10-04 14:01:57},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Kernel Methods for Deep Learning}},
  year = {2009}
}

@misc{Krauth16,
  abstract = {{We investigate the capabilities and limitations of Gaussian process models by
jointly exploring three complementary directions: (i) scalable and
statistically efficient inference; (ii) flexible kernels; and (iii) objective
functions for hyperparameter learning alternative to the marginal likelihood.
Our approach outperforms all previously reported GP methods on the standard
MNIST dataset; performs comparatively to previous kernel-based methods using
the RECTANGLES-IMAGE dataset; and breaks the 1\% error-rate barrier in GP models
using the MNIST8M dataset, showing along the way the scalability of our method
at unprecedented scale for GP models (8 million observations) in classification
problems. Overall, our approach represents a significant breakthrough in kernel
methods and GP models, bridging the gap between deep learning approaches and
kernel machines.}},
  author = {Krauth, Karl and Bonilla, Edwin V. and Cutajar, Kurt and Filippone, Maurizio},
  citeulike-article-id = {14445322},
  citeulike-linkout-0 = {http://arxiv.org/abs/1610.05392},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1610.05392},
  day = {6},
  eprint = {1610.05392},
  keywords = {gaussian\_processes},
  month = mar,
  posted-at = {2017-10-04 13:55:52},
  priority = {2},
  title = {{AutoGP}: Exploring the Capabilities and Limitations of Gaussian Process Models},
  year = {2017}
}

@article{GPflow17,
  author = {Matthews, Alexander G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke and {Boukouvalas}, Alexis and {Le{\'{o}}n-Villagr{\'{a}}}, Pablo and Ghahramani, Zoubin and Hensman, James},
  citeulike-article-id = {14445320},
  citeulike-linkout-0 = {http://jmlr.org/papers/v18/16-537.html},
  journal = {Journal of Machine Learning Research},
  keywords = {gaussian\_processes},
  month = apr,
  number = {40},
  pages = {1--6},
  posted-at = {2017-10-04 13:53:13},
  priority = {2},
  title = {{{GP}flow: A {G}aussian process library using {T}ensor{F}low}},
  volume = {18},
  year = {2017}
}

@inproceedings{Cutajar17,
  abstract = {{The composition of multiple Gaussian Processes as a Deep Gaussian Process DGP enables a deep probabilistic nonparametric approach to flexibly tackle complex machine learning problems with sound quantification of uncertainty. Existing inference approaches for DGP models have limited scalability and are notoriously cumbersome to construct. In this work we introduce a novel formulation of DGPs based on random feature expansions that we train using stochastic variational inference. This yields a practical learning framework which significantly advances the state-of-the-art in inference for DGPs, and enables accurate quantification of uncertainty. We extensively showcase the scalability and performance of our proposal on several datasets with up to 8 million observations, and various DGP architectures with up to 30 hidden layers.}},
  address = {International Convention Centre, Sydney, Australia},
  author = {Cutajar, Kurt and Bonilla, Edwin V. and Michiardi, Pietro and Filippone, Maurizio},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  citeulike-article-id = {14445318},
  citeulike-linkout-0 = {http://proceedings.mlr.press/v70/cutajar17a.html},
  editor = {Precup, Doina and Teh, Yee W.},
  keywords = {deep\_learning},
  month = aug,
  pages = {884--893},
  posted-at = {2017-10-04 13:49:30},
  priority = {2},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Random Feature Expansions for Deep {G}aussian Processes},
  volume = {70},
  year = {2017}
}

@misc{Kurakin17,
  abstract = {{Most existing machine learning classifiers are highly vulnerable to
adversarial examples. An adversarial example is a sample of input data which
has been modified very slightly in a way that is intended to cause a machine
learning classifier to misclassify it. In many cases, these modifications can
be so subtle that a human observer does not even notice the modification at
all, yet the classifier still makes a mistake. Adversarial examples pose
security concerns because they could be used to perform an attack on machine
learning systems, even if the adversary has no access to the underlying model.
Up to now, all previous work have assumed a threat model in which the adversary
can feed data directly into the machine learning classifier. This is not always
the case for systems operating in the physical world, for example those which
are using signals from cameras and other sensors as an input. This paper shows
that even in such physical world scenarios, machine learning systems are
vulnerable to adversarial examples. We demonstrate this by feeding adversarial
images obtained from cell-phone camera to an ImageNet Inception classifier and
measuring the classification accuracy of the system. We find that a large
fraction of adversarial examples are classified incorrectly even when perceived
through the camera.}},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  citeulike-article-id = {14260750},
  citeulike-linkout-0 = {http://arxiv.org/abs/1607.02533},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1607.02533},
  day = {11},
  eprint = {1607.02533},
  keywords = {deep\_learning},
  month = feb,
  note = {arXiv:1607.02533},
  posted-at = {2017-10-03 09:25:43},
  priority = {2},
  title = {{Adversarial examples in the physical world}},
  year = {2017}
}

@inproceedings{Calandra16,
  author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl E. and Deisenroth, Marc P.},
  booktitle = {2016 International Joint Conference on Neural Networks, {IJCNN} 2016, Vancouver, BC, Canada, July 24-29, 2016},
  citeulike-article-id = {14444486},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/IJCNN.2016.7727626},
  citeulike-linkout-1 = {https://doi.org/10.1109/IJCNN.2016.7727626},
  keywords = {gaussian\_processes},
  pages = {3338--3345},
  posted-at = {2017-10-03 09:09:47},
  priority = {2},
  title = {{Manifold Gaussian Processes for regression}},
  year = {2016}
}

@inproceedings{Titsias14,
  abstract = {{We propose a simple and effective variational inference algorithm based on stochastic optimisation that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference problems such as variable selection in logistic regression and fully Bayesian inference over kernel hyperparameters in Gaussian process regression.}},
  author = {Titsias, Michalis and L\'{a}zaro-gredilla, Miguel},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  citeulike-article-id = {14443888},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v32/titsias14.pdf},
  editor = {Jebara, Tony and Xing, Eric P.},
  keywords = {variational\_inference},
  pages = {1971--1979},
  posted-at = {2017-10-02 16:47:15},
  priority = {2},
  publisher = {JMLR Workshop and Conference Proceedings},
  title = {{Doubly Stochastic Variational Bayes for non-Conjugate Inference}},
  year = {2014}
}

@inproceedings{Rezende14,
  abstract = {{We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation -- rules for gradient backpropagation through stochastic variables -- and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}},
  author = {Rezende, Danilo J. and Mohamed, Shakir and Wierstra, Daan},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  citeulike-article-id = {14443887},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v32/rezende14.pdf},
  editor = {Jebara, Tony and Xing, Eric P.},
  keywords = {deep\_learning},
  pages = {1278--1286},
  posted-at = {2017-10-02 16:46:05},
  priority = {2},
  publisher = {JMLR Workshop and Conference Proceedings},
  title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
  year = {2014}
}

@inproceedings{Paisley12,
  address = {New York, NY, USA},
  author = {Blei, David M. and Jordan, Michael I. and Paisley, John W.},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  citeulike-article-id = {14443886},
  citeulike-linkout-0 = {http://icml.cc/2012/papers/687.pdf},
  editor = {Langford, John and Pineau, Joelle},
  keywords = {variational\_inference},
  pages = {1367--1374},
  posted-at = {2017-10-02 16:44:56},
  priority = {2},
  publisher = {ACM},
  title = {{Variational Bayesian Inference with Stochastic Search}},
  year = {2012}
}

@article{Cawley10,
  author = {Cawley, Gavin C. and Talbot, Nicola L. C.},
  citeulike-article-id = {14443885},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=1756006.1859921},
  journal = {J. Mach. Learn. Res.},
  keywords = {model\_selection},
  month = aug,
  pages = {2079--2107},
  posted-at = {2017-10-02 16:42:15},
  priority = {2},
  publisher = {JMLR.org},
  title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
  volume = {11},
  year = {2010}
}

@misc{Abadi15,
  author = {{Abadi et al}},
  citeulike-article-id = {14443884},
  citeulike-linkout-0 = {https://www.tensorflow.org/},
  keywords = {deep\_learning},
  note = {Software available from tensorflow.org},
  posted-at = {2017-10-02 16:41:17},
  priority = {2},
  title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year = {2015}
}

@article{Yang15b,
  address = {Los Alamitos, CA, USA},
  author = {Yang, Zichao and Moczulski, Marcin and Denil, Misha and de Freitas, Nando and Smola, Alex and Song, Le and Wang, Ziyu},
  citeulike-article-id = {14443882},
  citeulike-linkout-0 = {http://dx.doi.org/doi.ieeecomputersociety.org/10.1109/ICCV.2015.173},
  journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
  keywords = {deep\_learning},
  pages = {1476--1483},
  posted-at = {2017-10-02 16:38:57},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Deep Fried Convnets}},
  volume = {00},
  year = {2015}
}

@incollection{Wilson16,
  author = {Wilson, Andrew G. and Hu, Zhiting and Salakhutdinov, Ruslan R. and Xing, Eric P.},
  booktitle = {Advances in Neural Information Processing Systems 29},
  citeulike-article-id = {14443881},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  keywords = {deep\_learning},
  pages = {2586--2594},
  posted-at = {2017-10-02 16:34:37},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Stochastic Variational Deep Kernel Learning}},
  year = {2016}
}

@incollection{Yu16,
  author = {Yu, Felix X. and Suresh, Ananda T. and Choromanski, Krzysztof M. and Holtmann-Rice, Daniel N. and Kumar, Sanjiv},
  booktitle = {Advances in Neural Information Processing Systems 29},
  citeulike-article-id = {14443880},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/6246-orthogonal-random-features.pdf},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  keywords = {kernel},
  pages = {1975--1983},
  posted-at = {2017-10-02 16:32:35},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Orthogonal Random Features}},
  year = {2016}
}

@inproceedings{Le13,
  author = {Le, Quoc and Sarlos, Tamas and Smola, Alex},
  booktitle = {30th International Conference on Machine Learning (ICML)},
  citeulike-article-id = {14443877},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v28/le13.html},
  keywords = {kernel},
  posted-at = {2017-10-02 16:29:21},
  priority = {2},
  title = {{Fastfood - Approximating Kernel Expansions in Loglinear Time}},
  year = {2013}
}

@inproceedings{Gal16,
  author = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
  citeulike-article-id = {14443874},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=3045390.3045502},
  keywords = {deep\_learning},
  location = {New York, NY, USA},
  pages = {1050--1059},
  posted-at = {2017-10-02 16:22:16},
  priority = {2},
  publisher = {JMLR.org},
  series = {ICML'16},
  title = {{Dropout As a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
  year = {2016}
}

@misc{Gal16b,
  abstract = {{Convolutional neural networks (CNNs) work well on large datasets. But
labelled data is hard to collect, and in some applications larger amounts of
data are not available. The problem then is how to use CNNs with small data --
as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better
robustness to over-fitting on small data than traditional approaches. This is
by placing a probability distribution over the CNN's kernels. We approximate
our model's intractable posterior with Bernoulli variational distributions,
requiring no additional model parameters.


On the theoretical side, we cast dropout network training as approximate
inference in Bayesian neural networks. This allows us to implement our model
using existing tools in deep learning with no increase in time complexity,
while highlighting a negative result in the field. We show a considerable
improvement in classification accuracy compared to standard techniques and
improve on published state-of-the-art results for CIFAR-10.}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  citeulike-article-id = {14339219},
  citeulike-linkout-0 = {http://arxiv.org/abs/1506.02158},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1506.02158},
  day = {18},
  eprint = {1506.02158},
  keywords = {deep\_learning},
  month = jan,
  note = {arXiv:1506.02158},
  posted-at = {2017-10-02 16:21:08},
  priority = {2},
  title = {{Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference}},
  year = {2016}
}

@article{Srivastava14,
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  citeulike-article-id = {14443872},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
  journal = {Journal of Machine Learning Research},
  keywords = {combination, deep, deep\_learning, learning, model, networks, neural, regularization},
  month = jan,
  number = {1},
  pages = {1929--1958},
  posted-at = {2017-10-02 16:19:31},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
  volume = {15},
  year = {2014}
}

@inproceedings{Szegedy15,
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  citeulike-article-id = {14063674},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/cvpr.2015.7298594},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=7298594},
  institution = {Google Inc., Mountain View, CA, USA},
  isbn = {978-1-4673-6964-0},
  issn = {1063-6919},
  keywords = {cnn, deep\_learning},
  location = {Boston, MA, USA},
  month = jun,
  pages = {1--9},
  posted-at = {2017-10-02 16:17:47},
  priority = {2},
  publisher = {IEEE},
  title = {{Going deeper with convolutions}},
  year = {2015}
}

@article{Simonyan14,
  author = {Simonyan, Karen and Zisserman, Andrew},
  citeulike-article-id = {14443871},
  citeulike-linkout-0 = {http://arxiv.org/abs/1409.1556},
  eprint = {1409.1556},
  journal = {CoRR},
  keywords = {deep\_learning},
  posted-at = {2017-10-02 16:15:23},
  priority = {2},
  title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
  volume = {abs/1409.1556},
  year = {2014}
}

@inproceedings{Krizhevsky12,
  address = {USA},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
  citeulike-article-id = {14443869},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
  keywords = {deep\_learning},
  location = {Lake Tahoe, Nevada},
  pages = {1097--1105},
  posted-at = {2017-10-02 16:10:31},
  priority = {2},
  publisher = {Curran Associates Inc.},
  series = {NIPS'12},
  title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
  year = {2012}
}

@article{LeCun89,
  address = {Cambridge, MA, USA},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  citeulike-article-id = {14443868},
  citeulike-linkout-0 = {http://dx.doi.org/10.1162/neco.1989.1.4.541},
  journal = {Neural Comput.},
  keywords = {deep\_learning},
  month = dec,
  number = {4},
  pages = {541--551},
  posted-at = {2017-10-02 16:08:50},
  priority = {2},
  publisher = {MIT Press},
  title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
  volume = {1},
  year = {1989}
}

@incollection{Bonilla08,
  address = {Cambridge, MA},
  author = {Bonilla, Edwin and Chai, Kian M. and Williams, Chris},
  booktitle = {Advances in Neural Information Processing Systems 20},
  citeulike-article-id = {14426317},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S.},
  keywords = {gaussian\_processes},
  pages = {153--160},
  posted-at = {2017-09-06 15:01:37},
  priority = {2},
  publisher = {MIT Press},
  title = {{Multi-task Gaussian Process Prediction}},
  year = {2008}
}

@article{Lawrence05,
  author = {Lawrence, Neil},
  citeulike-article-id = {14426308},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=1046920.1194904},
  journal = {Journal of Machine Learning Research},
  keywords = {gaussian\_processes},
  month = dec,
  pages = {1783--1816},
  posted-at = {2017-09-06 14:41:41},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models}},
  volume = {6},
  year = {2005}
}

@misc{FitzsimonsBLD17,
  abstract = {{The log-determinant of a kernel matrix appears in a variety of machine
learning problems, ranging from determinantal point processes and generalized
Markov random fields, through to the training of Gaussian processes. Exact
calculation of this term is often intractable when the size of the kernel
matrix exceeds a few thousand. In the spirit of probabilistic numerics, we
reinterpret the problem of computing the log-determinant as a Bayesian
inference problem. In particular, we combine prior knowledge in the form of
bounds from matrix theory and evidence derived from stochastic trace estimation
to obtain probabilistic estimates for the log-determinant and its associated
uncertainty within a given computational budget. Beyond its novelty and
theoretic appeal, the performance of our proposal is competitive with
state-of-the-art approaches to approximating the log-determinant, while also
quantifying the uncertainty due to budget-constrained evidence.}},
  author = {Fitzsimons, Jack and Cutajar, Kurt and Osborne, Michael and Roberts, Stephen and Filippone, Maurizio},
  citeulike-article-id = {14332910},
  citeulike-linkout-0 = {http://arxiv.org/abs/1704.01445},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1704.01445},
  day = {5},
  eprint = {1704.01445},
  keywords = {gaussian\_processes},
  month = apr,
  note = {arXiv:1704.01445},
  posted-at = {2017-04-06 14:53:23},
  priority = {2},
  title = {{Bayesian Inference of Log Determinants}},
  year = {2017}
}

@proceedings{DBLP:conf/icml/2016c,
  citeulike-article-id = {14307298},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v48/},
  editor = {Balcan, Maria{-}Florina and Weinberger, Kilian Q.},
  keywords = {gaussian\_process},
  posted-at = {2017-03-08 10:24:57},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  volume = {48},
  year = {2016}
}

@inproceedings{Cutajar16,
  author = {Cutajar, Kurt and Osborne, Michael and Cunningham, John and Filippone, Maurizio},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  citeulike-article-id = {14307297},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v48/cutajar16.html},
  editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
  keywords = {gaussian\_process},
  pages = {2529--2538},
  posted-at = {2017-03-08 10:24:56},
  priority = {2},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  title = {{Preconditioning Kernel Matrices}},
  volume = {48},
  year = {2016}
}

@article{Ambikasaran16,
  address = {Los Alamitos, CA, USA},
  author = {Ambikasaran, Sivaram and Foreman-Mackey, Daniel and Greengard, Leslie and Hogg, David W. and O'Neil, Michael},
  citeulike-article-id = {14307292},
  citeulike-linkout-0 = {http://dx.doi.org/doi.ieeecomputersociety.org/10.1109/TPAMI.2015.2448083},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {gaussian\_processes},
  number = {2},
  pages = {252--265},
  posted-at = {2017-03-08 10:11:21},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Fast Direct Methods for Gaussian Processes}},
  volume = {38},
  year = {2016}
}

@misc{Ipsen11,
  abstract = {{A sequence of approximations for the determinant and its logarithm of a
complex matrixis derived, along with relative error bounds. The determinant
approximations are derived from expansions of det(X)=exp(trace(log(X))), and
they apply to non-Hermitian matrices. Examples illustrate that these
determinant approximations are efficient for lattice simulations of finite
temperature nuclear matter, and that they use significantly less space than
Gaussian elimination. The first approximation in the sequence is a block
diagonal approximation; it represents an extension of Fischer's and Hadamard's
inequalities to non-Hermitian matrices. In the special case of Hermitian
positive-definite matrices, block diagonal approximations can be competitive
with sparse inverse approximations. At last, a different representation of
sparse inverse approximations is given and it is shown that their accuracy
increases as more matrix elements are included.}},
  author = {Ipsen, Ilse C. F. and Lee, Dean J.},
  citeulike-article-id = {14307289},
  citeulike-linkout-0 = {http://arxiv.org/abs/1105.0437},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1105.0437},
  day = {2},
  eprint = {1105.0437},
  keywords = {algebra},
  month = may,
  posted-at = {2017-03-08 10:06:36},
  priority = {2},
  title = {{Determinant Approximations}},
  year = {2011}
}

@article{Braun06,
  author = {Braun, Mikio L.},
  citeulike-article-id = {14307288},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=1248547.1248629},
  journal = {Journal of Machine Learning Research},
  keywords = {algebra},
  month = dec,
  pages = {2303--2328},
  posted-at = {2017-03-08 10:01:18},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Accurate Error Bounds for the Eigenvalues of the Kernel Matrix}},
  volume = {7},
  year = {2006}
}

@inproceedings{Bardenet15,
  address = {Cambridge, MA, USA},
  author = {Bardenet, R{\'{e}}mi and Titsias, Michalis K.},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems},
  citeulike-article-id = {14307285},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=2969442.2969618},
  keywords = {gaussian\_process},
  location = {Montreal, Canada},
  pages = {3393--3401},
  posted-at = {2017-03-08 09:56:58},
  priority = {2},
  publisher = {MIT Press},
  series = {NIPS'15},
  title = {{Inference for Determinantal Point Processes Without Spectral Knowledge}},
  year = {2015}
}

@inproceedings{Peng15,
  author = {Peng, Hao and Qi, Yuan},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
  citeulike-article-id = {14307278},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=2832747.2832774},
  keywords = {gaussian\_process},
  location = {Buenos Aires, Argentina},
  pages = {3763--3769},
  posted-at = {2017-03-08 09:52:06},
  priority = {2},
  publisher = {AAAI Press},
  series = {IJCAI'15},
  title = {{EigenGP: Gaussian Process Models with Adaptive Eigenfunctions}},
  year = {2015}
}

@proceedings{DBLP:conf/icml/2015d,
  citeulike-article-id = {14307274},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v37/},
  editor = {Bach, Francis R. and Blei, David M.},
  keywords = {algebra},
  posted-at = {2017-03-08 09:46:07},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  volume = {37},
  year = {2015}
}

@inproceedings{Han15,
  author = {Han, Insu and Malioutov, Dmitry and Shin, Jinwoo},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  citeulike-article-id = {14307273},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v37/hana15.html},
  editor = {Bach, Francis R. and Blei, David M.},
  keywords = {algebra},
  pages = {908--917},
  posted-at = {2017-03-08 09:46:07},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {{Large-scale log-determinant computation through stochastic Chebyshev expansions}},
  volume = {37},
  year = {2015}
}

@article{Ubaru16,
  author = {Ubaru, Shashanka and Chen, Jie and Saad, Yousef},
  citeulike-article-id = {14307269},
  keywords = {algebra},
  posted-at = {2017-03-08 09:41:51},
  priority = {2},
  title = {FAST ESTIMATION OF tr (F (A)) VIA STOCHASTIC {L}ANCZOS QUADRATURE},
  year = {2016}
}

@misc{Ozdemir16,
  abstract = {{We propose a flexible procedure for large-scale image search by hash
functions with kernels. Our method treats binary codes and pairwise semantic
similarity as latent and observed variables, respectively, in a probabilistic
model based on Gaussian processes for binary classification. We present an
efficient inference algorithm with the sparse pseudo-input Gaussian process
(SPGP) model and parallelization. Experiments on three large-scale image
dataset demonstrate the effectiveness of the proposed hashing method, Gaussian
Process Hashing (GPH), for short binary codes and the datasets without
predefined classes in comparison to the state-of-the-art supervised hashing
methods.}},
  author = {Ozdemir, Bahadir and Davis, Larry S.},
  citeulike-article-id = {14155953},
  citeulike-linkout-0 = {http://arxiv.org/abs/1604.07335},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1604.07335},
  day = {25},
  eprint = {1604.07335},
  keywords = {gaussian\_processes},
  month = apr,
  posted-at = {2016-10-08 22:18:22},
  priority = {2},
  title = {{Scalable Gaussian Processes for Supervised Hashing}},
  year = {2016}
}

@inproceedings{Simard03,
  address = {Washington, DC, USA},
  author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, John C.},
  booktitle = {Proceedings of the Seventh International Conference on Document Analysis and Recognition - Volume 2},
  citeulike-article-id = {14155946},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=938980.939477},
  keywords = {deep\_neural\_nets},
  posted-at = {2016-10-08 21:55:07},
  priority = {2},
  publisher = {IEEE Computer Society},
  series = {ICDAR '03},
  title = {{Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis}},
  year = {2003}
}

@proceedings{DBLP:conf/nips/2014,
  citeulike-article-id = {14155944},
  citeulike-linkout-0 = {http://papers.nips.cc/book/advances-in-neural-information-processing-systems-27-2014},
  editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
  keywords = {gaussian\_processes},
  posted-at = {2016-10-08 21:42:20},
  priority = {2},
  title = {{Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada}},
  year = {2014}
}

@inproceedings{Gal14,
  author = {Gal, Yarin and van der Wilk, Mark and Rasmussen, Carl E.},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada},
  citeulike-article-id = {14155943},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/5593-distributed-variational-inference-in-sparse-gaussian-process-regression-and-latent-variable-models},
  editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
  keywords = {gaussian\_processes},
  pages = {3257--3265},
  posted-at = {2016-10-08 21:42:20},
  priority = {2},
  title = {{Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models}},
  year = {2014}
}

@misc{Dai15,
  abstract = {{We develop a scalable deep non-parametric generative model by augmenting deep
Gaussian processes with a recognition model. Inference is performed in a novel
scalable variational framework where the variational posterior distributions
are reparametrized through a multilayer perceptron. The key aspect of this
reformulation is that it prevents the proliferation of variational parameters
which otherwise grow linearly in proportion to the sample size. We derive a new
formulation of the variational lower bound that allows us to distribute most of
the computation in a way that enables to handle datasets of the size of
mainstream deep learning tasks. We show the efficacy of the method on a variety
of challenges including deep unsupervised learning and deep Bayesian
optimization.}},
  author = {Dai, Zhenwen and Damianou, Andreas and Gonz\'{a}lez, Javier and Lawrence, Neil},
  citeulike-article-id = {13885566},
  citeulike-linkout-0 = {http://arxiv.org/abs/1511.06455},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1511.06455},
  day = {29},
  eprint = {1511.06455},
  keywords = {deep\_gps},
  month = feb,
  posted-at = {2016-10-06 21:47:36},
  priority = {2},
  title = {{Variational Auto-encoded Deep Gaussian Processes}},
  year = {2016}
}

@inproceedings{Sopena99,
  author = {Sopena, J. M. and Romero, E. and Alquezar, R.},
  booktitle = {Artificial Neural Networks, 1999. ICANN 99. Ninth International Conference on (Conf. Publ. No. 470)},
  citeulike-article-id = {14153071},
  citeulike-linkout-0 = {http://dx.doi.org/10.1049/cp:19991129},
  keywords = {deep\_neural\_nets},
  posted-at = {2016-10-06 21:36:30},
  priority = {2},
  title = {{Neural networks with periodic and monotonic activation functions: a comparative study in classification problems}},
  volume = {1},
  year = {1999}
}

@proceedings{DBLP:conf/nips/2015,
  citeulike-article-id = {14153063},
  citeulike-linkout-0 = {http://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  keywords = {deep\_neural\_nets},
  posted-at = {2016-10-06 21:14:32},
  priority = {2},
  title = {{Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada}},
  year = {2015}
}

@inproceedings{Novikov15,
  author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry P.},
  booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
  citeulike-article-id = {14153062},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/5787-tensorizing-neural-networks},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  keywords = {deep\_neural\_nets},
  pages = {442--450},
  posted-at = {2016-10-06 21:14:32},
  priority = {2},
  title = {{Tensorizing Neural Networks}},
  year = {2015}
}

@proceedings{DBLP:conf/icassp/2013,
  citeulike-article-id = {14153061},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6619549},
  keywords = {deep\_neural\_nets},
  posted-at = {2016-10-06 21:11:03},
  priority = {2},
  publisher = {{IEEE}},
  title = {{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2013, Vancouver, BC, Canada, May 26-31, 2013},
  year = {2013}
}

@inproceedings{Sainath13,
  author = {Sainath, Tara N. and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2013, Vancouver, BC, Canada, May 26-31, 2013},
  citeulike-article-id = {14153060},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICASSP.2013.6638949},
  keywords = {deep\_neural\_nets},
  pages = {6655--6659},
  posted-at = {2016-10-06 21:11:03},
  priority = {2},
  publisher = {{IEEE}},
  title = {{Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets}},
  year = {2013}
}

@proceedings{DBLP:conf/nips/2013,
  citeulike-article-id = {14153059},
  citeulike-linkout-0 = {http://papers.nips.cc/book/advances-in-neural-information-processing-systems-26-2013},
  editor = {Burges, Christopher J. C. and Bottou, L{\'{e}}on and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  keywords = {deep\_neural\_nets},
  posted-at = {2016-10-06 21:09:25},
  priority = {2},
  title = {{Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States}},
  year = {2013}
}

@inproceedings{Denil13,
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and de Freitas, Nando},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.},
  citeulike-article-id = {14153058},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning},
  editor = {Burges, Christopher J. C. and Bottou, L{\'{e}}on and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  keywords = {deep\_neural\_nets},
  pages = {2148--2156},
  posted-at = {2016-10-06 21:09:25},
  priority = {2},
  title = {{Predicting Parameters in Deep Learning}},
  year = {2013}
}

@misc{Tran16,
  abstract = {{Variational inference is a powerful tool for approximate inference, and it
has been recently applied for representation learning with deep generative
models. We develop the variational Gaussian process (VGP), a Bayesian
nonparametric variational family, which adapts its shape to match complex
posterior distributions. The VGP generates approximate posterior samples by
generating latent inputs and warping them through random non-linear mappings;
the distribution over random mappings is learned during inference, enabling the
transformed outputs to adapt to varying complexity. We prove a universal
approximation theorem for the VGP, demonstrating its representative power for
learning any model. For inference we present a variational objective inspired
by auto-encoders and perform black box inference over a wide class of models.
The VGP achieves new state-of-the-art results for unsupervised learning,
inferring models such as the deep latent Gaussian model and the recently
proposed DRAW.}},
  author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
  citeulike-article-id = {13847437},
  citeulike-linkout-0 = {http://arxiv.org/abs/1511.06499},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1511.06499},
  day = {17},
  eprint = {1511.06499},
  keywords = {gaussian\_processes},
  month = apr,
  posted-at = {2016-08-04 10:18:58},
  priority = {2},
  title = {{The Variational Gaussian Process}},
  year = {2016}
}

@proceedings{DBLP:conf/icml/2015a,
  citeulike-article-id = {14106773},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v37/},
  editor = {Bach, Francis R. and Blei, David M.},
  keywords = {gaussian\_processes},
  posted-at = {2016-08-03 10:47:59},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  volume = {37},
  year = {2015}
}

@inproceedings{Gal15,
  author = {Gal, Yarin and Turner, Richard},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  citeulike-article-id = {14106772},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v37/galb15.html},
  editor = {Bach, Francis R. and Blei, David M.},
  keywords = {gaussian\_processes},
  pages = {655--664},
  posted-at = {2016-08-03 10:47:59},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {{Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs}},
  volume = {37},
  year = {2015}
}

@proceedings{DBLP:conf/icml/2015,
  citeulike-article-id = {14105680},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v37/},
  editor = {Bach, Francis R. and Blei, David M.},
  keywords = {deep\_nets},
  posted-at = {2016-08-01 14:20:10},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  volume = {37},
  year = {2015}
}

@inproceedings{Blundell15,
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  citeulike-article-id = {14105679},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v37/blundell15.html},
  editor = {Bach, Francis R. and Blei, David M.},
  keywords = {deep\_nets},
  pages = {1613--1622},
  posted-at = {2016-08-01 14:20:10},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {{Weight Uncertainty in Neural Network}},
  volume = {37},
  year = {2015}
}

@proceedings{DBLP:conf/icml/2016b,
  citeulike-article-id = {14105677},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v48/},
  editor = {Balcan, Maria{-}Florina and Weinberger, Kilian Q.},
  keywords = {deep\_nets},
  posted-at = {2016-08-01 14:16:43},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  volume = {48},
  year = {2016}
}

@proceedings{DBLP:conf/aistats/2014,
  citeulike-article-id = {14105674},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v33/},
  keywords = {deep\_gps},
  posted-at = {2016-08-01 14:15:03},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2014, Reykjavik, Iceland, April 22-25, 2014},
  volume = {33},
  year = {2014}
}

@inproceedings{Duvenaud14,
  author = {Duvenaud, David K. and Rippel, Oren and Adams, Ryan P. and Ghahramani, Zoubin},
  booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2014, Reykjavik, Iceland, April 22-25, 2014},
  citeulike-article-id = {14105673},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v33/duvenaud14.html},
  keywords = {deep\_gps},
  pages = {202--210},
  posted-at = {2016-08-01 14:15:03},
  priority = {2},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  title = {{Avoiding pathologies in very deep networks}},
  volume = {33},
  year = {2014}
}

@proceedings{DBLP:conf/icml/2016,
  citeulike-article-id = {14105671},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v48/},
  editor = {Balcan, Maria{-}Florina and Weinberger, Kilian Q.},
  keywords = {deep\_gps},
  posted-at = {2016-08-01 14:13:03},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Workshop and Conference Proceedings},
  title = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  volume = {48},
  year = {2016}
}

@inproceedings{Bui16,
  author = {Bui, Thang D. and Hern\'{a}ndez-Lobato, Daniel and Hern\'{a}ndez-Lobato, Jos\'{e} M. and Li, Yingzhen and Turner, Richard E.},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  citeulike-article-id = {14105670},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v48/bui16.html},
  editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
  keywords = {deep\_gps},
  pages = {1472--1481},
  posted-at = {2016-08-01 14:13:02},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Deep Gaussian Processes for Regression using Approximate Expectation Propagation}},
  volume = {48},
  year = {2016}
}

@inproceedings{Kingma14,
  author = {Kingma, Diederik P. and Welling, Max},
  booktitle = {Proceedings of the Second International Conference on Learning Representations (ICLR 2014)},
  citeulike-article-id = {14105668},
  keywords = {deep\_nets},
  month = apr,
  posted-at = {2016-08-01 14:06:32},
  priority = {2},
  title = {{Auto-Encoding Variational Bayes}},
  year = {2014}
}

@incollection{Graves11,
  author = {Graves, Alex},
  booktitle = {Advances in Neural Information Processing Systems 24},
  citeulike-article-id = {14105665},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  keywords = {deep\_nets},
  pages = {2348--2356},
  posted-at = {2016-08-01 14:00:40},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Practical Variational Inference for Neural Networks}},
  year = {2011}
}

@article{Chang11,
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  citeulike-article-id = {14035370},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  keywords = {svm},
  note = {Software available at \\url{http://www.csie.ntu.edu.tw/\~{}cjlin/libsvm}},
  posted-at = {2016-05-15 13:01:35},
  priority = {2},
  title = {{LIBSVM}: A library for support vector machines},
  volume = {2},
  year = {2011}
}

@article{vonLuxburg07,
  abstract = {{In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.}},
  author = {von Luxburg, Ulrike},
  citeulike-article-id = {14035341},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s11222-007-9033-z},
  journal = {Statistics and Computing},
  keywords = {clustering},
  number = {4},
  pages = {395--416},
  posted-at = {2016-05-15 11:09:01},
  priority = {2},
  title = {{A tutorial on spectral clustering}},
  volume = {17},
  year = {2007}
}

@article{Shi00,
  author = {Shi, Jianbo and Malik, Jitendra},
  citeulike-article-id = {14035339},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/34.868688},
  journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {clustering},
  number = {8},
  pages = {888--905},
  posted-at = {2016-05-15 11:06:59},
  priority = {2},
  title = {{Normalized Cuts and Image Segmentation}},
  volume = {22},
  year = {2000}
}

@proceedings{DBLP:conf/aaai/2016,
  citeulike-article-id = {14033229},
  citeulike-linkout-0 = {http://www.aaai.org/Library/AAAI/aaai16contents.php},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  keywords = {clustering},
  posted-at = {2016-05-11 13:07:38},
  priority = {2},
  publisher = {{AAAI} Press},
  title = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {USA}},
  year = {2016}
}

@inproceedings{Li16,
  author = {Li, Yeqing and Huang, Junzhou and Liu, Wei},
  booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {USA.}},
  citeulike-article-id = {14033228},
  citeulike-linkout-0 = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12396},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  keywords = {clustering},
  pages = {1809--1815},
  posted-at = {2016-05-11 13:07:38},
  priority = {2},
  publisher = {{AAAI} Press},
  title = {{Scalable Sequential Spectral Clustering}},
  year = {2016}
}

@misc{Smith14,
  abstract = {The techniques and analysis presented in this paper provide new methods to
solve optimization problems posed on Riemannian manifolds. A new point of view
is offered for the solution of constrained optimization problems. Some
classical optimization techniques on Euclidean space are generalized to
Riemannian manifolds. Several algorithms are presented and their convergence
properties are analyzed employing the Riemannian structure of the manifold.
Specifically, two apparently new algorithms, which can be thought of as
Newton's method and the conjugate gradient method on Riemannian manifolds, are
presented and shown to possess, respectively, quadratic and superlinear
convergence. Examples of each method on certain Riemannian manifolds are given
with the results of numerical experiments. Rayleigh's quotient defined on the
sphere is one example. It is shown that Newton's method applied to this
function converges cubically, and that the Rayleigh quotient iteration is an
efficient approximation of Newton's method. The Riemannian version of the
conjugate gradient method applied to this function gives a new algorithm for
finding the eigenvectors corresponding to the extreme eigenvalues of a
symmetric matrix. Another example arises from extremizing the function
\$\mathop{\rm tr} {\Theta}^{\scriptscriptstyle\rm T}Q{\Theta}N\$ on the special
orthogonal group. In a similar example, it is shown that Newton's method
applied to the sum of the squares of the off-diagonal entries of a symmetric
matrix converges cubically.},
  author = {Smith, Steven T.},
  citeulike-article-id = {14033193},
  citeulike-linkout-0 = {http://arxiv.org/abs/1407.5965},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1407.5965},
  day = {22},
  eprint = {1407.5965},
  keywords = {optimization},
  month = jul,
  note = {arXiv:1407.5965},
  posted-at = {2016-05-11 12:32:11},
  priority = {2},
  title = {{Optimization Techniques on Riemannian Manifolds}},
  year = {2014}
}

@article{Fowlkes04,
  address = {Washington, DC, USA},
  author = {Fowlkes, Charless and Belongie, Serge and Chung, Fan and Malik, Jitendra},
  citeulike-article-id = {14033184},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TPAMI.2004.1262185},
  journal = {IEEE Transactions on Pattern Analysis and Machince Intelligence},
  keywords = {and, approximation, clustering, cuts, graph, image, normalized, nystrom, segmentation, spectral, theory, video},
  month = jan,
  number = {2},
  pages = {214--225},
  posted-at = {2016-05-11 12:24:29},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Spectral Grouping Using the Nystr\"{o}M Method}},
  volume = {26},
  year = {2004}
}

@inproceedings{Yan09,
  address = {New York, NY, USA},
  author = {Yan, Donghui and Huang, Ling and Jordan, Michael I.},
  booktitle = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  citeulike-article-id = {14033177},
  citeulike-linkout-0 = {http://dx.doi.org/10.1145/1557019.1557118},
  citeulike-linkout-1 = {http://doi.acm.org/10.1145/1557019.1557118},
  keywords = {clustering, data, learning, quantization, spectral, unsupervised},
  location = {Paris, France},
  pages = {907--916},
  posted-at = {2016-05-11 12:19:33},
  priority = {2},
  publisher = {ACM},
  series = {KDD '09},
  title = {{Fast Approximate Spectral Clustering}},
  year = {2009}
}

@inproceedings{Boutsidis15,
  author = {Boutsidis, Christos and Kambadur, Prabhanjan and Gittens, Alex},
  booktitle = {{Proceedings of the 32nd International Conference on Machine Learning}},
  citeulike-article-id = {14033171},
  keywords = {clustering},
  pages = {40--48},
  posted-at = {2016-05-11 12:11:34},
  priority = {2},
  publisher = {{JMLR.org}},
  series = {{JMLR Proceedings}},
  title = {{Spectral Clustering via the Power Method — Provably}},
  volume = {37},
  year = {2015}
}

@article{Wen13,
  abstract = {Minimization with orthogonality constraints (e.g., {\\$}{\\$}X^{\\backslash}top X = I{\\$}{\\$} ) and/or spherical constraints (e.g., {\\$}{\\$}{\\backslash}Vert x{\\backslash}Vert {\_}2 = 1{\\$}{\\$} ) has wide applications in polynomial optimization, combinatorial optimization, eigenvalue problems, sparse PCA, p-harmonic flows, 1-bit compressive sensing, matrix rank minimization, etc. These problems are difficult because the constraints are not only non-convex but numerically expensive to preserve during iterations. To deal with these difficulties, we apply the Cayley transform---a Crank-Nicolson-like update scheme---to preserve the constraints and based on it, develop curvilinear search algorithms with lower flops compared to those based on projections and geodesics. The efficiency of the proposed algorithms is demonstrated on a variety of test problems. In particular, for the maxcut problem, it exactly solves a decomposition formulation for the SDP relaxation. For polynomial optimization, nearest correlation matrix estimation and extreme eigenvalue problems, the proposed algorithms run very fast and return solutions no worse than those from their state-of-the-art algorithms. For the quadratic assignment problem, a gap 0.842 {\%} to the best known solution on the largest problem ``tai256c'' in QAPLIB can be reached in 5 min on a typical laptop.},
  author = {Wen, Zaiwen and Yin, Wotao},
  citeulike-article-id = {14030402},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10107-012-0584-1},
  journal = {Mathematical Programming},
  keywords = {clustering},
  number = {1},
  pages = {397--434},
  posted-at = {2016-05-09 08:51:32},
  priority = {2},
  title = {{A feasible method for optimization with orthogonality constraints}},
  volume = {142},
  year = {2013}
}

@article{Bonnabel13,
  author = {Bonnabel, S.},
  citeulike-article-id = {14021294},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TAC.2013.2254619},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {algorithmnumerical, algorithmsnonlinear, approximation, caseriemannian, covariance, descent, functioncovariance, geometrygradient, geometrystochastic, gradient, identificationriemannian, manifoldcost, matricesdifferential, matricesgossip, methodsstochastic, optimization, processeseuclidian, testingstochastic},
  number = {9},
  pages = {2217--2229},
  posted-at = {2016-04-25 16:25:28},
  priority = {2},
  title = {Stochastic Gradient Descent on {R}iemannian Manifolds},
  volume = {58},
  year = {2013}
}

@phdthesis{Davies14,
  author = {Davies, Alex},
  citeulike-article-id = {13924561},
  keywords = {gaussian\_processes},
  posted-at = {2016-02-03 10:13:48},
  priority = {2},
  school = {University of Cambridge},
  title = {{Effective Implementation of Gaussian Process Regression for Machine Learning}},
  year = {2014}
}

@article{Notay00,
  author = {Notay, Yvan},
  citeulike-article-id = {13924558},
  citeulike-linkout-0 = {http://dx.doi.org/10.1137/S1064827599362314},
  journal = {SIAM Journal on Scientific Computing},
  keywords = {algebra},
  number = {4},
  pages = {1444--1460},
  posted-at = {2016-02-03 10:00:59},
  priority = {2},
  title = {{Flexible Conjugate Gradients}},
  volume = {22},
  year = {2000}
}

@proceedings{DBLP:conf/aistats/2007,
  citeulike-article-id = {13924555},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v2/},
  editor = {Meila, Marina and Shen, Xiaotong},
  keywords = {gaussian\_processes},
  posted-at = {2016-02-03 09:57:49},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Proceedings},
  title = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, {AISTATS} 2007, San Juan, Puerto Rico, March 21-24, 2007},
  volume = {2},
  year = {2007}
}

@inproceedings{Snelson07,
  author = {Snelson, Edward and Ghahramani, Zoubin},
  booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, {AISTATS} 2007, San Juan, Puerto Rico, March 21-24, 2007},
  citeulike-article-id = {13924554},
  citeulike-linkout-0 = {http://www.jmlr.org/proceedings/papers/v2/snelson07a.html},
  editor = {Meila, Marina and Shen, Xiaotong},
  keywords = {gaussian\_processes},
  pages = {524--531},
  posted-at = {2016-02-03 09:57:49},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Proceedings},
  title = {{Local and global sparse Gaussian process approximations}},
  volume = {2},
  year = {2007}
}

@article{Chalupka13,
  author = {Chalupka, Krzysztof and Williams, Christopher K. I. and Murray, Iain},
  citeulike-article-id = {13916364},
  citeulike-linkout-0 = {http://jmlr.csail.mit.edu/papers/v14/chalupka13a.html},
  journal = {Journal of Machine Learning Research},
  keywords = {gaussian\_processes},
  posted-at = {2016-01-25 15:25:21},
  priority = {2},
  title = {A framework for evaluating approximation methods for {G}aussian process regression},
  volume = {14},
  year = {2013}
}

@article{Halko11,
  address = {Philadelphia, PA, USA},
  author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
  citeulike-article-id = {13916357},
  citeulike-linkout-0 = {http://dx.doi.org/10.1137/090771806},
  journal = {SIAM Rev.},
  keywords = {algebra, algorithm, analysis, approximation, component, decomposition, dimension, eigenvalue, factorization, interpolative, johnson-lindenstrauss, lemma, matrix, parallel, pass-efficient, principal, qr, random, randomized, rank-revealing, reduction, singular, streaming, value},
  month = may,
  number = {2},
  pages = {217--288},
  posted-at = {2016-01-25 15:21:12},
  priority = {2},
  publisher = {Society for Industrial and Applied Mathematics},
  title = {{Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions}},
  volume = {53},
  year = {2011}
}

@inproceedings{Wilson15,
  author = {Wilson, Andrew and Nickisch, Hannes},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  citeulike-article-id = {13916352},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v37/wilson15.pdf},
  editor = {Blei, David and Bach, Francis},
  keywords = {gaussian\_processes},
  pages = {1775--1784},
  posted-at = {2016-01-25 15:08:28},
  priority = {2},
  publisher = {JMLR Workshop and Conference Proceedings},
  title = {{Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)}},
  year = {2015}
}

@incollection{Rahimi08b,
  author = {Rahimi, Ali and Recht, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems 20},
  citeulike-article-id = {13916348},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  keywords = {gaussian\_processes},
  pages = {1177--1184},
  posted-at = {2016-01-25 15:04:21},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Random Features for Large-Scale Kernel Machines}},
  year = {2008}
}

@incollection{Yang12,
  author = {Yang, Tianbao and Li, Yu-feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},
  booktitle = {Advances in Neural Information Processing Systems 25},
  citeulike-article-id = {13916346},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  keywords = {gaussian\_processes},
  pages = {476--484},
  posted-at = {2016-01-25 15:01:27},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Nystr\"{o}m Method vs Random Fourier Features: A Theoretical and Empirical Comparison}},
  year = {2012}
}

@article{Calderhead14,
  author = {Calderhead, Ben},
  citeulike-article-id = {13846989},
  citeulike-linkout-0 = {http://dx.doi.org/10.1073/pnas.1408184111},
  journal = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA},
  keywords = {mcmc},
  pages = {17408--17413},
  posted-at = {2015-11-26 10:22:14},
  priority = {2},
  title = {{A general construction for parallelizing Metropolis-Hastings algorithms}},
  volume = {111},
  year = {2014}
}

@article{Du10,
  abstract = {{High-throughput profiling of DNA methylation status of CpG islands is crucial to understand the epigenetic regulation of genes. The microarray-based Infinium methylation assay by Illumina is one platform for low-cost high-throughput methylation profiling. Both Beta-value and M-value statistics have been used as metrics to measure methylation levels. However, there are no detailed studies of their relations and their strengths and limitations.}},
  author = {Du, Pan and Zhang, Xiao and Huang, Chiang-Ching and Jafari, Nadereh and Kibbe, Warren A. and Hou, Lifang and Lin, Simon M.},
  citeulike-article-id = {8386190},
  citeulike-linkout-0 = {http://dx.doi.org/10.1186/1471-2105-11-587},
  citeulike-linkout-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3012676/},
  citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/21118553},
  citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=21118553},
  day = {30},
  issn = {1471-2105},
  journal = {BMC Bioinformatics},
  keywords = {epigenomics},
  month = nov,
  number = {1},
  pages = {587+},
  pmcid = {PMC3012676},
  pmid = {21118553},
  posted-at = {2015-11-09 14:26:24},
  priority = {2},
  publisher = {BioMed Central Ltd},
  title = {{Comparison of Beta-value and M-value methods for quantifying methylation levels by microarray analysis}},
  volume = {11},
  year = {2010}
}

@article{ENCODE04,
  abstract = {{
                The ENCyclopedia Of DNA Elements (ENCODE) Project aims to identify all functional elements in the human genome sequence. The pilot phase of the Project is focused on a specified 30 megabases (approximately 1\%) of the human genome sequence and is organized as an international consortium of computational and laboratory-based scientists working to develop and apply high-throughput approaches for detecting all sequence elements that confer biological function. The results of this pilot phase will guide future efforts to analyze the entire human genome.
            }},
  citeulike-article-id = {149217},
  citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.1105136},
  citeulike-linkout-1 = {http://www.sciencemag.org/content/306/5696/636.abstract},
  citeulike-linkout-2 = {http://www.sciencemag.org/content/306/5696/636.full.pdf},
  citeulike-linkout-3 = {http://www.sciencemag.org/cgi/content/abstract/306/5696/636},
  citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/15499007},
  citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=15499007},
  day = {22},
  issn = {0036-8075},
  journal = {Science},
  keywords = {epigenomics},
  month = oct,
  number = {5696},
  pages = {636--640},
  pmid = {15499007},
  posted-at = {2015-11-09 14:23:16},
  priority = {2},
  publisher = {American Association for the Advancement of Science},
  title = {{The ENCODE (ENCyclopedia Of DNA Elements) Project}},
  volume = {306},
  year = {2004}
}

@article{Hansen11,
  abstract = {{
                Tumor heterogeneity is a major barrier to effective cancer diagnosis and treatment. We recently identified cancer-specific differentially DNA-methylated regions (cDMRs) in colon cancer, which also distinguish normal tissue types from each other, suggesting that these cDMRs might be generalized across cancer types. Here we show stochastic methylation variation of the same cDMRs, distinguishing cancer from normal tissue, in colon, lung, breast, thyroid and Wilms' tumors, with intermediate variation in adenomas. Whole-genome bisulfite sequencing shows these variable cDMRs are related to loss of sharply delimited methylation boundaries at CpG islands. Furthermore, we find hypomethylation of discrete blocks encompassing half the genome, with extreme gene expression variability. Genes associated with the cDMRs and large blocks are involved in mitosis and matrix remodeling, respectively. We suggest a model for cancer involving loss of epigenetic stability of well-defined genomic domains that underlies increased methylation variability in cancer that may contribute to tumor heterogeneity.
            }},
  author = {Hansen, Kasper D. and Timp, Winston and Bravo, H\'{e}ctor C. and Sabunciyan, Sarven and Langmead, Benjamin and McDonald, Oliver G. and Wen, Bo and Wu, Hao and Liu, Yun and Diep, Dinh and Briem, Eirikur and Zhang, Kun and Irizarry, Rafael A. and Feinberg, Andrew P.},
  citeulike-article-id = {9589272},
  citeulike-linkout-0 = {http://dx.doi.org/10.1038/ng.865},
  citeulike-linkout-1 = {http://dx.doi.org/10.1038/ng.865},
  citeulike-linkout-2 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3145050/},
  citeulike-linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/21706001},
  citeulike-linkout-4 = {http://www.hubmed.org/display.cgi?uids=21706001},
  day = {26},
  issn = {1061-4036},
  journal = {Nature Genetics},
  keywords = {epigenomics},
  month = jun,
  number = {8},
  pages = {768--775},
  pmcid = {PMC3145050},
  pmid = {21706001},
  posted-at = {2015-11-09 14:17:17},
  priority = {2},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  title = {{Increased methylation variation in epigenetic domains across cancer types}},
  volume = {43},
  year = {2011}
}

@article{Hinoue12,
  abstract = {{Colorectal cancer (CRC) is a heterogeneous disease in which unique subtypes are characterized by distinct genetic and epigenetic alterations. Here we performed comprehensive genome-scale DNA methylation profiling of 125 colorectal tumors and 29 adjacent normal tissues. We identified four DNA methylation–based subgroups of CRC using model-based cluster analyses. Each subtype shows characteristic genetic and clinical features, indicating that they represent biologically distinct subgroups. A CIMP-high (CIMP-H) subgroup, which exhibits an exceptionally high frequency of cancer-specific DNA hypermethylation, is strongly associated with MLH1 DNA hypermethylation and the BRAFV600E mutation. A CIMP-low (CIMP-L) subgroup is enriched for KRAS mutations and characterized by DNA hypermethylation of a subset of CIMP-H-associated markers rather than a unique group of CpG islands. Non-CIMP tumors are separated into two distinct clusters. One non-CIMP subgroup is distinguished by a significantly higher frequency of TP53 mutations and frequent occurrence in the distal colon, while the tumors that belong to the fourth group exhibit a low frequency of both cancer-specific DNA hypermethylation and gene mutations and are significantly enriched for rectal tumors. Furthermore, we identified 112 genes that were down-regulated more than twofold in CIMP-H tumors together with promoter DNA hypermethylation. These represent ∼7\% of genes that acquired promoter DNA methylation in CIMP-H tumors. Intriguingly, 48/112 genes were also transcriptionally down-regulated in non-CIMP subgroups, but this was not attributable to promoter DNA hypermethylation. Together, we identified four distinct DNA methylation subgroups of CRC and provided novel insight regarding the role of CIMP-specific DNA hypermethylation in gene silencing.}},
  author = {Hinoue, Toshinori and Weisenberger, Daniel J. and Lange, Christopher P. E. and Shen, Hui and Byun, Hyang-Min and Van Den Berg, David and Malik, Simeen and Pan, Fei and Noushmehr, Houtan and van Dijk, Cornelis M. and Tollenaar, Rob A. E. M. and Laird, Peter W.},
  citeulike-article-id = {9396487},
  citeulike-linkout-0 = {http://dx.doi.org/10.1101/gr.117523.110},
  citeulike-linkout-1 = {http://genome.cshlp.org/content/22/2/271.abstract},
  citeulike-linkout-2 = {http://genome.cshlp.org/content/22/2/271.full.pdf},
  citeulike-linkout-3 = {http://genome.cshlp.org/cgi/content/abstract/22/2/271},
  citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/21659424},
  citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=21659424},
  day = {1},
  issn = {1549-5469},
  journal = {Genome Research},
  keywords = {epigenomics},
  month = feb,
  number = {2},
  pages = {271--282},
  pmid = {21659424},
  posted-at = {2015-11-09 14:14:47},
  priority = {2},
  publisher = {Cold Spring Harbor Laboratory Press},
  title = {{Genome-scale analysis of aberrant DNA methylation in colorectal cancer}},
  volume = {22},
  year = {2012}
}

@incollection{Rahimi08,
  author = {Rahimi, Ali and Recht, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems 20},
  citeulike-article-id = {13833660},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  keywords = {gaussian\_processes},
  pages = {1177--1184},
  posted-at = {2015-11-09 09:41:59},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Random Features for Large-Scale Kernel Machines}},
  year = {2008}
}

@proceedings{DBLP:conf/aistats/2013,
  citeulike-article-id = {13833654},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v31/},
  keywords = {gaussian\_processes},
  posted-at = {2015-11-09 09:37:26},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Proceedings},
  title = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2013, Scottsdale, AZ, USA, April 29 - May 1, 2013},
  volume = {31},
  year = {2013}
}

@inproceedings{Damianou13,
  author = {Damianou, Andreas C. and Lawrence, Neil D.},
  booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2013, Scottsdale, AZ, USA, April 29 - May 1, 2013},
  citeulike-article-id = {13833653},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v31/damianou13a.html},
  keywords = {gaussian\_processes},
  pages = {207--215},
  posted-at = {2015-11-09 09:37:26},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Proceedings},
  title = {{Deep Gaussian Processes}},
  volume = {31},
  year = {2013}
}

@misc{Hensman14,
  abstract = {{Deep Gaussian processes provide a flexible approach to probabilistic
modelling of data using either supervised or unsupervised learning. For
tractable inference approximations to the marginal likelihood of the model must
be made. The original approach to approximate inference in these models used
variational compression to allow for approximate variational marginalization of
the hidden variables leading to a lower bound on the marginal likelihood of the
model [Damianou and Lawrence, 2013]. In this paper we extend this idea with a
nested variational compression. The resulting lower bound on the likelihood can
be easily parallelized or adapted for stochastic variational inference.}},
  author = {Hensman, James and Lawrence, Neil D.},
  citeulike-article-id = {13451892},
  citeulike-linkout-0 = {http://arxiv.org/abs/1412.1370},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1412.1370},
  day = {3},
  eprint = {1412.1370},
  keywords = {gaussian\_processes},
  month = dec,
  posted-at = {2015-11-09 09:34:42},
  priority = {2},
  title = {{Nested Variational Compression in Deep Gaussian Processes}},
  note = {arxiv:1412.1370},
  year = {2014}
}

@book{Titov97,
  author = {Titov, V. V. and Gonzalez, F. I. and Pacific Marine Environmental Laboratory},
  citeulike-article-id = {13833644},
  citeulike-linkout-0 = {http://purl.access.gpo.gov/GPO/LPS46461},
  keywords = {tsunami},
  posted-at = {2015-11-09 09:13:24},
  priority = {2},
  publisher = {U.S. Dept. of Commerce, National Oceanic and Atmospheric Administration, Environmental Research Laboratories, Pacific Marine Environmental Laboratory ; For sale by the National Technical Information Service Seattle, Wash. : Springfield, VA},
  title = {{Implementation and testing of the Method of Splitting Tsunami (MOST) model}},
  year = {1997}
}

@incollection{Synolakis09,
  author = {Synolakis, C. E. and Bernard, E. N. and Titov, V. V. and K\^{a}no\u{g}lu, U. and Gonz\'{a}lez, F. I.},
  booktitle = {Tsunami Science Four Years after the 2004 Indian Ocean Tsunami},
  citeulike-article-id = {13817689},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-0346-0057-6\_11},
  editor = {Cummins, PhilR and Satake, Kenji and Kong, LauraS},
  keywords = {and, benchmarked, models, numerical, tsunami, validated, verified},
  pages = {2197--2228},
  posted-at = {2015-10-30 13:58:43},
  priority = {2},
  publisher = {Birkh\"{a}user Basel},
  series = {Pageoph Topical Volumes},
  title = {{Validation and Verification of Tsunami Numerical Models}},
  year = {2009}
}

@article{Sarri12,
  author = {Sarri, A. and Guillas, S. and Dias, F.},
  citeulike-article-id = {13817687},
  citeulike-linkout-0 = {http://dx.doi.org/10.5194/nhess-12-2003-2012},
  citeulike-linkout-1 = {http://www.nat-hazards-earth-syst-sci.net/12/2003/2012/},
  journal = {Natural Hazards and Earth System Science},
  keywords = {tsunami},
  number = {6},
  pages = {2003--2018},
  posted-at = {2015-10-30 13:53:27},
  priority = {2},
  title = {{Statistical emulation of a tsunami model for sensitivity analysis and uncertainty quantification}},
  volume = {12},
  year = {2012}
}

@article{Minson13,
  abstract = {{The estimation of finite fault earthquake source models is an inherently underdetermined problem: there is no unique solution to the inverse problem of determining the rupture history at depth as a function of time and space when our data are limited to observations at the Earth's surface. Bayesian methods allow us to determine the set of all plausible source model parameters that are consistent with the observations, our a priori assumptions about the physics of the earthquake source and wave propagation, and models for the observation errors and the errors due to the limitations in our forward model. Because our inversion approach does not require inverting any matrices other than covariance matrices, we can restrict our ensemble of solutions to only those models that are physically defensible while avoiding the need to restrict our class of models based on considerations of numerical invertibility. We only use prior information that is consistent with the physics of the problem rather than some artefice (such as smoothing) needed to produce a unique optimal model estimate. Bayesian inference can also be used to estimate model-dependent and internally consistent effective errors due to shortcomings in the forward model or data interpretation, such as poor Green's functions or extraneous signals recorded by our instruments. Until recently, Bayesian techniques have been of limited utility for earthquake source inversions because they are computationally intractable for problems with as many free parameters as typically used in kinematic finite fault models. Our algorithm, called cascading adaptive transitional metropolis in parallel (CATMIP), allows sampling of high-dimensional problems in a parallel computing framework. CATMIP combines the Metropolis algorithm with elements of simulated annealing and genetic algorithms to dynamically optimize the algorithm's efficiency as it runs. The algorithm is a generic Bayesian Markov Chain Monte Carlo sampler; it works independently of the model design, a priori constraints and data under consideration, and so can be used for a wide variety of scientific problems. We compare CATMIP's efficiency relative to several existing sampling algorithms and then present synthetic performance tests of finite fault earthquake rupture models computed using CATMIP.}},
  author = {Minson, S. E. and Simons, M. and Beck, J. L.},
  citeulike-article-id = {13817684},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/gji/ggt180},
  citeulike-linkout-1 = {http://gji.oxfordjournals.org/content/early/2013/06/19/gji.ggt180.abstract},
  eprint = {http://gji.oxfordjournals.org/content/early/2013/06/19/gji.ggt180.full.pdf+html},
  journal = {Geophysical Journal International},
  keywords = {earthquake},
  posted-at = {2015-10-30 13:46:59},
  priority = {2},
  title = {{Bayesian inversion for finite fault earthquake source models I—theory and algorithm}},
  year = {2013}
}

@article{Marzocchi12,
  abstract = {{The assessment of earthquake forecast models for practical purposes requires more than simply checking model consistency in a statistical framework. One also needs to understand how to construct the best model for specific forecasting applications. We describe a Bayesian approach to evaluating earthquake forecasting models, and we consider related procedures for constructing ensemble forecasts. We show how evaluations based on Bayes factors, which measure the relative skill among forecasts, can be complementary to common goodness‐of‐fit tests used to measure the absolute consistency of forecasts with data. To construct ensemble forecasts, we consider averages across a forecast set, weighted by either posterior probabilities or inverse log‐likelihoods derived during prospective earthquake forecasting experiments. We account for model correlations by conditioning weights using the Garthwaite–Mubwandarikwa capped eigenvalue scheme. We apply these methods to the Regional Earthquake Likelihood Models (RELM) five‐year earthquake forecast experiment in California, and we discuss how this approach can be generalized to other ensemble forecasting applications. Specific applications of seismological importance include experiments being conducted within the Collaboratory for the Study of Earthquake Predictability (CSEP) and ensemble methods for operational earthquake forecasting.Online Material: Tables of likelihoods for each testing phase and code to analyze the RELM experiment.}},
  author = {Marzocchi, Warner and Zechar, J. Douglas and Jordan, Thomas H.},
  citeulike-article-id = {13817683},
  citeulike-linkout-0 = {http://dx.doi.org/10.1785/0120110327},
  citeulike-linkout-1 = {http://www.bssaonline.org/content/102/6/2574.abstract},
  eprint = {http://www.bssaonline.org/content/102/6/2574.full.pdf+html},
  journal = {Bulletin of the Seismological Society of America},
  keywords = {earthquake},
  number = {6},
  pages = {2574--2584},
  posted-at = {2015-10-30 13:41:16},
  priority = {2},
  title = {{Bayesian Forecast Evaluation and Ensemble Earthquake Forecasting}},
  volume = {102},
  year = {2012}
}

@article{Wyss97,
  author = {Wyss, Max},
  citeulike-article-id = {13817578},
  citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.278.5337.487},
  citeulike-linkout-1 = {http://www.sciencemag.org/content/278/5337/487.short},
  eprint = {http://www.sciencemag.org/content/278/5337/487.full.pdf},
  journal = {Science},
  keywords = {earthquake},
  number = {5337},
  pages = {487--490},
  posted-at = {2015-10-30 10:49:57},
  priority = {2},
  title = {{Cannot Earthquakes Be Predicted?}},
  volume = {278},
  year = {1997}
}

@article{Geller97,
  author = {Geller, Robert J.},
  citeulike-article-id = {13817576},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1365-246X.1997.tb06588.x},
  journal = {Geophysical Journal International},
  keywords = {earthquake, prediction},
  number = {3},
  pages = {425--450},
  posted-at = {2015-10-30 10:43:31},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  title = {{Earthquake prediction: a critical review}},
  volume = {131},
  year = {1997}
}

@article{Baker15,
  author = {Baker, A. H. and Hammerling, D. M. and Levy, M. N. and Xu, H. and Dennis, J. M. and Eaton, B. E. and Edwards, J. and Hannay, C. and Mickelson, S. A. and Neale, R. B. and Nychka, D. and Shollenberger, J. and Tribbia, J. and Vertenstein, M. and Williamson, D.},
  citeulike-article-id = {13816731},
  citeulike-linkout-0 = {http://dx.doi.org/10.5194/gmd-8-2829-2015},
  citeulike-linkout-1 = {http://www.geosci-model-dev.net/8/2829/2015/},
  journal = {Geoscientific Model Development},
  keywords = {climate},
  number = {9},
  pages = {2829--2840},
  posted-at = {2015-10-29 15:22:48},
  priority = {2},
  title = {{A new ensemble-based consistency test for the Community Earth System Model (pyCECT v1.0)}},
  volume = {8},
  year = {2015}
}

@article{Ho11,
  abstract = {{No abstract available.}},
  author = {Ho, Chun K. and Stephenson, David B. and Collins, Matthew and Ferro, Christopher A. T. and Brown, Simon J.},
  booktitle = {Bulletin of the American Meteorological Society},
  citeulike-article-id = {10383657},
  citeulike-linkout-0 = {http://journals.ametsoc.org/doi/abs/10.1175/2011BAMS3110.1},
  citeulike-linkout-1 = {http://dx.doi.org/10.1175/2011bams3110.1},
  day = {13},
  journal = {Bull. Amer. Meteor. Soc.},
  keywords = {climate},
  month = sep,
  number = {1},
  pages = {21--26},
  posted-at = {2015-10-29 15:12:07},
  priority = {2},
  publisher = {American Meteorological Society},
  title = {{Calibration Strategies: A Source of Additional Uncertainty in Climate Change Projections}},
  volume = {93},
  year = {2011}
}

@article{Feser11,
  abstract = {{An important challenge in current climate modeling is to realistically describe small-scale weather statistics, such as topographic precipitation and coastal wind patterns, or regional phenomena like polar lows. Global climate models simulate atmospheric processes with increasingly higher resolutions, but still regional climate models have a lot of advantages. They consume less computation time because of their limited simulation area and thereby allow for higher resolution both in time and space as well as for longer integration times. Regional climate models can be used for dynamical down-scaling purposes because their output data can be processed to produce higher resolved atmospheric fields, allowing the representation of small-scale processes and a more detailed description of physiographic details (such as mountain ranges, coastal zones, and details of soil properties). However, does higher resolution add value when compared to global model results? Most studies implicitly assume that dynamical downscaling leads to output fields that are superior to the driving global data, but little work has been carried out to substantiate these expectations. Here a series of articles is reviewed that evaluate the benefit of dynamical downscaling by explicitly comparing results of global and regional climate model data to the observations. These studies show that the regional climate model generally performs better for the medium spatial scales, but not always for the larger spatial scales. Regional models can add value, but only for certain variables and locations?particularly those influenced by regional specifics, such as coasts, or mesoscale dynamics, such as polar lows. Therefore, the decision of whether a regional climate model simulation is required depends crucially on the scientific question being addressed.}},
  author = {Feser, Frauke and Rockel, Burkhardt and von Storch, Hans and Winterfeldt, J\"{o}rg and Zahn, Matthias},
  citeulike-article-id = {13365618},
  citeulike-linkout-0 = {http://journals.ametsoc.org/doi/abs/10.1175/2011BAMS3061.1},
  citeulike-linkout-1 = {http://dx.doi.org/10.1175/2011bams3061.1},
  day = {3},
  journal = {Bull. Amer. Meteor. Soc.},
  keywords = {climate},
  month = may,
  number = {9},
  pages = {1181--1192},
  posted-at = {2015-10-29 15:07:19},
  priority = {2},
  publisher = {American Meteorological Society},
  title = {{Regional Climate Models Add Value to Global Model Data: A Review and Selected Examples}},
  volume = {92},
  year = {2011}
}

@article{Pielke12,
  author = {Pielke, Roger A. and Wilby, Robert L.},
  citeulike-article-id = {13816724},
  citeulike-linkout-0 = {http://dx.doi.org/10.1029/2012EO050008},
  journal = {Eos, Transactions American Geophysical Union},
  keywords = {american, analyses, and, approximations, atmosphere, climate, downscaling, dynamic, global, impacts, models, monsoon, north, numerical, statistical, vulnerability},
  number = {5},
  pages = {52--53},
  posted-at = {2015-10-29 14:56:12},
  priority = {2},
  title = {{Regional climate downscaling: What's the point?}},
  volume = {93},
  year = {2012}
}

@article{Rao14,
  author = {Rao, Koteswara K. and Patwardhan, S. K. and Kulkarni, Ashwini and Kamala, K. and Sabade, S. S. and Kumar, Krishna K.},
  citeulike-article-id = {13816722},
  citeulike-linkout-0 = {http://dx.doi.org/http://dx.doi.org/10.1016/j.gloplacha.2013.12.006},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S0921818113002774},
  journal = {Global and Planetary Change},
  keywords = {climate, indian},
  pages = {77--90},
  posted-at = {2015-10-29 14:52:10},
  priority = {2},
  title = {Projected changes in mean and extreme precipitation indices over India using {PRECIS}},
  volume = {113},
  year = {2014}
}

@article{Wuebbles13,
  abstract = {{This is the fourth in a series of four articles on historical and projected climate extremes in the United States. Here, we examine the results of historical and future climate model experiments from the phase 5 of the Coupled Model Intercomparison Project (CMIP5) based on work presented at the World Climate Research Programme (WCRP) Workshop on CMIP5 Climate Model Analyses held in March 2012. Our analyses assess the ability of CMIP5 models to capture observed trends, and we also evaluate the projected future changes in extreme events over the contiguous Unites States. Consistent with the previous articles, here we focus on model-simulated historical trends and projections for temperature extremes, heavy precipitation, large-scale drivers of precipitation variability and drought, and extratropical storms. Comparing new CMIP5 model results with earlier CMIP3 simulations shows that in general CMIP5 simulations give similar patterns and magnitudes of future temperature and precipitation extremes in the United States relative to the projections from the earlier phase 3 of the Coupled Model Intercomparison Project (CMIP3) models. Specifically, projections presented here show significant changes in hot and cold temperature extremes, heavy precipitation, droughts, atmospheric patterns such as the North American monsoon and the North Atlantic subtropical high that affect interannual precipitation, and in extratropical storms over the twenty-first century. Most of these trends are consistent with, although in some cases (such as heavy precipitation) underestimate, observed trends.}},
  author = {Wuebbles, Donald and Meehl, Gerald and Hayhoe, Katharine and Karl, Thomas R. and Kunkel, Kenneth and Santer, Benjamin and Wehner, Michael and Colle, Brian and Fischer, Erich M. and Fu, Rong and Goodman, Alex and Janssen, Emily and Kharin, Viatcheslav and Lee, Huikyo and Li, Wenhong and Long, Lindsey N. and Olsen, Seth C. and Pan, Zaitao and Seth, Anji and Sheffield, Justin and Sun, Liqiang},
  citeulike-article-id = {13816717},
  citeulike-linkout-0 = {http://journals.ametsoc.org/doi/abs/10.1175/BAMS-D-12-00172.1},
  citeulike-linkout-1 = {http://dx.doi.org/10.1175/bams-d-12-00172.1},
  day = {18},
  journal = {Bull. Amer. Meteor. Soc.},
  keywords = {climate},
  month = jul,
  number = {4},
  pages = {571--583},
  posted-at = {2015-10-29 14:48:06},
  priority = {2},
  publisher = {American Meteorological Society},
  title = {{CMIP5 Climate Model Analyses: Climate Extremes in the United States}},
  volume = {95},
  year = {2013}
}

@article{Cai14,
  author = {Cai, Wenju and Borlace, Simon and Lengaigne, Matthieu and van Rensch, Peter and Collins, Mat and Vecchi, Gabriel and Timmermann, Axel and Santoso, Agus and McPhaden, Michael J. and Wu, Lixin and England, Matthew H. and Wang, Guojian and Guilyardi, Eric and Jin, Fei-Fei},
  citeulike-article-id = {12928912},
  citeulike-linkout-0 = {http://dx.doi.org/10.1038/nclimate2100},
  citeulike-linkout-1 = {http://dx.doi.org/10.1038/nclimate2100},
  day = {19},
  issn = {1758-678X},
  journal = {Nature Climate Change},
  keywords = {climate},
  month = jan,
  number = {2},
  pages = {111--116},
  posted-at = {2015-10-29 14:44:34},
  priority = {2},
  publisher = {Nature Publishing Group},
  title = {{Increasing frequency of extreme El Ni\~{n}o events due to greenhouse warming}},
  volume = {4},
  year = {2014}
}

@article{Taylor11,
  abstract = {{The fifth phase of the Coupled Model Intercomparison Project (CMIP5) will produce a state-of-the- art multimodel dataset designed to advance our knowledge of climate variability and climate change. Researchers worldwide are analyzing the model output and will produce results likely to underlie the forthcoming Fifth Assessment Report by the Intergovernmental Panel on Climate Change. Unprecedented in scale and attracting interest from all major climate modeling groups, CMIP5 includes ?long term? simulations of twentieth-century climate and projections for the twenty-first century and beyond. Conventional atmosphere?ocean global climate models and Earth system models of intermediate complexity are for the first time being joined by more recently developed Earth system models under an experiment design that allows both types of models to be compared to observations on an equal footing. Besides the longterm experiments, CMIP5 calls for an entirely new suite of ?near term? simulations focusing on recent decades and the future to year 2035. These ?decadal predictions? are initialized based on observations and will be used to explore the predictability of climate and to assess the forecast system's predictive skill. The CMIP5 experiment design also allows for participation of stand-alone atmospheric models and includes a variety of idealized experiments that will improve understanding of the range of model responses found in the more complex and realistic simulations. An exceptionally comprehensive set of model output is being collected and made freely available to researchers through an integrated but distributed data archive. For researchers unfamiliar with climate models, the limitations of the models and experiment design are described.}},
  author = {Taylor, Karl E. and Stouffer, Ronald J. and Meehl, Gerald A.},
  booktitle = {Bulletin of the American Meteorological Society},
  citeulike-article-id = {10490392},
  citeulike-linkout-0 = {http://journals.ametsoc.org/doi/abs/10.1175/BAMS-D-11-00094.1},
  citeulike-linkout-1 = {http://dx.doi.org/10.1175/bams-d-11-00094.1},
  day = {7},
  journal = {Bull. Amer. Meteor. Soc.},
  keywords = {climate},
  month = oct,
  number = {4},
  pages = {485--498},
  posted-at = {2015-10-29 14:38:36},
  priority = {2},
  publisher = {American Meteorological Society},
  title = {{An Overview of CMIP5 and the Experiment Design}},
  volume = {93},
  year = {2012}
}

@article{Haughton15,
  author = {Haughton, Ned and Abramowitz, Gab and Pitman, Andy and Phipps, StevenJ},
  citeulike-article-id = {13816710},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s00382-015-2531-3},
  journal = {Climate Dynamics},
  keywords = {climate},
  pages = {1--13},
  posted-at = {2015-10-29 14:29:21},
  priority = {2},
  publisher = {Springer Berlin Heidelberg},
  title = {{Weighting climate model ensembles for mean and variance estimates}},
  year = {2015}
}

@article{Hawkins14,
  abstract = {{Arising from C. Mora et al.Nature502, 183\&\#19;18710.1038/nature12540 (2013)The question of when the signal of climate change will emerge from the background noise of climate variability\&\#20;the \&\#24;time of emergence\&\#25;\&\#20;is potentially important for adaptation planning. Mora et al. presented precise projections of the time of emergence of unprecedented regional climates. However, their methodology produces artificially early dates at which specific regions will permanently experience unprecedented climates and artificially low uncertainty in those dates everywhere. This overconfidence could impair the effectiveness of climate risk management decisions. There is a Reply to this Brief Communication Arising by Mora, C. et al. Nature511,http://dx.doi.org/10.1038/nature13524 (2014).}},
  author = {Hawkins, Ed and Anderson, Bruce and Diffenbaugh, Noah and Mahlstein, Irina and Betts, Richard and Hegerl, Gabi and Joshi, Manoj and Knutti, Reto and McNeall, Doug and Solomon, Susan and Sutton, Rowan and Syktus, Jozef and Vecchi, Gabriel},
  citeulike-article-id = {13251292},
  citeulike-linkout-0 = {http://dx.doi.org/10.1038/nature13523},
  day = {2},
  issn = {0028-0836},
  journal = {Nature},
  keywords = {climate},
  month = jul,
  number = {7507},
  pages = {E3--E5},
  posted-at = {2015-10-29 14:25:34},
  priority = {2},
  publisher = {Nature Publishing Group},
  title = {{Uncertainties in the timing of unprecedented climates}},
  volume = {511},
  year = {2014}
}

@article{Coppede14,
  author = {Copped\`{e}, Fabio},
  citeulike-article-id = {13816583},
  citeulike-linkout-0 = {http://dx.doi.org/http://dx.doi.org/10.1016/j.canlet.2011.12.030},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S0304383511007890},
  journal = {Cancer Letters},
  keywords = {cancer, colorectal, epigenomics},
  note = {Cancer Epigenetics as Biomarkers of Clinical Significance},
  number = {2},
  pages = {238--247},
  posted-at = {2015-10-29 10:47:42},
  priority = {2},
  title = {Epigenetic biomarkers of colorectal cancer: Focus on {DNA} methylation},
  volume = {342},
  year = {2014}
}

@article{Yang15,
  abstract = {{One of the most important recent findings in cancer genomics is the identification of novel driver mutations which often target genes that regulate genome-wide chromatin and DNA methylation marks. Little is known, however, as to whether these genes exhibit patterns of epigenomic deregulation that transcend cancer types.}},
  author = {Yang, Zhen and Jones, Allison and Widschwendter, Martin and Teschendorff, Andrew E.},
  citeulike-article-id = {13673959},
  citeulike-linkout-0 = {http://dx.doi.org/10.1186/s13059-015-0699-9},
  citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/26169266},
  citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=26169266},
  day = {14},
  issn = {1465-6906},
  journal = {Genome Biology},
  keywords = {epigenomics},
  month = jul,
  number = {1},
  pages = {140+},
  pmid = {26169266},
  posted-at = {2015-10-29 10:44:43},
  priority = {2},
  publisher = {BioMed Central Ltd},
  title = {{An integrative pan-cancer-wide analysis of epigenetic enzymes reveals universal patterns of epigenomic deregulation in cancer}},
  volume = {16},
  year = {2015}
}

@article{Rakyan11,
  abstract = {{Despite the success of genome-wide association studies (GWASs) in identifying loci associated with common diseases, a substantial proportion of the causality remains unexplained. Recent advances in genomic technologies have placed us in a position to initiate large-scale studies of human disease-associated epigenetic variation, specifically variation in DNA methylation. Such epigenome-wide association studies (EWASs) present novel opportunities but also create new challenges that are not encountered in GWASs. We discuss EWAS design, cohort and sample selections, statistical significance and power, confounding factors and follow-up studies. We also discuss how integration of EWASs with GWASs can help to dissect complex GWAS haplotypes for functional analysis.}},
  author = {Rakyan, Vardhman K. and Down, Thomas A. and Balding, David J. and Beck, Stephan},
  citeulike-article-id = {9550041},
  citeulike-linkout-0 = {http://dx.doi.org/10.1038/nrg3000},
  citeulike-linkout-1 = {http://dx.doi.org/10.1038/nrg3000},
  citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/21747404},
  citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=21747404},
  day = {12},
  issn = {1471-0064},
  journal = {Nature reviews. Genetics},
  keywords = {epigenomics},
  month = jul,
  number = {8},
  pages = {529--541},
  pmid = {21747404},
  posted-at = {2015-10-29 10:39:13},
  priority = {2},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  title = {{Epigenome-wide association studies for common human diseases.}},
  volume = {12},
  year = {2011}
}

@article{Whitelaw06,
  abstract = {{Despite our detailed characterization of the human genome at the level of the primary DNA sequence, we are still far from understanding the molecular events underlying phenotypic variation. Epigenetic modifications to the DNA sequence and associated chromatin are known to regulate gene expression and, as such, are a significant contributor to phenotype. Studies of inbred mice and monozygotic twins show that variation in the epigenotype can be seen even between genetically identical individuals and that this, in some cases at least, is associated with phenotypic differences. Moreover, recent evidence suggests that the epigenome can be influenced by the environment and these changes can last a lifetime. However, we also know that epigenetic states in real-time are in continual flux and, as a result, the epigenome exhibits instability both within and across generations. We still do not understand the rules governing the establishment and maintenance of the epigenotype at any particular locus. The underlying DNA sequence itself and the sequence at unlinked loci (modifier loci) are certainly involved. Recent support for the existence of transgenerational epigenetic inheritance in mammals suggests that the epigenetic state of the locus in the previous generation may also play a role. Over the next decade, many of these processes will be better understood, heralding a greater capacity for us to correlate measurable molecular marks with phenotype and providing the opportunity for improved diagnosis and presymptomatic healthcare.}},
  author = {Whitelaw, Nadia C. and Whitelaw, Emma},
  citeulike-article-id = {7341831},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/hmg/ddl200},
  citeulike-linkout-1 = {http://hmg.oxfordjournals.org/cgi/content/abstract/15/suppl\_2/R131},
  citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/16987876},
  citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=16987876},
  day = {15},
  issn = {0964-6906},
  journal = {Human Molecular Genetics},
  keywords = {epigenomics},
  month = oct,
  number = {suppl 2},
  pages = {R131--R137},
  pmid = {16987876},
  posted-at = {2015-10-29 10:30:20},
  priority = {2},
  title = {{How lifetimes shape epigenotype within and across generations}},
  volume = {15},
  year = {2006}
}

@article{Johannes09,
  abstract = {{Loss or gain of DNA methylation can affect gene expression and is sometimes transmitted across generations. Such epigenetic alterations are thus a possible source of heritable phenotypic variation in the absence of DNA sequence change. However, attempts to assess the prevalence of stable epigenetic variation in natural and experimental populations and to quantify its impact on complex traits have been hampered by the confounding effects of DNA sequence polymorphisms. To overcome this problem as much as possible, two parents with little DNA sequence differences, but contrasting DNA methylation profiles, were used to derive a panel of epigenetic Recombinant Inbred Lines (epiRILs) in the reference plant Arabidopsis thaliana. The epiRILs showed variation and high heritability for flowering time and plant height (∼30\%), as well as stable inheritance of multiple parental DNA methylation variants (epialleles) over at least eight generations. These findings provide a first rationale to identify epiallelic variants that contribute to heritable variation in complex traits using linkage or association studies. More generally, the demonstration that numerous epialleles across the genome can be stable over many generations in the absence of selection or extensive DNA sequence variation highlights the need to integrate epigenetic information into population genetics studies. DNA methylation is defined as an epigenetic modification because it can be inherited across cell division. Since variations in DNA methylation can affect gene expression and be inherited across generations, they can provide a source of heritable phenotypic variation that is not caused by changes in the DNA sequence. However, the extent to which this type of phenotypic variation occurs in natural or experimental populations is unknown, partly because of the difficulty in teasing apart the effect of DNA methylation variants (epialleles) from that of the DNA sequence variants also present in these populations. To overcome this problem, we have derived a population of epigenetic recombinant inbred lines in the plant Arabidopsis thaliana, using parents with few DNA sequence differences but contrasting DNA methylation profiles. This population showed variation and a high degree of heritability for two complex traits, flowering time and plant height. Multiple parental DNA methylation differences were also found to be stably inherited over eight generations in this population. These findings reveal the potential impact of heritable DNA methylation variation on complex traits and demonstrate the importance of integrating epigenetic information in population genetics studies.}},
  author = {Johannes, Frank and Porcher, Emmanuelle and Teixeira, Felipe K. and Saliba-Colombani, Vera and Simon, Matthieu and Agier, Nicolas and Bulski, Agn\`{e}s and Albuisson, Juliette and Heredia, Fabiana and Audigier, Pascal and Bouchez, David and Dillmann, Christine and Guerche, Philippe and Hospital, Fr\'{e}d\'{e}ric and Colot, Vincent},
  citeulike-article-id = {5084838},
  citeulike-linkout-0 = {http://dx.doi.org/10.1371/journal.pgen.1000530},
  citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/19557164},
  citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=19557164},
  day = {26},
  issn = {1553-7404},
  journal = {PLoS Genet},
  keywords = {epigenomics},
  month = jun,
  number = {6},
  pages = {e1000530+},
  pmid = {19557164},
  posted-at = {2015-10-29 10:28:26},
  priority = {2},
  publisher = {Public Library of Science},
  title = {{Assessing the Impact of Transgenerational Epigenetic Variation on Complex Traits}},
  volume = {5},
  year = {2009}
}

@article{Liebers14,
  abstract = {{Genomic concepts are based on the assumption that phenotypes arise from the expression of genetic variants. However, the presence of non-Mendelian inheritance patterns provides a direct challenge to this view and suggests an important role for alternative mechanisms of gene regulation and inheritance. Over the past few years, a highly complex and diverse network of noncoding RNAs has been discovered. Research in animal models has shown that RNAs can be inherited and that RNA methyltransferases can be important for the transmission and expression of modified phenotypes in the next generation. We discuss possible mechanisms of RNA-mediated inheritance and the role of these mechanisms for human health and disease.}},
  author = {Liebers, Reinhard and Rassoulzadegan, Minoo and Lyko, Frank},
  citeulike-article-id = {13141742},
  citeulike-linkout-0 = {http://dx.doi.org/10.1371/journal.pgen.1004296},
  day = {17},
  journal = {PLoS Genet},
  keywords = {epigenomics},
  month = apr,
  number = {4},
  pages = {e1004296+},
  posted-at = {2015-10-29 10:23:06},
  priority = {2},
  publisher = {Public Library of Science},
  title = {{Epigenetic Regulation by Heritable RNA}},
  volume = {10},
  year = {2014}
}

@article{Ringrose04,
  author = {Ringrose, Leonie and Paro, Renato},
  citeulike-article-id = {13816543},
  citeulike-linkout-0 = {http://dx.doi.org/10.1146/annurev.genet.38.072902.091907},
  eprint = {http://dx.doi.org/10.1146/annurev.genet.38.072902.091907},
  journal = {Annual Review of Genetics},
  keywords = {epigenomics},
  note = {PMID: 15568982},
  number = {1},
  pages = {413--443},
  posted-at = {2015-10-29 10:19:14},
  priority = {2},
  title = {{Epigenetic Regulation of Cellular Memory by the Polycomb and Trithorax Group Proteins}},
  volume = {38},
  year = {2004}
}

@article{Richards10,
  abstract = {{To explore the potential evolutionary relevance of heritable epigenetic variation, the National Evolutionary Synthesis Center recently hosted a catalysis meeting that brought together molecular epigeneticists, experimental evolutionary ecologists, and theoretical population and quantitative geneticists working across a wide variety of systems. The group discussed the methods available to investigate epigenetic variation and epigenetic inheritance, and how to evaluate their importance for phenotypic evolution. We found that understanding the relevance of epigenetic effects in phenotypic evolution will require clearly delineating epigenetics within existing terminology and expanding research efforts into ecologically relevant circumstances across model and nonmodel organisms. In addition, a critical component of understanding epigenetics will be the development of new and current statistical approaches and expansion of quantitative and population genetic theory. Although the importance of heritable epigenetic effects on evolution is still under discussion, investigating them in the context of a multidisciplinary approach could transform the field.}},
  author = {Richards, Christina L. and Bossdorf, Oliver and Pigliucci, Massimo},
  citeulike-article-id = {13816539},
  citeulike-linkout-0 = {http://dx.doi.org/10.1525/bio.2010.60.3.9},
  citeulike-linkout-1 = {http://bioscience.oxfordjournals.org/content/60/3/232.abstract},
  eprint = {http://bioscience.oxfordjournals.org/content/60/3/232.full.pdf+html},
  journal = {BioScience},
  keywords = {epigenomics},
  number = {3},
  pages = {232--237},
  posted-at = {2015-10-29 10:15:39},
  priority = {2},
  title = {{What Role Does Heritable Epigenetic Variation Play in Phenotypic Evolution?}},
  volume = {60},
  year = {2010}
}

@article{Romanoski15,
  author = {Romanoski, Casey E. and Glass, Christopher K. and Stunnenberg, Hendrik G. and Wilson, Laurence and Almouzni, Genevieve},
  citeulike-article-id = {13816532},
  citeulike-linkout-0 = {http://dx.doi.org/10.1038/518314a},
  journal = {Nature},
  keywords = {epigenomics},
  month = {print},
  number = {7539},
  pages = {314--316},
  posted-at = {2015-10-29 10:10:10},
  priority = {2},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  title = {{Epigenomics: Roadmap for regulation}},
  type = {News \& Views},
  volume = {518},
  year = {2015}
}

@article{Haller12,
  abstract = {{BACKGROUND AND PURPOSE: Brain MR imaging is routinely performed in the work-up of suspected PD, yet its role is essentially limited to the exclusion of other pathologies. We performed a pattern-recognition analysis based on DTI data to detect subjects with PD at the individual level.MATERIALS AND METHODS: We included 40 consecutive patients with Parkinsonism suggestive of PD who had DTI at 3T, brain 123I ioflupane SPECT (DaTSCAN), and extensive neurologic testing including follow-up (17 PD: age range, 67.8 ± 6.7 years; 9 women; 23 Other: consisting of atypical forms of Parkinsonism; age range, 67.2 ± 9.7 years; 7 women). Data analysis included group-level TBSS and individual-level SVM classification.RESULTS: At the group level, patients with PD versus Other had spatially consistent increase in FA and decrease in RD and MD in a bilateral network, predominantly in the right frontal white matter. At the individual level, SVM correctly classified patients with PD at the individual level with accuracies up to 97\%.CONCLUSIONS: Support vector machine–based pattern recognition of DTI data provides highly accurate detection of patients with PD among those with suspected PD at an individual level, which is potentially clinically applicable. Because most suspected subjects with PD undergo brain MR imaging, already existing MR imaging data may be reused; this practice is very cost-efficient.}},
  author = {Haller, S. and Badoud, S. and Nguyen, D. and Garibotto, V. and Lovblad, K. O. and Burkhard, P. R.},
  citeulike-article-id = {13816521},
  citeulike-linkout-0 = {http://dx.doi.org/10.3174/ajnr.A3126},
  citeulike-linkout-1 = {http://www.ajnr.org/content/33/11/2123.abstract},
  eprint = {http://www.ajnr.org/content/33/11/2123.full.pdf+html},
  journal = {American Journal of Neuroradiology},
  keywords = {neuroimaging},
  number = {11},
  pages = {2123--2128},
  posted-at = {2015-10-29 10:00:57},
  priority = {2},
  title = {{Individual Detection of Patients with Parkinson Disease using Support Vector Machine Analysis of Diffusion Tensor Imaging Data: Initial Results}},
  volume = {33},
  year = {2012}
}

@article{RizkJackson11,
  author = {Rizk-Jackson, Angela and Stoffers, Diederick and Sheldon, Sarah and Kuperman, Josh and Dale, Anders and Goldstein, Jody and Corey-Bloom, Jody and Poldrack, Russell A. and Aron, Adam R.},
  citeulike-article-id = {13816515},
  citeulike-linkout-0 = {http://dx.doi.org/DOI:\%2010.1016/j.neuroimage.2010.04.273},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S1053811910007019},
  journal = {NeuroImage},
  keywords = {disease, huntingtons, neuroimaging},
  note = {Multivariate Decoding and Brain Reading},
  number = {2},
  pages = {788--796},
  posted-at = {2015-10-29 09:53:11},
  priority = {2},
  title = {{Evaluating imaging biomarkers for neurodegeneration in pre-symptomatic Huntington's disease using machine learning techniques}},
  volume = {56},
  year = {2011}
}

@article{Davatzikos11,
  author = {Davatzikos, Christos and Bhatt, Priyanka and Shaw, Leslie M. and Batmanghelich, Kayhan N. and Trojanowski, John Q.},
  citeulike-article-id = {13816512},
  citeulike-linkout-0 = {http://dx.doi.org/http://dx.doi.org/10.1016/j.neurobiolaging.2010.05.023},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S019745801000237X},
  journal = {Neurobiology of Aging},
  keywords = {alzheimers, disease, neuroimaging},
  number = {12},
  posted-at = {2015-10-29 09:50:41},
  priority = {2},
  title = {Prediction of {MCI} to {AD} conversion, via {MRI}, {CSF} biomarkers, and pattern classification},
  volume = {32},
  year = {2011}
}

@article{Davatzikos09,
  abstract = {{A challenge in developing informative neuroimaging biomarkers for early diagnosis of Alzheimers disease is the need to identify biomarkers that are evident before the onset of clinical symptoms, and which have sufficient sensitivity and specificity on an individual patient basis. Recent literature suggests that spatial patterns of brain atrophy discriminate amongst Alzheimers disease, mild cognitive impairment (MCI) and cognitively normal (CN) older adults with high accuracy on an individual basis, thereby offering promise that subtle brain changes can be detected during prodromal Alzheimers disease stages. Here, we investigate whether these spatial patterns of brain atrophy can be detected in CN and MCI individuals and whether they are associated with cognitive decline. Images from the Alzheimers Disease Neuroimaging Initiative (ADNI) were used to construct a pattern classifier that recognizes spatial patterns of brain atrophy which best distinguish Alzheimers disease patients from CN on an individual person basis. This classifier was subsequently applied to longitudinal magnetic resonance imaging scans of CN and MCI participants in the Baltimore Longitudinal Study of Aging (BLSA) neuroimaging study. The degree to which Alzheimers disease-like patterns were present in CN and MCI subjects was evaluated longitudinally in relation to cognitive performance. The oldest BLSA CN individuals showed progressively increasing Alzheimers disease-like patterns of atrophy, and individuals with these patterns had reduced cognitive performance. MCI was associated with steeper longitudinal increases of Alzheimers disease-like patterns of atrophy, which separated them from CN (receiver operating characteristic area under the curve equal to 0.89). Our results suggest that imaging-based spatial patterns of brain atrophy of Alzheimers disease, evaluated with sophisticated pattern analysis and recognition methods, may be useful in discriminating among CN individuals who are likely to be stable versus those who will show cognitive decline. Future prospective studies will elucidate the temporal dynamics of spatial atrophy patterns and the emergence of clinical symptoms.}},
  author = {Davatzikos, C. and Xu, F. and An, Y. and Fan, Y. and Resnick, S. M.},
  citeulike-article-id = {13816508},
  journal = {Brain},
  keywords = {neuroimaging},
  note = {Times Cited: 7},
  pages = {2026--2035},
  posted-at = {2015-10-29 09:46:54},
  priority = {2},
  title = {{Longitudinal progression of Alzheimers-like patterns of atrophy in normal older adults: the SPARE-AD index}},
  volume = {132},
  year = {2009}
}

@article{Kloppel09,
  abstract = {Background: Treatment of neurodegenerative diseases is likely to be most beneficial in the very early, possibly preclinical stages of degeneration. We explored the usefulness of fully automatic structural {MRI} classification methods for detecting subtle degenerative change. The availability of a definitive genetic test for Huntington disease ({HD}) provides an excellent metric for judging the performance of such methods in gene mutation carriers who are free of symptoms. Methods: Using the gray matter segment of {MRI} scans, this study explored the usefulness of a multivariate support vector machine to automatically identify presymptomatic {HD} gene mutation carriers ({PSCs}) in the absence of any a priori information. A multicenter data set of 96 {PSCs} and 95 age- and sex-matched controls was studied. The {PSC} group was subclassified into three groups based on time from predicted clinical onset, an estimate that is a function of {DNA} mutation size and age. Results: Subjects with at least a 33\% chance of developing unequivocal signs of {HD} in 5 years were correctly assigned to the {PSC} group 69\% of the time. Accuracy improved to 83\% when regions affected by the disease were selected a priori for analysis. Performance was at chance when the probability of developing symptoms in 5 years was less than 10\%. Conclusions: Presymptomatic Huntington disease gene mutation carriers close to estimated diagnostic onset were successfully separated from controls on the basis of single anatomic scans, without additional a priori information. Prior information is required to allow separation when degenerative changes are either subtle or variable. 10.1212/01.wnl.0000341768.28646.b6},
  author = {Kl\"{o}ppel, S. and Chu, C. and Tan, G. C. and Draganski, B. and Johnson, H. and Paulsen, J. S. and Kienzle, W. and Tabrizi, S. J. and Ashburner, J. and Frackowiak, R. S. J. and of the Huntington Study Group, PREDICT-HD Investigators},
  citeulike-article-id = {13816503},
  citeulike-linkout-0 = {http://dx.doi.org/10.1212/01.wnl.0000341768.28646.b6},
  day = {3},
  journal = {Neurology},
  keywords = {classification, detection, machine\_learning, neurodegeneration, neuroimaging},
  month = feb,
  number = {5},
  pages = {426--431},
  posted-at = {2015-10-29 09:39:34},
  priority = {2},
  title = {{Automatic detection of preclinical neurodegeneration: Presymptomatic Huntington disease}},
  volume = {72},
  year = {2009}
}

@article{Marquand08,
  abstract = {The functional neuroanatomy of verbal working memory is a potential diagnostic biomarker for depression. Twenty patients with unipolar depression and 20 healthy controls performed a variable load version (n-back) of the task. Functional {MRI} data were analysed with support vector machine methods. Diagnostic classification was highest at the mid-level of task difficulty (2-back) (sensitivity 65\%, specificity 70\%, P<0.009). Significant classification of clinical response (>or=50\% reduction in clinical symptom ratings) was found at the most difficult level (3-back) (sensitivity 85\%, specificity 52\%, P<0.003). The functional neuroanatomy of verbal working memory provides a statistically significant but clinically moderate contribution as a diagnostic biomarker for depression, whereas its potential as a neural predictor of clinical response requires further investigation.},
  author = {Marquand, Andre F. and Mour\~{a}o Miranda, Janaina and Brammer, Michael J. and Cleare, Anthony J. and Fu, Cynthia H.},
  citeulike-article-id = {13816501},
  citeulike-linkout-0 = {http://dx.doi.org/10.1097/WNR.0b013e328310425e},
  day = {8},
  journal = {Neuroreport},
  keywords = {across\_subject, clinical, depression, fmri, machine\_learning, neuroimaging},
  month = oct,
  number = {15},
  pages = {1507--1511},
  posted-at = {2015-10-29 09:38:25},
  priority = {2},
  title = {{Neuroanatomy of verbal working memory as a diagnostic biomarker for depression.}},
  volume = {19},
  year = {2008}
}

@article{Kloppel08,
  author = {Kl{\"{o}}ppel, Stefan and Stonnington, Cynthia M. and Chu, Carlton and Draganski, Bogdan and Scahill, Rachael I. and Rohrer, Jonathan D. and Fox, Nick C. and Jack, Clifford R. and Ashburner, John and Frackowiak, Richard S. J.},
  citeulike-article-id = {13816497},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/brain/awm319},
  citeulike-linkout-1 = {http://brain.oxfordjournals.org/content/131/3/681.abstract},
  eprint = {http://brain.oxfordjournals.org/content/131/3/681.full.pdf+html},
  journal = {Brain},
  keywords = {neuroimaging},
  number = {3},
  pages = {681--689},
  posted-at = {2015-10-29 09:36:29},
  priority = {2},
  title = {Automatic classification of {MR} scans in {A}lzheimer's disease},
  volume = {131},
  year = {2008}
}

@article{Agullo09,
  abstract = {{The emergence and continuing use of multi-core architectures and graphics processing units require changes in the existing software and sometimes even a redesign of the established algorithms in order to take advantage of now prevailing parallelism. Parallel Linear Algebra for Scalable Multi-core Architectures (PLASMA) and Matrix Algebra on GPU and Multics Architectures (MAGMA) are two projects that aims to achieve high performance and portability across a wide range of multi-core architectures and hybrid systems respectively. We present in this document a comparative study of PLASMA's performance against established linear algebra packages and some preliminary results of MAGMA on hybrid multi-core and GPU systems.}},
  author = {Agullo, Emmanuel and Demmel, Jim and Dongarra, Jack and Hadri, Bilel and Kurzak, Jakub and Langou, Julien and Ltaief, Hatem and Luszczek, Piotr and Tomov, Stanimire},
  citeulike-article-id = {13816491},
  citeulike-linkout-0 = {http://stacks.iop.org/1742-6596/180/i=1/a=012037},
  journal = {Journal of Physics: Conference Series},
  keywords = {computing},
  number = {1},
  pages = {012037},
  posted-at = {2015-10-29 09:30:27},
  priority = {2},
  title = {{Numerical linear algebra on emerging architectures: The PLASMA and MAGMA projects}},
  volume = {180},
  year = {2009}
}

@book{Grama03,
  author = {Grama, A.},
  citeulike-article-id = {13815941},
  citeulike-linkout-0 = {https://books.google.fr/books?id=B3jR2EhdZaMC},
  keywords = {computing},
  posted-at = {2015-10-28 16:06:00},
  priority = {2},
  publisher = {Addison-Wesley},
  series = {Pearson Education},
  title = {{Introduction to Parallel Computing}},
  year = {2003}
}

@article{Mack11,
  author = {Mack, C. A.},
  citeulike-article-id = {13815924},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TSM.2010.2096437},
  journal = {Semiconductor Manufacturing, IEEE Transactions on},
  keywords = {access, circuitplanar, circuitseconomicsindustriesmanufacturingrandom, circuitssilicontransistorsic, complexitymoore, computing, curve, elemental, historysiintegrated, law, lawlearning, memorysilicontransistorshistorymoores, semiconductorshistoryintegrated, silicon, transistordriver},
  month = may,
  number = {2},
  pages = {202--207},
  posted-at = {2015-10-28 15:48:29},
  priority = {2},
  title = {{Fifty Years of Moore's Law}},
  volume = {24},
  year = {2011}
}

@article{Schaller97,
  author = {Schaller, R. R.},
  citeulike-article-id = {13815910},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/6.591665},
  journal = {IEEE Spectrum},
  keywords = {assumptionelectronic, circuit, complexitymoores, computing, consumer, electronicsintegrated, forecastingic, implicationsconsumer, innovation, lawbaseline, lawroadssilicontechnological, productsfuture, productsinnovationpolicy, productsmoores, trendshigh-technology, yieldtechnological},
  number = {6},
  pages = {52--59},
  posted-at = {2015-10-28 15:38:15},
  priority = {2},
  title = {{Moore's law: past, present and future}},
  volume = {34},
  year = {1997}
}

@article{Ghahramani15,
  author = {Ghahramani, Zoubin},
  citeulike-article-id = {13629733},
  citeulike-linkout-0 = {http://dx.doi.org/10.1038/nature14541},
  citeulike-linkout-1 = {http://dx.doi.org/10.1038/nature14541},
  day = {28},
  issn = {0028-0836},
  journal = {Nature},
  keywords = {artificial\_intelligence},
  month = may,
  number = {7553},
  pages = {452--459},
  posted-at = {2015-10-28 10:08:52},
  priority = {2},
  publisher = {Nature Research},
  title = {{Probabilistic machine learning and artificial intelligence}},
  volume = {521},
  year = {2015}
}

@book{Russell03,
  author = {Russell, Stuart J. and Norvig, Peter},
  citeulike-article-id = {13814256},
  edition = {2},
  keywords = {artificial\_intelligence},
  posted-at = {2015-10-27 10:21:28},
  priority = {2},
  publisher = {Pearson Education},
  title = {{Artificial Intelligence: A Modern Approach}},
  year = {2003}
}

@inproceedings{FilipponeICML15,
  author = {Filippone, Maurizio and Engler, Raphael},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, July 6-11, 2015},
  citeulike-article-id = {13648072},
  keywords = {mcmc},
  posted-at = {2015-06-17 14:18:03},
  priority = {2},
  title = {Enabling scalable stochastic gradient-based inference for {G}aussian processes by employing the {U}nbiased {LI}near {S}ystem {S}olv{E}r ({ULISSE})},
  year = {2015}
}

@proceedings{DBLP:conf/icml/2004,
  citeulike-article-id = {13615801},
  editor = {Brodley, Carla E.},
  keywords = {gaussian\_processes},
  posted-at = {2015-05-15 19:29:29},
  priority = {2},
  publisher = {{ACM}},
  series = {{ACM} International Conference Proceeding Series},
  title = {Machine Learning, Proceedings of the Twenty-first International Conference {(ICML} 2004), Banff, Alberta, Canada, July 4-8, 2004},
  volume = {69},
  year = {2004}
}

@inproceedings{Gramacy04,
  author = {Gramacy, Robert B. and Lee, Herbert K. H. and Macready, William G.},
  booktitle = {Machine Learning, Proceedings of the Twenty-first International Conference {(ICML} 2004), Banff, Alberta, Canada, July 4-8, 2004},
  citeulike-article-id = {13615800},
  citeulike-linkout-0 = {http://dx.doi.org/10.1145/1015330.1015367},
  citeulike-linkout-1 = {http://doi.acm.org/10.1145/1015330.1015367},
  editor = {Brodley, Carla E.},
  keywords = {gaussian\_processes},
  posted-at = {2015-05-15 19:29:29},
  priority = {2},
  publisher = {{ACM}},
  series = {{ACM} International Conference Proceeding Series},
  title = {{Parameter space exploration with Gaussian process trees}},
  volume = {69},
  year = {2004}
}

@article{Gilboa15,
  author = {Gilboa, Elad and Saatci, Yunus and Cunningham, John P.},
  citeulike-article-id = {13615798},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TPAMI.2013.192},
  citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2013.192},
  journal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  keywords = {*file-import-15-05-15},
  number = {2},
  pages = {424--436},
  posted-at = {2015-05-15 19:26:34},
  priority = {2},
  title = {{Scaling Multidimensional Inference for Structured Gaussian Processes}},
  volume = {37},
  year = {2015}
}

@misc{Vollmer15,
  abstract = {{Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data
sets is computationally expensive. Both the calculation of the acceptance
probability and the creation of informed proposals usually require an iteration
through the whole data set. The recently proposed stochastic gradient Langevin
dynamics (SGLD) method circumvents this problem in three ways: it generates
proposals which are only based on a subset of the data, it skips the
accept-reject step and it uses sequences of decreasing step-sizes. In (Teh,
Thi\'ery and Vollmer, 2014), we provided the mathematical foundations for the
decreasing step size SGLD for example, by establishing a central limit theorem.
However, the step size of the SGLD and its extensions is in practice not
decreased to zero. This is the focus of the present article. We characterise
the asymptotic bias explicitly and its dependence on the step size and the
variance of the stochastic gradient. On that basis we derive a modified SGLD
which removes the bias due to the variance of the stochastic gradient up to
first order in the step size. Moreover, we are able to obtain bounds on finite
time bias and the MSE. The theory is illustrated with a Gaussian toy model for
which the bias and the MSE for the estimation of moments can be obtained
explicitly. For this toy model we study the gain of the SGLD over the standard
Euler method in the limit of data items \$N\rightarrow \infty\$.}},
  author = {Vollmer, Sebastian J. and Zygalakis, Konstantinos C. and and Yee Whye Teh},
  citeulike-article-id = {13615792},
  citeulike-linkout-0 = {http://arxiv.org/abs/1501.00438},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1501.00438},
  day = {2},
  eprint = {1501.00438},
  keywords = {mcmc},
  month = jan,
  note = {arXiv:1501.00438},
  posted-at = {2015-05-15 19:06:56},
  priority = {2},
  title = {{(Non-) asymptotic properties of Stochastic Gradient Langevin Dynamics}},
  year = {2015}
}

@misc{Teh14,
  abstract = {Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data
sets is computationally expensive. Both the calculation of the acceptance
probability and the creation of informed proposals usually require an iteration
through the whole data set. The recently proposed stochastic gradient Langevin
dynamics (SGLD) method circumvents this problem by generating proposals which
are only based on a subset of the data, by skipping the accept-reject step and
by using decreasing step-sizes sequence \$(\delta\_m)\_{m \geq 0}\$.


\%Under appropriate Lyapunov conditions, We provide in this article a rigorous
mathematical framework for analysing this algorithm. We prove that, under
verifiable assumptions, the algorithm is consistent, satisfies a central limit
theorem (CLT) and its asymptotic bias-variance decomposition can be
characterized by an explicit functional of the step-sizes sequence
\$(\delta\_m)\_{m \geq 0}\$. We leverage this analysis to give practical
recommendations for the notoriously difficult tuning of this algorithm: it is
asymptotically optimal to use a step-size sequence of the type \$\delta\_m \asymp
m^{-1/3}\$, leading to an algorithm whose mean squared error (MSE) decreases at
rate \$\mathcal{O}(m^{-1/3})\$},
  author = {Teh, Yee W. and Thi\'{e}ry, Alexandre and Vollmer, Sebastian},
  citeulike-article-id = {13456667},
  citeulike-linkout-0 = {http://arxiv.org/abs/1409.0578},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1409.0578},
  day = {1},
  eprint = {1409.0578},
  keywords = {mcmc},
  month = sep,
  note = {arXiv:1409.0578},
  posted-at = {2015-05-15 19:01:17},
  priority = {2},
  title = {{Consistency and fluctuations for stochastic gradient Langevin dynamics}},
  year = {2014}
}

@article{Smidl14,
  author = {\v{S}m\'{\i}dl, V\'{a}clav and Hofman, Radek},
  citeulike-article-id = {13594074},
  citeulike-linkout-0 = {http://dx.doi.org/10.1080/00401706.2013.860917},
  journal = {Technometrics},
  keywords = {mcmc},
  number = {4},
  pages = {514--528},
  posted-at = {2015-04-28 13:28:55},
  priority = {2},
  title = {{Efficient Sequential Monte Carlo Sampling for Continuous Monitoring of a Radiation Situation}},
  volume = {56},
  year = {2014}
}

@article{Beskos13,
  author = {Beskos, A. and Pillai, N. and Roberts, G. O. and Sanz-Serna, J. M. and Stuart, A. M.},
  citeulike-article-id = {13593991},
  citeulike-linkout-0 = {http://dx.doi.org/10.3150/12-BEJ414},
  journal = {Bernoulli},
  keywords = {mcmc},
  pages = {1501--1534},
  posted-at = {2015-04-28 11:54:29},
  priority = {2},
  title = {{Optimal tuning of hybrid Monte Carlo algorithm}},
  volume = {19},
  year = {2013}
}

@proceedings{DBLP:conf/nips/2010c,
  citeulike-article-id = {13495998},
  citeulike-linkout-0 = {http://papers.nips.cc/book/advances-in-neural-information-processing-systems-23-2010},
  editor = {Lafferty, John D. and Williams, Christopher K. I. and Shawe{-}Taylor, John and Zemel, Richard S. and Culotta, Aron},
  keywords = {gaussian\_processes},
  posted-at = {2015-01-19 23:48:08},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada}},
  year = {2010}
}

@inproceedings{Murray10,
  author = {Murray, Iain and Adams, Ryan P.},
  booktitle = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada.},
  citeulike-article-id = {13495997},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/4114-slice-sampling-covariance-hyperparameters-of-latent-gaussian-models},
  editor = {Lafferty, John D. and Williams, Christopher K. I. and Shawe{-}Taylor, John and Zemel, Richard S. and Culotta, Aron},
  keywords = {gaussian\_processes},
  pages = {1732--1740},
  posted-at = {2015-01-19 23:48:07},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Slice sampling covariance hyperparameters of latent Gaussian models}},
  year = {2010}
}

@proceedings{DBLP:conf/nips/2000,
  citeulike-article-id = {13495996},
  citeulike-linkout-0 = {http://papers.nips.cc/book/advances-in-neural-information-processing-systems-13-2000},
  editor = {Leen, Todd K. and Dietterich, Thomas G. and Tresp, Volker},
  keywords = {kernel},
  posted-at = {2015-01-19 23:42:38},
  priority = {2},
  publisher = {{MIT} Press},
  title = {Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems {(NIPS)} 2000, Denver, CO, {USA}},
  year = {2001}
}

@inproceedings{Gray00,
  author = {Gray, Alexander G. and Moore, Andrew W.},
  booktitle = {Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems {(NIPS)} 2000, Denver, CO, {USA}},
  citeulike-article-id = {13495995},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/1848-n-body-problems-in-statistical-learning},
  editor = {Leen, Todd K. and Dietterich, Thomas G. and Tresp, Volker},
  keywords = {kernel},
  pages = {521--527},
  posted-at = {2015-01-19 23:42:38},
  priority = {2},
  publisher = {MIT Press},
  title = {{'N-Body' Problems in Statistical Learning}},
  year = {2000}
}

@proceedings{DBLP:conf/icml/2011,
  citeulike-article-id = {13495994},
  editor = {Getoor, Lise and Scheffer, Tobias},
  keywords = {mcmc},
  posted-at = {2015-01-19 23:37:15},
  priority = {2},
  publisher = {Omnipress},
  title = {Proceedings of the 28th International Conference on Machine Learning, {ICML} 2011, Bellevue, Washington, USA, June 28 - July 2, 2011},
  year = {2011}
}

@inproceedings{Welling11,
  author = {Welling, Max and Teh, Yee W.},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning, {ICML} 2011, Bellevue, Washington, USA, June 28 - July 2, 2011},
  citeulike-article-id = {13495993},
  editor = {Getoor, Lise and Scheffer, Tobias},
  keywords = {mcmc},
  pages = {681--688},
  posted-at = {2015-01-19 23:37:15},
  priority = {2},
  publisher = {Omnipress},
  title = {{Bayesian Learning via Stochastic Gradient Langevin Dynamics}},
  year = {2011}
}

@proceedings{DBLP:conf/aistats/2009,
  citeulike-article-id = {13495992},
  citeulike-linkout-0 = {http://jmlr.org/proceedings/papers/v5/},
  editor = {Dyk, David A. and Welling, Max},
  keywords = {gaussian\_processes},
  posted-at = {2015-01-19 23:35:39},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Proceedings},
  title = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2009, Clearwater Beach, Florida, USA, April 16-18, 2009},
  volume = {5},
  year = {2009}
}

@inproceedings{Titsias09,
  author = {Titsias, Michalis K.},
  booktitle = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, {AISTATS} 2009, Clearwater Beach, Florida, USA, April 16-18, 2009},
  citeulike-article-id = {13495991},
  citeulike-linkout-0 = {http://www.jmlr.org/proceedings/papers/v5/titsias09a.html},
  editor = {Dyk, David A. and Welling, Max},
  keywords = {gaussian\_processes},
  pages = {567--574},
  posted-at = {2015-01-19 23:35:39},
  priority = {2},
  publisher = {JMLR.org},
  series = {{JMLR} Proceedings},
  title = {{Variational Learning of Inducing Variables in Sparse Gaussian Processes}},
  volume = {5},
  year = {2009}
}

@proceedings{DBLP:conf/icpr/2014,
  citeulike-article-id = {13495988},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6966883},
  keywords = {gaussian\_processes},
  posted-at = {2015-01-19 23:26:27},
  priority = {2},
  publisher = {{IEEE}},
  title = {22nd International Conference on Pattern Recognition, {ICPR} 2014, Stockholm, Sweden, August 24-28, 2014},
  year = {2014}
}

@inproceedings{OHarneyICPR14,
  author = {O'Harney, Andrew D. and Marquand, Andre and Rubia, Katya and Chantiluke, Kaylita and Smith, Anna B. and Cubillo, Ana and Blain, Camilla and Filippone, Maurizio},
  booktitle = {22nd International Conference on Pattern Recognition, {ICPR} 2014, Stockholm, Sweden, August 24-28, 2014},
  citeulike-article-id = {13495987},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICPR.2014.549},
  keywords = {gaussian\_processes},
  pages = {3185--3190},
  posted-at = {2015-01-19 23:26:27},
  priority = {2},
  publisher = {{IEEE}},
  title = {Pseudo-Marginal {B}ayesian Multiple-Class Multiple-Kernel Learning for Neuroimaging Data},
  year = {2014}
}

@inproceedings{FilipponeICPR14,
  author = {Filippone, Maurizio},
  booktitle = {22nd International Conference on Pattern Recognition, {ICPR} 2014, Stockholm, Sweden, August 24-28, 2014},
  citeulike-article-id = {13495986},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICPR.2014.116},
  keywords = {gaussian\_processes},
  pages = {614--619},
  posted-at = {2015-01-19 23:26:26},
  priority = {2},
  publisher = {{IEEE}},
  title = {Bayesian Inference for {G}aussian Process Classifiers with Annealing and Pseudo-Marginal {MCMC}},
  year = {2014}
}

@article{Banerjee12,
  abstract = {{Gaussian processes are widely used in nonparametric regression, classification and spatiotemporal modelling, facilitated in part by a rich literature on their theoretical properties. However, one of their practical limitations is expensive computation, typically on the order of n3 where n is the number of data points, in performing the necessary matrix inversions. For large datasets, storage and processing also lead to computational bottlenecks, and numerical stability of the estimates and predicted values degrades with increasing n. Various methods have been proposed to address these problems, including predictive processes in spatial data analysis and the subset-of-regressors technique in machine learning. The idea underlying these approaches is to use a subset of the data, but this raises questions concerning sensitivity to the choice of subset and limitations in estimating fine-scale structure in regions that are not well covered by the subset. Motivated by the literature on compressive sensing, we propose an alternative approach that involves linear projection of all the data points onto a lower-dimensional subspace. We demonstrate the superiority of this approach from a theoretical perspective and through simulated and real data examples.}},
  author = {Banerjee, Anjishnu and Dunson, David B. and Tokdar, Surya T.},
  citeulike-article-id = {13495985},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/biomet/ass068},
  citeulike-linkout-1 = {http://biomet.oxfordjournals.org/content/100/1/75.abstract},
  eprint = {http://biomet.oxfordjournals.org/content/100/1/75.full.pdf+html},
  journal = {Biometrika},
  keywords = {gaussian\_processes},
  number = {1},
  pages = {75--89},
  posted-at = {2015-01-19 23:23:06},
  priority = {2},
  title = {{Efficient Gaussian process regression for large datasets}},
  volume = {100},
  year = {2013}
}

@article{Kennedy85,
  author = {Kennedy, A. D. and Kuti, J.},
  citeulike-article-id = {13495542},
  citeulike-linkout-0 = {http://dx.doi.org/10.1103/PhysRevLett.54.2473},
  citeulike-linkout-1 = {http://link.aps.org/doi/10.1103/PhysRevLett.54.2473},
  journal = {Physical Review Letters},
  keywords = {mcmc},
  pages = {2473--2476},
  posted-at = {2015-01-19 11:56:19},
  priority = {2},
  publisher = {American Physical Society},
  title = {{Noise without Noise: A New Monte Carlo Method}},
  volume = {54},
  year = {1985}
}

@incollection{Liu00,
  author = {Liu, Keh-Fei},
  booktitle = {Numerical Challenges in Lattice Quantum Chromodynamics},
  citeulike-article-id = {13495539},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-58333-9\_11},
  editor = {Frommer, Andreas and Lippert, Thomas and Medeke, Bj\"{o}rn and Schilling, Klaus},
  keywords = {mcmc},
  pages = {142--152},
  posted-at = {2015-01-19 11:55:35},
  priority = {2},
  publisher = {Springer Berlin Heidelberg},
  series = {Lecture Notes in Computational Science and Engineering},
  title = {{A Noisy Monte Carlo Algorithm with Fermion Determinant}},
  volume = {15},
  year = {2000}
}

@proceedings{DBLP:conf/nips/1995,
  citeulike-article-id = {13495461},
  citeulike-linkout-0 = {http://papers.nips.cc/book/advances-in-neural-information-processing-systems-8-1995},
  editor = {Touretzky, David S. and Mozer, Michael and Hasselmo, Michael E.},
  keywords = {gaussian\_processes},
  posted-at = {2015-01-19 10:17:42},
  priority = {2},
  publisher = {{MIT} Press},
  title = {{Advances in Neural Information Processing Systems 8, NIPS, Denver, CO, November 27-30, 1995}},
  year = {1996}
}

@inproceedings{Williams95,
  author = {Williams, Christopher K. I. and Rasmussen, Carl E.},
  booktitle = {Advances in Neural Information Processing Systems 8, NIPS, Denver, CO, November 27-30, 1995},
  citeulike-article-id = {13495460},
  citeulike-linkout-0 = {http://papers.nips.cc/paper/1048-gaussian-processes-for-regression},
  editor = {Touretzky, David S. and Mozer, Michael and Hasselmo, Michael E.},
  keywords = {gaussian\_processes},
  pages = {514--520},
  posted-at = {2015-01-19 10:17:42},
  priority = {2},
  publisher = {{MIT} Press},
  title = {{Gaussian Processes for Regression}},
  year = {1995}
}

@inproceedings{Moore00,
  author = {Moore, Andrew},
  booktitle = {Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence},
  citeulike-article-id = {13494258},
  keywords = {kernels},
  pages = {397--405},
  posted-at = {2015-01-16 16:07:58},
  priority = {2},
  publisher = {AAAI Press},
  title = {{The Anchors Hierarchy: Using the Triangle Inequality to Survive High-Dimensional Data}},
  year = {2000}
}

@misc{Maclaurin14,
  abstract = {{Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose
tool for Bayesian inference. However, MCMC cannot be practically applied to
large data sets because of the prohibitive cost of evaluating every likelihood
term at every iteration. Here we present Firefly Monte Carlo (FlyMC) an
auxiliary variable MCMC algorithm that only queries the likelihoods of a
potentially small subset of the data at each iteration yet simulates from the
exact posterior distribution, in contrast to recent proposals that are
approximate even in the asymptotic limit. FlyMC is compatible with a wide
variety of modern MCMC algorithms, and only requires a lower bound on the
per-datum likelihood factors. In experiments, we find that FlyMC generates
samples from the posterior more than an order of magnitude faster than regular
MCMC, opening up MCMC methods to larger datasets than were previously
considered feasible.}},
  author = {Maclaurin, Dougal and Adams, Ryan P.},
  citeulike-article-id = {13117317},
  citeulike-linkout-0 = {http://arxiv.org/abs/1403.5693},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1403.5693},
  day = {22},
  eprint = {1403.5693},
  keywords = {mcmc},
  month = mar,
  note = {arXiv:1403.5693},
  posted-at = {2015-01-16 13:58:53},
  priority = {2},
  title = {{Firefly Monte Carlo: Exact MCMC with Subsets of Data}},
  year = {2014}
}

@misc{Hensman13,
  abstract = {{We introduce stochastic variational inference for Gaussian process models.
This enables the application of Gaussian process (GP) models to data sets
containing millions of data points. We show how GPs can be vari- ationally
decomposed to depend on a set of globally relevant inducing variables which
factorize the model in the necessary manner to perform variational inference.
Our ap- proach is readily extended to models with non-Gaussian likelihoods and
latent variable models based around Gaussian processes. We demonstrate the
approach on a simple toy problem and two real world data sets.}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
  citeulike-article-id = {12658572},
  citeulike-linkout-0 = {http://arxiv.org/abs/1309.6835},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1309.6835},
  day = {26},
  eprint = {1309.6835},
  keywords = {gaussian\_processes},
  month = sep,
  note = {arXiv:1309.6835},
  posted-at = {2015-01-16 13:57:02},
  priority = {2},
  title = {{Gaussian Processes for Big Data}},
  year = {2013}
}

@phdthesis{Saatci11,
  author = {Saat\c{c}i, Yunus},
  citeulike-article-id = {13494183},
  keywords = {gaussian\_processes},
  posted-at = {2015-01-16 13:48:59},
  priority = {2},
  school = {University of Cambridge},
  title = {{Scalable Inference for Structured Gaussian Process Models}},
  year = {2011}
}

@article{Thron98,
  author = {Thron, C. and Dong, S. J. and Liu, K. F. and Ying, H. P.},
  citeulike-article-id = {13492631},
  citeulike-linkout-0 = {http://dx.doi.org/10.1103/PhysRevD.57.1642},
  citeulike-linkout-1 = {http://link.aps.org/doi/10.1103/PhysRevD.57.1642},
  journal = {Phys. Rev. D},
  keywords = {algebra},
  pages = {1642--1653},
  posted-at = {2015-01-15 15:48:13},
  priority = {2},
  publisher = {American Physical Society},
  title = {Pad\'{e}-\${Z}\_{2}\$ estimator of determinants},
  volume = {57},
  year = {1998}
}

@article{Kennedy01,
  author = {Kennedy, Marc C. and O'Hagan, Anthony},
  citeulike-article-id = {13492619},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/1467-9868.00294},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {analysis, calibration, computer, deterministic, experiments, gaussian, gaussian\_processes, inadequacy, interpolation, model, models, process, sensitivity, uncertainty},
  number = {3},
  pages = {425--464},
  posted-at = {2015-01-15 15:27:14},
  priority = {2},
  publisher = {Blackwell Publishers Ltd.},
  title = {{Bayesian calibration of computer models}},
  volume = {63},
  year = {2001}
}

@article{Baboulin09,
  author = {Baboulin, Marc and Buttari, Alfredo and Dongarra, Jack and Kurzak, Jakub and Langou, Julie and Langou, Julien and Luszczek, Piotr and Tomov, Stanimire},
  citeulike-article-id = {13446772},
  citeulike-linkout-0 = {http://dx.doi.org/http://dx.doi.org/10.1016/j.cpc.2008.11.005},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/pii/S0010465508003846},
  journal = {Computer Physics Communications},
  keywords = {algebra, linear, numerical},
  number = {12},
  pages = {2526--2533},
  posted-at = {2014-11-27 21:36:07},
  priority = {2},
  title = {{Accelerating scientific computations with mixed precision algorithms}},
  volume = {180},
  year = {2009}
}

@misc{Hennig14,
  abstract = {{This manuscript proposes a probabilistic framework for algorithms that
iteratively solve unconstrained linear problems \$Bx = b\$ with positive definite
\$B\$ for \$x\$. The goal is to replace the point estimates returned by existing
methods with a Gaussian posterior belief over the elements of the inverse of
\$B\$, which can be used to estimate errors. Recent probabilistic interpretations
of the secant family of quasi-Newton optimization algorithms are extended.
Combined with properties of the conjugate gradient algorithm, this leads to
uncertainty-calibrated methods with very limited cost overhead over conjugate
gradients, a self-contained novel interpretation of the quasi-Newton and
conjugate gradient algorithms, and a foundation for new nonlinear optimization
methods.}},
  author = {Hennig, Philipp},
  citeulike-article-id = {13446767},
  citeulike-linkout-0 = {http://arxiv.org/abs/1402.2058},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1402.2058},
  day = {15},
  eprint = {1402.2058},
  keywords = {algebra},
  month = oct,
  note = {arXiv:1402.2058},
  posted-at = {2014-11-27 21:32:24},
  priority = {2},
  title = {{Probabilistic Interpretation of Linear Solvers}},
  year = {2014}
}

@incollection{Cevahir09,
  author = {Cevahir, Ali and Nukada, Akira and Matsuoka, Satoshi},
  booktitle = {Computational Science – ICCS 2009},
  citeulike-article-id = {13446766},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-01970-8\_90},
  editor = {Allen, Gabrielle and Nabrzyski, Jaros{\l}aw and Seidel, Edward and van Albada, GeertDick and Dongarra, Jack and Sloot, PeterM},
  keywords = {gpu},
  pages = {893--903},
  posted-at = {2014-11-27 21:30:05},
  priority = {2},
  publisher = {Springer Berlin Heidelberg},
  series = {Lecture Notes in Computer Science},
  title = {{Fast Conjugate Gradients with Multiple GPUs}},
  volume = {5544},
  year = {2009}
}

@misc{Jang11,
  abstract = {{GPU has a significantly higher performance in single-precision computing than
that of double precision. Hence, it is important to take a maximal advantage of
the single precision in the CG inverter, using the mixed precision method. We
have implemented mixed precision algorithm to our multi GPU conjugate gradient
solver. The single precision calculation use half of the memory that is used by
the double precision calculation, which allows twice faster data transfer in
memory I/O. In addition, the speed of floating point calculations is 8 times
faster in single precision than in double precision. The overall performance of
our CUDA code for CG is 145 giga flops per GPU (GTX480), which does not include
the infiniband network communication. If we include the infiniband
communication, the overall performance is 36 giga flops per GPU (GTX480).}},
  author = {Jang, Yong-Chull and Kim, Hyung-Jin and Lee, Weonjong},
  citeulike-article-id = {9978850},
  citeulike-linkout-0 = {http://arxiv.org/abs/1111.0125},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1111.0125},
  day = {1},
  eprint = {1111.0125},
  keywords = {gpu},
  month = nov,
  note = {arXiv:1111.0125},
  posted-at = {2014-11-27 21:25:05},
  priority = {2},
  title = {{Multi GPU Performance of Conjugate Gradient Solver with Staggered Fermions in Mixed Precision}},
  year = {2011}
}

@book{Higham08,
  address = {Philadelphia, PA, USA},
  author = {Higham, Nicholas J.},
  citeulike-article-id = {13446762},
  keywords = {algebra},
  posted-at = {2014-11-27 21:15:44},
  priority = {2},
  publisher = {Society for Industrial and Applied Mathematics},
  title = {Functions of Matrices: {Theory} and Computation},
  year = {2008}
}

@article{Antinescu12,
  author = {Anitescu, M. and Chen, J. and Wang, L.},
  citeulike-article-id = {13446760},
  citeulike-linkout-0 = {http://dx.doi.org/10.1137/110831143},
  eprint = {http://dx.doi.org/10.1137/110831143},
  journal = {SIAM Journal on Scientific Computing},
  keywords = {gaussian\_processes},
  number = {1},
  pages = {A240--A262},
  posted-at = {2014-11-27 21:11:49},
  priority = {2},
  title = {{A Matrix-free Approach for Solving the Parametric Gaussian Process Maximum Likelihood Problem}},
  volume = {34},
  year = {2012}
}

@article{Chen11,
  author = {Chen, J. and Anitescu, M. and Saad, Y.},
  citeulike-article-id = {13446759},
  citeulike-linkout-0 = {http://dx.doi.org/10.1137/090778250},
  journal = {SIAM Journal on Scientific Computing},
  keywords = {gaussian\_processes},
  number = {1},
  pages = {195--222},
  posted-at = {2014-11-27 21:09:49},
  priority = {2},
  title = {{Computing f(A)b via Least Squares Polynomial Approximations}},
  volume = {33},
  year = {2011}
}

@article{Stein13,
  author = {Stein, Michael L. and Chen, Jie and Anitescu, Mihai},
  citeulike-article-id = {13446758},
  citeulike-linkout-0 = {http://dx.doi.org/10.1214/13-AOAS627},
  journal = {The Annals of Applied Statistics},
  keywords = {gaussian\_processes},
  number = {2},
  pages = {1162--1191},
  posted-at = {2014-11-27 21:06:30},
  priority = {2},
  publisher = {The Institute of Mathematical Statistics},
  title = {{Stochastic approximation of score functions for Gaussian processes}},
  volume = {7},
  year = {2013}
}

@misc{Srinivasan14,
  abstract = {A primary computational problem in kernel regression is solution of a dense
linear system with the \$N\times N\$ kernel matrix. Because a direct solution has
an O(\$N^3\$) cost, iterative Krylov methods are often used with fast
matrix-vector products. For poorly conditioned problems, convergence of the
iteration is slow and preconditioning becomes necessary. We investigate
preconditioning from the viewpoint of scalability and efficiency. The problems
that conventional preconditioners face when applied to kernel methods are
demonstrated. A \emph{novel flexible preconditioner }that not only improves
convergence but also allows utilization of fast kernel matrix-vector products
is introduced. The performance of this preconditioner is first illustrated on
synthetic data, and subsequently on a suite of test problems in kernel
regression and geostatistical kriging.},
  author = {Srinivasan, Balaji V. and Hu, Qi and Gumerov, Nail A. and Murtugudde, Raghu and Duraiswami, Ramani},
  citeulike-article-id = {13446554},
  citeulike-linkout-0 = {http://arxiv.org/abs/1408.1237},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1408.1237},
  day = {6},
  eprint = {1408.1237},
  keywords = {kernel},
  month = aug,
  note = {arXiv:1408.1237},
  posted-at = {2014-11-27 13:59:12},
  priority = {2},
  title = {{Preconditioned Krylov solvers for kernel regression}},
  year = {2014}
}

@article{Lyne14,
  abstract = {{A large number of statistical models are "doubly-intractable": the likelihood
normalising term, which is a function of the model parameters, is intractable,
as well as the marginal likelihood (model evidence). This means that standard
inference techniques to sample from the posterior, such as Markov chain Monte
Carlo (MCMC), cannot be used. Examples include, but are not confined to,
massive Gaussian Markov random fields, autologistic models and Exponential
random graph models. A number of approximate schemes based on MCMC techniques,
Approximate Bayesian computation (ABC) or analytic approximations to the
posterior have been suggested, and these are reviewed here. Exact MCMC schemes,
which can be applied to a subset of doubly-intractable distributions, have also
been developed and are described in this paper. As yet, no general method
exists which can be applied to all classes of models with doubly-intractable
posteriors. In addition, taking inspiration from the Physics literature, we
study an alternative method based on representing the intractable likelihood as
an infinite series. Unbiased estimates of the likelihood can then be obtained
by finite time stochastic truncation of the series via Russian Roulette
sampling, although the estimates are not necessarily positive. Results from the
Quantum Chromodynamics literature are exploited to allow the use of possibly
negative estimates in a pseudo-marginal MCMC scheme such that expectations with
respect to the posterior distribution are preserved. The methodology is
reviewed on well-known examples such as the parameters in Ising models, the
posterior for Fisher-Bingham distributions on the \$d\$-Sphere and a large-scale
Gaussian Markov Random Field model describing the Ozone Column data. This leads
to a critical assessment of the strengths and weaknesses of the methodology
with pointers to ongoing research.}},
  author = {Lyne, Anne-Marie and Girolami, Mark and Atchade, Yves and Strathmann, Heiko and Simpson, Daniel},
  citeulike-article-id = {12438320},
  citeulike-linkout-0 = {http://arxiv.org/abs/1306.4032},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1306.4032},
  citeulike-linkout-2 = {http://dx.doi.org/10.1214/15-sts523},
  day = {10},
  eprint = {1306.4032},
  issn = {0883-4237},
  journal = {Statistical Science},
  keywords = {gaussian\_processes, mcmc},
  month = dec,
  note = {arXiv:1306.4032},
  number = {4},
  pages = {443--467},
  posted-at = {2014-11-27 13:56:30},
  priority = {2},
  title = {{On Russian Roulette Estimates for Bayesian Inference with Doubly-Intractable Likelihoods}},
  volume = {30},
  year = {2015}
}

@techreport{Seeger00,
  author = {Seeger, Matthias},
  citeulike-article-id = {701159},
  howpublished = {http://www.dai.ed.ac.uk/\~{}seeger/papers.html},
  institution = {Institute for ANC, Edinburgh, UK},
  keywords = {gaussian\_processes},
  posted-at = {2014-11-27 13:52:07},
  priority = {2},
  title = {{Skilling techniques for Bayesian analysis}},
  year = {2000}
}

@incollection{Skilling93,
  author = {Skilling, John},
  booktitle = {Physics and Probability},
  citeulike-article-id = {13446548},
  citeulike-linkout-0 = {http://dx.doi.org/10.1017/CBO9780511524448.020},
  editor = {Grandy, W. T. and Milonni, P. W.},
  keywords = {gaussian\_processes},
  note = {Cambridge Books Online},
  pages = {207--222},
  posted-at = {2014-11-27 13:50:34},
  priority = {2},
  publisher = {Cambridge University Press},
  title = {{Bayesian Numerical Analysis}},
  year = {1993}
}

@misc{Murray09,
  author = {Murray, Iain},
  citeulike-article-id = {13446539},
  citeulike-linkout-0 = {http://www.cs.toronto.edu/\~{}murray/pub/09gp\_eval/},
  keywords = {gaussian\_processes},
  note = {Presented at the Numerical Mathematics in Machine Learning workshop at the 26th International Conference on Machine Learning (ICML 2009), Montreal, Canada.},
  posted-at = {2014-11-27 13:40:41},
  priority = {2},
  title = {{G}aussian processes and fast matrix-vector multiplies},
  year = {2009}
}

@phdthesis{GibbsPhD97,
  author = {Gibbs, M. N.},
  citeulike-article-id = {13446537},
  keywords = {gaussian\_processes},
  posted-at = {2014-11-27 13:38:21},
  priority = {2},
  school = {University of Cambridge},
  title = {{Bayesian} {G}aussian processes for regression and classification},
  year = {1997}
}

@techreport{Gibbs97,
  address = {Cambridge, UK},
  author = {Gibbs, Mark and MacKay, David J. C.},
  citeulike-article-id = {12738074},
  institution = {Cavendish Laboratory},
  keywords = {gaussian\_processes},
  posted-at = {2014-11-27 13:35:45},
  priority = {2},
  title = {{Efficient Implementation of Gaussian Processes}},
  year = {1997}
}

@misc{Banterle14,
  abstract = {{MCMC algorithms such as Metropolis-Hastings algorithms are slowed down by the
computation of complex target distributions as exemplified by huge datasets. We
offer in this paper an approach to reduce the computational costs of such
algorithms by a simple and universal divide-and-conquer strategy. The idea
behind the generic acceleration is to divide the acceptance step into several
parts, aiming at a major reduction in computing time that outranks the
corresponding reduction in acceptance probability. The division decomposes the
"prior x likelihood" term into a product such that some of its components are
much cheaper to compute than others. Each of the components can be sequentially
compared with a uniform variate, the first rejection signalling that the
proposed value is considered no further, This approach can in turn be
accelerated as part of a prefetching algorithm taking advantage of the parallel
abilities of the computer at hand. We illustrate those accelerating features on
a series of toy and realistic examples.}},
  author = {Banterle, Marco and Grazian, Clara and Robert, Christian P.},
  citeulike-article-id = {13218299},
  citeulike-linkout-0 = {http://arxiv.org/abs/1406.2660},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1406.2660},
  day = {10},
  eprint = {1406.2660},
  keywords = {mcmc},
  month = jun,
  note = {arXiv:1406.2660},
  posted-at = {2014-11-27 13:28:28},
  priority = {2},
  title = {{Accelerating Metropolis-Hastings algorithms: Delayed acceptance with prefetching}},
  year = {2014}
}

@misc{Ranganath13,
  abstract = {{Variational inference has become a widely used method to approximate
posteriors in complex latent variables models. However, deriving a variational
inference algorithm generally requires significant model-specific analysis, and
these efforts can hinder and deter us from quickly developing and exploring a
variety of models for a problem at hand. In this paper, we present a "black
box" variational inference algorithm, one that can be quickly applied to many
models with little additional derivation. Our method is based on a stochastic
optimization of the variational objective where the noisy gradient is computed
from Monte Carlo samples from the variational distribution. We develop a number
of methods to reduce the variance of the gradient, always maintaining the
criterion that we want to avoid difficult model-based derivations. We evaluate
our method against the corresponding black box sampling based methods. We find
that our method reaches better predictive likelihoods much faster than sampling
methods. Finally, we demonstrate that Black Box Variational Inference lets us
easily explore a wide space of models by quickly constructing and evaluating
several models of longitudinal healthcare data.}},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
  citeulike-article-id = {12898741},
  citeulike-linkout-0 = {http://arxiv.org/abs/1401.0118},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1401.0118},
  day = {31},
  eprint = {1401.0118},
  keywords = {variational\_inference},
  month = dec,
  note = {arXiv:1401.0118},
  posted-at = {2014-11-27 12:01:26},
  priority = {2},
  title = {{Black Box Variational Inference}},
  year = {2013}
}

@article{Robbins51,
  author = {Robbins, Herbert and Monro, Sutton},
  citeulike-article-id = {13446477},
  citeulike-linkout-0 = {http://dx.doi.org/10.1214/aoms/1177729586},
  journal = {The Annals of Mathematical Statistics},
  keywords = {stochastic\_gradient},
  pages = {400--407},
  posted-at = {2014-11-27 11:59:52},
  priority = {2},
  title = {{A Stochastic Approximation Method}},
  volume = {22},
  year = {1951}
}

@article{Duchi11,
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  citeulike-article-id = {13446471},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
  journal = {Journal of Machine Learning Research},
  keywords = {stochastic\_gradient},
  month = jul,
  pages = {2121--2159},
  posted-at = {2014-11-27 11:54:15},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
  volume = {12},
  year = {2011}
}

@misc{Simpson13,
  abstract = {{Sampling from Gaussian Markov random fields (GMRFs), that is multivariate
Gaussian ran- dom vectors that are parameterised by the inverse of their
covariance matrix, is a fundamental problem in computational statistics. In
this paper, we show how we can exploit arbitrarily accu- rate approximations to
a GMRF to speed up Krylov subspace sampling methods. We also show that these
methods can be used when computing the normalising constant of a large
multivariate Gaussian distribution, which is needed for both any
likelihood-based inference method. The method we derive is also applicable to
other structured Gaussian random vectors and, in particu- lar, we show that
when the precision matrix is a perturbation of a (block) circulant matrix, it
is still possible to derive O(n log n) sampling schemes.}},
  author = {Simpson, Daniel P. and Turner, Ian W. and Strickland, Christopher M. and Pettitt, Anthony N.},
  citeulike-article-id = {13446467},
  citeulike-linkout-0 = {http://arxiv.org/abs/1312.1476},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1312.1476},
  day = {5},
  eprint = {1312.1476},
  keywords = {gaussian\_processes, mcmc},
  month = dec,
  note = {arXiv:1312.1476},
  posted-at = {2014-11-27 11:50:59},
  priority = {2},
  title = {{Scalable iterative methods for sampling from massive Gaussian random vectors}},
  year = {2013}
}

@article{FilipponeIEEETPAMI14,
  author = {Filippone, Maurizio and Girolami, Mark},
  citeulike-article-id = {13446462},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TPAMI.2014.2316530},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {mcmc},
  number = {11},
  pages = {2214--2226},
  posted-at = {2014-11-27 11:43:50},
  priority = {2},
  title = {Pseudo-Marginal {B}ayesian Inference for {G}aussian Processes},
  volume = {36},
  year = {2014}
}

@article{ZZZ,
  abstract = {{BACKGROUND:Support Vector Machines (SVMs) - using a variety of string kernels - have been successfully applied to biological sequence classification problems. While SVMs achieve high classification accuracy they lack interpretability. In many applications, it does not suffice that an algorithm just detects a biological signal in the sequence, but it should also provide means to interpret its solution in order to gain biological insight.RESULTS:We propose novel and efficient algorithms for solving the so-called Support Vector Multiple Kernel Learning problem. The developed techniques can be used to understand the obtained support vector decision function in order to extract biologically relevant knowledge about the sequence analysis problem at hand. We apply the proposed methods to the task of acceptor splice site prediction and to the problem of recognizing alternatively spliced exons. Our algorithms compute sparse weightings of substring locations, highlighting which parts of the sequence are important for discrimination.CONCLUSION:The proposed method is able to deal with thousands of examples while combining hundreds of kernels within reasonable time, and reliably identifies a few statistically significant positions.}},
  author = {Ratsch, Gunnar and Sonnenburg, Soren and Schafer, Christin},
  citeulike-article-id = {6620971},
  citeulike-linkout-0 = {http://dx.doi.org/10.1186/1471-2105-7-s1-s9},
  citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/16723012},
  citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=16723012},
  issn = {1471-2105},
  journal = {BMC Bioinformatics},
  keywords = {zzz},
  number = {Suppl 1},
  pages = {S9+},
  pmid = {16723012},
  posted-at = {2014-05-05 14:08:54},
  priority = {2},
  title = {{Learning Interpretable SVMs for Biological Sequence Classification}},
  volume = {7},
  year = {2006}
}

@article{MarquandPLOSONE13,
  abstract = {{Progressive supranuclear palsy (PSP), multiple system atrophy (MSA) and idiopathic Parkinson's disease (IPD) can be clinically indistinguishable, especially in the early stages, despite distinct patterns of molecular pathology. Structural neuroimaging holds promise for providing objective biomarkers for discriminating these diseases at the single subject level but all studies to date have reported incomplete separation of disease groups. In this study, we employed multi-class pattern recognition to assess the value of anatomical patterns derived from a widely available structural neuroimaging sequence for automated classification of these disorders. To achieve this, 17 patients with PSP, 14 with IPD and 19 with MSA were scanned using structural MRI along with 19 healthy controls (HCs). An advanced probabilistic pattern recognition approach was employed to evaluate the diagnostic value of several pre-defined anatomical patterns for discriminating the disorders, including: (i) a subcortical motor network; (ii) each of its component regions and (iii) the whole brain. All disease groups could be discriminated simultaneously with high accuracy using the subcortical motor network. The region providing the most accurate predictions overall was the midbrain/brainstem, which discriminated all disease groups from one another and from HCs. The subcortical network also produced more accurate predictions than the whole brain and all of its constituent regions. PSP was accurately predicted from the midbrain/brainstem, cerebellum and all basal ganglia compartments; MSA from the midbrain/brainstem and cerebellum and IPD from the midbrain/brainstem only. This study demonstrates that automated analysis of structural MRI can accurately predict diagnosis in individual patients with Parkinsonian disorders, and identifies distinct patterns of regional atrophy particularly useful for this process. }},
  author = {Marquand, Andre F. and Filippone, Maurizio and Ashburner, John and Girolami, Mark and Mourao-Miranda, Janaina and Barker, Gareth J. and Williams, Steven C. R. and Leigh, P. Nigel and Blain, Camilla R. V.},
  citeulike-article-id = {13073647},
  citeulike-linkout-0 = {http://dx.doi.org/10.1371/journal.pone.0069237},
  day = {15},
  journal = {PLoS ONE},
  keywords = {classification},
  month = jul,
  number = {7},
  pages = {e69237+},
  posted-at = {2014-02-27 09:22:17},
  priority = {2},
  publisher = {Public Library of Science},
  title = {{Automated, High Accuracy Classification of Parkinsonian Disorders: A Pattern Recognition Approach}},
  volume = {8},
  year = {2013}
}

@article{Behrens10,
  abstract = {{The method of tempered transitions was proposed by Neal (1996) for tackling
the difficulties arising when using Markov chain Monte Carlo to sample from
multimodal distributions. In common with methods such as simulated tempering
and Metropolis-coupled MCMC, the key idea is to utilise a series of
successively easier to sample distributions to improve movement around the
state space. Tempered transitions does this by incorporating moves through
these less modal distributions into the MCMC proposals. Unfortunately the
improved movement between modes comes at a high computational cost with a low
acceptance rate of expensive proposals. We consider how the algorithm may be
tuned to increase the acceptance rates for a given number of temperatures. We
find that the commonly assumed geometric spacing of temperatures is reasonable
in many but not all applications.}},
  author = {Behrens, Gundula and Friel, Nial and Hurn, Merrilee},
  citeulike-article-id = {8026934},
  citeulike-linkout-0 = {http://arxiv.org/abs/1010.0842},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1010.0842},
  citeulike-linkout-2 = {http://dx.doi.org/10.1007/s11222-010-9206-z},
  citeulike-linkout-3 = {http://www.springerlink.com/content/r050834657k730x2},
  day = {5},
  eprint = {1010.0842},
  issn = {0960-3174},
  journal = {Statistics and Computing},
  keywords = {mcmc},
  month = oct,
  number = {1},
  pages = {65--78},
  posted-at = {2013-11-21 15:19:33},
  priority = {2},
  publisher = {Springer Netherlands},
  title = {{Tuning Tempered Transitions}},
  volume = {22},
  year = {2010}
}

@article{Neal96b,
  author = {Neal, Radford M.},
  citeulike-article-id = {9054881},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF00143556},
  citeulike-linkout-1 = {http://www.springerlink.com.cyber.usask.ca/content/x6417mm4h7845735/},
  journal = {Statistics and Computing},
  keywords = {mcmc},
  pages = {353--366},
  posted-at = {2013-11-21 14:38:44},
  priority = {2},
  title = {{Sampling from multimodal distributions using tempered transitions}},
  volume = {6},
  year = {1996}
}

@book{Shawe-Taylor04,
  address = {New York, NY, USA},
  author = {Shawe-Taylor, John and Cristianini, Nello},
  citeulike-article-id = {12781358},
  keywords = {kernels},
  posted-at = {2013-11-11 10:59:14},
  priority = {2},
  publisher = {Cambridge University Press},
  title = {{Kernel Methods for Pattern Analysis}},
  year = {2004}
}

@article{Rue04,
  author = {Rue, H\r{a}vard and Steinsland, Ingelin and Erland, Sveinung},
  citeulike-article-id = {12734600},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2004.B5590.x},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {autoregressive, block, carlo, chain, conditional, field, gaussian, gp, hidden, independence, markov, methods, metropolized, model, models, monte, random, sampler, sampling, sequential},
  number = {4},
  pages = {877--892},
  posted-at = {2013-10-24 13:16:05},
  priority = {2},
  publisher = {Blackwell Publishing},
  title = {{Approximating hidden Gaussian Markov random fields}},
  volume = {66},
  year = {2004}
}

@article{Neal01,
  address = {Hingham, MA, USA},
  author = {Neal, Radford M.},
  citeulike-article-id = {12734536},
  citeulike-linkout-0 = {http://dx.doi.org/10.1023/A:1008923215028},
  journal = {Statistics and Computing},
  keywords = {computation, constants, energy, estimation, free, importance, mcmc, normalizing, of, sampling, sequential, tempered, transitions},
  month = apr,
  number = {2},
  pages = {125--139},
  posted-at = {2013-10-24 11:57:57},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{Annealed importance sampling}},
  volume = {11},
  year = {2001}
}

@article{FilipponeML13,
  author = {Filippone, Maurizio and Zhong, Mingjun and Girolami, Mark},
  citeulike-article-id = {12682120},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10994-013-5388-x},
  journal = {Machine Learning},
  keywords = {mcmc},
  number = {1},
  pages = {93--114},
  posted-at = {2013-10-02 15:46:51},
  priority = {2},
  publisher = {Springer US},
  title = {A comparative evaluation of stochastic-based inference methods for {G}aussian process models},
  volume = {93},
  year = {2013}
}

@article{Muller13,
  author = {M\"{u}ller, Peter and Mitra, Riten},
  citeulike-article-id = {12416499},
  citeulike-linkout-0 = {http://dx.doi.org/10.1214/13-ba811},
  issn = {1936-0975},
  journal = {Bayesian Analysis},
  month = jun,
  number = {2},
  pages = {269--302},
  posted-at = {2013-06-14 14:54:47},
  priority = {2},
  title = {{Bayesian Nonparametric Inference – Why and How}},
  volume = {8},
  year = {2013}
}

@book{Thompson11,
  author = {Thompson, M. B.},
  citeulike-article-id = {12411256},
  citeulike-linkout-0 = {http://books.google.co.uk/books?id=H0s9MwEACAAJ},
  keywords = {mcmc},
  posted-at = {2013-06-12 16:03:17},
  priority = {2},
  publisher = {University of Toronto},
  title = {{Slice Sampling with Multivariate Steps}},
  year = {2011}
}

@article{Hanson11,
  abstract = {{We present a simple, efficient, and computationally cheap sampling method for exploring an un-normalized multivariate density on ℝ(d), such as a posterior density, called the Polya tree sampler. The algorithm constructs an independent proposal based on an approximation of the target density. The approximation is built from a set of (initial) support points - data that act as parameters for the approximation - and the predictive density of a finite multivariate Polya tree. In an initial "warming-up" phase, the support points are iteratively relocated to regions of higher support under the target distribution to minimize the distance between the target distribution and the Polya tree predictive distribution. In the "sampling" phase, samples from the final approximating mixture of finite Polya trees are used as candidates which are accepted with a standard Metropolis-Hastings acceptance probability. Several illustrations are presented, including comparisons of the proposed approach to Metropolis-within-Gibbs and delayed rejection adaptive Metropolis algorithm.}},
  author = {Hanson, T. E. and Monteiro, J. V. D. and Jara, A.},
  citeulike-article-id = {12411165},
  journal = {J Comput Graph Stat},
  keywords = {mcmc},
  number = {1},
  pages = {41--62},
  posted-at = {2013-06-12 14:43:16},
  priority = {2},
  title = {{The Polya Tree Sampler: Towards Efficient and Automatic Independent Metropolis-Hastings Proposals.}},
  volume = {20},
  year = {2011}
}

@article{Anitescu12,
  author = {Anitescu, Mihai and Chen, Jie and Wang, Lei},
  citeulike-article-id = {12410051},
  journal = {SIAM J. Scientific Computing},
  number = {1},
  posted-at = {2013-06-12 14:34:48},
  priority = {2},
  title = {{A Matrix-free Approach for Solving the Parametric Gaussian Process Maximum Likelihood Problem}},
  volume = {34},
  year = {2012}
}

@article{Roberts97b,
  abstract = {{This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm, in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to 1. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain, converge to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straight-forward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.}},
  author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
  citeulike-article-id = {3417187},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.40.6598},
  journal = {Annals of Applied Probability},
  keywords = {mcmc},
  pages = {110--120},
  posted-at = {2013-06-12 14:29:36},
  priority = {2},
  title = {{Weak convergence and optimal scaling of random walk Metropolis algorithms}},
  volume = {7},
  year = {1997}
}

@article{Bungartz04,
  author = {Bungartz, Hans-Joachim and Griebel, Michael},
  citeulike-article-id = {12409820},
  journal = {Acta numerica},
  number = {1},
  pages = {147--269},
  posted-at = {2013-06-12 12:09:22},
  priority = {2},
  publisher = {Cambridge Univ Press},
  title = {{Sparse grids}},
  volume = {13},
  year = {2004}
}

@article{Lunn00,
  address = {Hingham, MA, USA},
  author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
  citeulike-article-id = {12409781},
  citeulike-linkout-0 = {http://dx.doi.org/10.1023/A:1008929526011},
  journal = {Statistics and Computing},
  keywords = {acyclic, bugs, carlo, chain, directed, extension, graphs, linking, markov, mcmc, monte, object-orientation, run-time, type, winbugs},
  month = oct,
  number = {4},
  pages = {325--337},
  posted-at = {2013-06-12 11:30:23},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{WinBUGS \\– A Bayesian modelling framework: Concepts, structure, and extensibility}},
  volume = {10},
  year = {2000}
}

@proceedings{DBLP:conf/nips/2010b,
  booktitle = {NIPS},
  citeulike-article-id = {12270654},
  editor = {Lafferty, John D. and Williams, Christopher K. I. and Shawe-Taylor, John and Zemel, Richard S. and Culotta, Aron},
  keywords = {mcmc},
  posted-at = {2013-04-14 12:17:27},
  priority = {2},
  publisher = {Curran Associates},
  title = {{Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada}},
  year = {2010}
}

@proceedings{DBLP:conf/nips/2010,
  booktitle = {NIPS},
  citeulike-article-id = {12270652},
  editor = {Lafferty, John D. and Williams, Christopher K. I. and Shawe-Taylor, John and Zemel, Richard S. and Culotta, Aron},
  posted-at = {2013-04-14 12:13:56},
  priority = {2},
  publisher = {Curran Associates},
  title = {{Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada}},
  year = {2010}
}

@inproceedings{Wilson10,
  author = {Wilson, Andrew G. and Ghahramani, Zoubin},
  booktitle = {NIPS},
  citeulike-article-id = {12270651},
  editor = {Lafferty, John D. and Williams, Christopher K. I. and Shawe-Taylor, John and Zemel, Richard S. and Culotta, Aron},
  pages = {2460--2468},
  posted-at = {2013-04-14 12:13:55},
  priority = {2},
  publisher = {Curran Associates, Inc.},
  title = {{Copula Processes}},
  year = {2010}
}

@article{Murray10b,
  author = {Murray, Iain and Adams, Ryan P. and MacKay, David J. C.},
  citeulike-article-id = {12270650},
  journal = {Journal of Machine Learning Research - Proceedings Track},
  keywords = {mcmc},
  pages = {541--548},
  posted-at = {2013-04-14 12:09:10},
  priority = {2},
  title = {{Elliptical slice sampling}},
  volume = {9},
  year = {2010}
}

@inproceedings{Minka01,
  address = {San Francisco, CA, USA},
  author = {Minka, Thomas P.},
  booktitle = {Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence},
  citeulike-article-id = {12270649},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=647235.720257},
  pages = {362--369},
  posted-at = {2013-04-14 12:06:45},
  priority = {2},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series = {UAI '01},
  title = {{Expectation Propagation for approximate Bayesian inference}},
  year = {2001}
}

@techreport{Thompson10,
  abstract = {{We describe two slice sampling methods for taking multivariate steps using
the crumb framework. These methods use the gradients at rejected proposals to
adapt to the local curvature of the log-density surface, a technique that can
produce much better proposals when parameters are highly correlated. We
evaluate our methods on four distributions and compare their performance to
that of a non-adaptive slice sampling method and a Metropolis method. The
adaptive methods perform favorably on low-dimensional target distributions with
highly-correlated parameters.}},
  author = {Thompson, Madeleine and Neal, Radford M.},
  citeulike-article-id = {12261917},
  citeulike-linkout-0 = {http://arxiv.org/abs/1003.3201},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1003.3201},
  day = {16},
  eprint = {1003.3201},
  institution = {Department of Statistics, University of Toronto},
  keywords = {mcmc},
  month = mar,
  number = {1002},
  posted-at = {2013-04-12 17:47:04},
  priority = {2},
  title = {{Covariance-Adaptive Slice Sampling}},
  year = {2010}
}

@article{Hoffman12,
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  citeulike-article-id = {12261911},
  journal = {Journal of Machine Learning Research},
  keywords = {mcmc},
  posted-at = {2013-04-12 17:43:08},
  priority = {2},
  title = {The No-{U}-Turn Sampler: Adaptively Setting Path Lengths in {H}amiltonian {M}onte {C}arlo},
  year = {In press}
}

@article{Vanhatalo07,
  author = {Vanhatalo, Jarno and Vehtari, Aki},
  citeulike-article-id = {12261909},
  journal = {Journal of Machine Learning Research - Proceedings Track},
  keywords = {mcmc},
  pages = {73--89},
  posted-at = {2013-04-12 17:41:17},
  priority = {2},
  title = {{Sparse Log Gaussian Processes via MCMC for Spatial Epidemiology}},
  volume = {1},
  year = {2007}
}

@article{Christensen05,
  abstract = {{Summary.  The paper considers high dimensional Metropolis and Langevin algorithms in their initial transient phase. In stationarity, these algorithms are well understood and it is now well known how to scale their proposal distribution variances. For the random-walk Metropolis algorithm, convergence during the transient phase is extremely regular—to the extent that the algo-rithm's sample path actually resembles a deterministic trajectory. In contrast, the Langevin algorithm with variance scaled to be optimal for stationarity performs rather erratically. We give weak convergence results which explain both of these types of behaviour and practical guidance on implementation based on our theory.}},
  author = {Christensen, Ole F. and Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  citeulike-article-id = {120511},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2005.00500.x},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/bpl/rssb/2005/00000067/00000002/art00004},
  day = {1},
  issn = {1369-7412},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {mcmc},
  month = apr,
  number = {2},
  pages = {253--268},
  posted-at = {2013-04-12 17:39:01},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  title = {{Scaling limits for the transient phase of local Metropolis–Hastings algorithms}},
  volume = {67},
  year = {2005}
}

@book{Rue05,
  address = {London},
  author = {Rue, H. and Held, L.},
  citeulike-article-id = {12120758},
  posted-at = {2013-03-07 11:38:41},
  priority = {2},
  publisher = {Chapman \& Hall},
  series = {Monographs on Statistics and Applied Probability},
  title = {Gaussian {M}arkov Random Fields: {T}heory and Applications},
  volume = {104},
  year = {2005}
}

@article{Durbin00,
  abstract = {{The analysis of non-Gaussian time series using state space models is considered from both classical and Bayesian perspectives. The treatment in both cases is based on simulation using importance sampling and antithetic variables; Markov chain Monte Carlo methods are not employed. Non-Gaussian disturbances for the state equation as well as for the observation equation are considered. Methods for estimating conditional and posterior means of functions of the state vector given the observations, and the mean-square errors of their estimates, are developed. These methods are extended to cover the estimation of conditional and posterior densities and distribution functions. Choice of importance sampling densities and antithetic variables is discussed. The techniques work well in practice and are computationally efficient. Their use is illustrated by applying them to a univariate discrete time series, a series with outliers and a volatility series.}},
  author = {Durbin, J. and Koopman, S. J.},
  citeulike-article-id = {12120676},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2680676},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  number = {1},
  posted-at = {2013-03-07 10:51:02},
  priority = {2},
  publisher = {Wiley for the Royal Statistical Society},
  title = {{Time Series Analysis of Non-Gaussian Observations Based on State Space Models from Both Classical and Bayesian Perspectives}},
  volume = {62},
  year = {2000}
}

@article{Skilling12,
  author = {Skilling, John},
  citeulike-article-id = {12081659},
  citeulike-linkout-0 = {http://dx.doi.org/10.1063/1.3703630},
  citeulike-linkout-1 = {http://link.aip.org/link/?APC/1443/145/1},
  editor = {Goyal, Philip and Giffin, Adom and Knuth, Kevin H. and Vrscay, Edward},
  journal = {AIP Conference Proceedings},
  number = {1},
  pages = {145--156},
  posted-at = {2013-02-27 13:42:26},
  priority = {2},
  publisher = {AIP},
  title = {{Bayesian computation in big spaces-nested sampling and Galilean Monte Carlo}},
  volume = {1443},
  year = {2012}
}

@article{Creutz88,
  author = {Creutz, Michael},
  citeulike-article-id = {12081413},
  citeulike-linkout-0 = {http://dx.doi.org/10.1103/PhysRevD.38.1228},
  citeulike-linkout-1 = {http://link.aps.org/doi/10.1103/PhysRevD.38.1228},
  journal = {Phys. Rev. D},
  month = aug,
  pages = {1228--1238},
  posted-at = {2013-02-27 12:04:45},
  priority = {2},
  publisher = {American Physical Society},
  title = {{Global Monte Carlo algorithms for many-fermion systems}},
  volume = {38},
  year = {1988}
}

@article{Griewank96,
  address = {New York, NY, USA},
  author = {Griewank, Andreas and Juedes, David and Utke, Jean},
  citeulike-article-id = {12081343},
  citeulike-linkout-0 = {http://dx.doi.org/10.1145/229473.229474},
  citeulike-linkout-1 = {http://doi.acm.org/10.1145/229473.229474},
  journal = {ACM Trans. Math. Softw.},
  keywords = {automatic, chain, coefficients, differentiation, forward, gradients, hessians, mode, overloading, reverse, rule, taylor},
  month = jun,
  number = {2},
  pages = {131--167},
  posted-at = {2013-02-27 11:41:27},
  priority = {2},
  publisher = {ACM},
  title = {{Algorithm 755: ADOL-C: a package for the automatic differentiation of algorithms written in C/C++}},
  volume = {22},
  year = {1996}
}

@article{Hestenes52,
  author = {Hestenes, Magnus R. and Stiefel, Eduard},
  citeulike-article-id = {12077623},
  journal = {Journal of Research of the National Bureau of Standards},
  keywords = {bibtex-import},
  month = dec,
  pages = {409--436},
  posted-at = {2013-02-26 14:44:13},
  priority = {2},
  title = {{Methods of Conjugate Gradients for Solving Linear Systems}},
  volume = {49},
  year = {1952}
}

@article{Hoffman11,
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm
that avoids the random walk behavior and sensitivity to correlated parameters
that plague many MCMC methods by taking a series of steps informed by
first-order gradient information. These features allow it to converge to
high-dimensional target distributions much more quickly than simpler methods
such as random walk Metropolis or Gibbs sampling. However, HMC's performance is
highly sensitive to two user-specified parameters: a step size {\epsilon} and a
desired number of steps L. In particular, if L is too small then the algorithm
exhibits undesirable random walk behavior, while if L is too large the
algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an
extension to HMC that eliminates the need to set a number of steps L. NUTS uses
a recursive algorithm to build a set of likely candidate points that spans a
wide swath of the target distribution, stopping automatically when it starts to
double back and retrace its steps. Empirically, NUTS perform at least as
efficiently as and sometimes more efficiently than a well tuned standard HMC
method, without requiring user intervention or costly tuning runs. We also
derive a method for adapting the step size parameter {\epsilon} on the fly
based on primal-dual averaging. NUTS can thus be used with no hand-tuning at
all. NUTS is also suitable for applications such as BUGS-style automatic
inference engines that require efficient "turnkey" sampling algorithms.},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  citeulike-article-id = {10064245},
  citeulike-linkout-0 = {http://arxiv.org/abs/1111.4246},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1111.4246},
  day = {18},
  eprint = {1111.4246},
  journal = {Journal of Machine Learning Research},
  month = nov,
  posted-at = {2013-02-26 13:47:31},
  priority = {2},
  title = {{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}},
  volume = {to appear},
  year = {2012}
}

@article{Hofer98,
  abstract = {{Summary The goal of animal breeding is to improve performance traits of animal populations through selection. The selection criterion is usually based on best linear unbiased prediction (BLUP) of additive genetic effects. BLUP requires knowledge about variance components that have to be estimated in practice. Due to its desirable properties restricted maximum likelihood (REML) has become the method of choice for the estimation of variance components of mixed linear models in animal breeding. Impressive progress has been made in the development of efficient computing algorithms that allow REML to be used for general mixed linear models of large data sets. Various flexible computer programs have become available. Due to the adoption of Markov chain Monte Carlo procedures Bayesian estimation has recently become feasible and is increasingly used in animal breeding applications. The availability of powerful computers and advances in the efficiency of computing algorithms allow the consideration of increasingly complex models. Therefore, the development and application of appropriate statistical procedures for model comparison is becoming more important. Zusammenfassung Varianzkomponentensch\"{a}tzung in der Tierzucht-eine UebersichtDas Ziel der Tierzucht ist die Verbesserung der Leistung von Tierapopulationen durch Selektion. Als Selektionskriterium wird \"{u}blicherweise die beste lineare Vorhersge (BLUP) des additiv genetischen Effektes verwendet. BLUP setzt bekannte Varianzkomponenten voraus, die in der praktischen Anwendung jedoch gesch\"{a}tzt werden m\"{u}ssen. Die attraktiven Eigenschaften haben Restricted Maximum Likelihood (REML) zur Methode der Wahl f\"{u}r die Sch\"{a}tzung von Varianzkomponenten gemischter linearer Modelle in der Tierzucht werden lassen. Beachtliche Fortschritte wurden bei der Entwicklung von effizienten Rechenalgorithmen erzielt, die es erlauben REML f\"{u}r arbitr\"{a}re gemischte Modelle und grosse Datenmengen zu verwenden. Mehrere flexible Computerprogramme wurden entwickelt und verft\"{u}gbar gemacht. Die Verwendung von Markov chain Monte Carlo Prozeduren hat die praktische Anwendung der Bayes'schen Sch\"{a}tzung in der Tierzucht erm\"{o}glicht. Die stetige Steigerung der Rechenleistung von Computern und die Fortschritte bei der Entwicklung effizienter Rechenalgorithmen erlauben die Anwendung immer komplexerer Modelle. Daher gewinnt die Entwicklung und Anwendung von statistischen Verfahren f\"{u}r den Vergleich alternativer Modelle an Bedeutung.}},
  author = {Hofer, A.},
  citeulike-article-id = {12077361},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1439-0388.1998.tb00347.x},
  day = {12},
  journal = {Journal of Animal Breeding and Genetics},
  month = jan,
  number = {1-6},
  pages = {247--265},
  posted-at = {2013-02-26 13:42:08},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  title = {{Variance component estimation in animal breeding: a review{\dag}}},
  volume = {115},
  year = {1998}
}

@article{Smith01,
  author = {Smith, S. P.},
  citeulike-article-id = {12077353},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/10618600152628338},
  citeulike-linkout-1 = {http://amstat.tandfonline.com/doi/abs/10.1198/10618600152628338},
  eprint = {http://amstat.tandfonline.com/doi/pdf/10.1198/10618600152628338},
  journal = {Journal of Computational and Graphical Statistics},
  number = {2},
  pages = {350--369},
  posted-at = {2013-02-26 13:38:46},
  priority = {2},
  title = {{Likelihood-Based Analysis of Linear State-Space Models Using the Cholesky Decomposition}},
  volume = {10},
  year = {2001}
}

@article{Fournier12,
  author = {Fournier, David A. and Skaug, Hans J. and Ancheta, Johnoel and Ianelli, James and Magnusson, Arni and Maunder, Mark N. and Nielsen, Anders and Sibert, John},
  citeulike-article-id = {12077183},
  citeulike-linkout-0 = {http://dx.doi.org/10.1080/10556788.2011.597854},
  citeulike-linkout-1 = {http://www.tandfonline.com/doi/abs/10.1080/10556788.2011.597854},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1080/10556788.2011.597854},
  journal = {Optimization Methods and Software},
  number = {2},
  pages = {233--249},
  posted-at = {2013-02-26 12:46:05},
  priority = {2},
  title = {{AD Model Builder: using automatic differentiation for statistical inference of highly parameterized complex nonlinear models}},
  volume = {27},
  year = {2012}
}

@article{Skaug02,
  abstract = {{Maximum likelihood estimation in random effects models for non-Gaussian data is a computationally challenging task that currently receives much attention. This article shows that the estimation process can be facilitated by the use of automatic differentiation, which is a technique for exact numerical differentiation of functions represented as computer programs. Automatic differentiation is applied to an approximation of the likelihood function, obtained by using either Laplace's method of integration or importance sampling. The approach is applied to generalized linear mixed models. The computational speed is high compared to the Monte Carlo EM algorithm and the Monte Carlo Newton-Raphson method.}},
  author = {Skaug, Hans J.},
  citeulike-article-id = {12077177},
  citeulike-linkout-0 = {http://www.jstor.org/stable/1391062},
  journal = {Journal of Computational and Graphical Statistics},
  number = {2},
  posted-at = {2013-02-26 12:43:45},
  priority = {2},
  publisher = {American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of America},
  title = {{Automatic Differentiation to Facilitate Maximum Likelihood Estimation in Nonlinear Random Effects Models}},
  volume = {11},
  year = {2002}
}

@article{Smith12,
  author = {Smith, John R. and Nikolic, Milan and Smith, Stephen P.},
  citeulike-article-id = {12077157},
  eprint = {1201.1699},
  posted-at = {2013-02-26 12:41:03},
  priority = {2},
  title = {{Hunting the Higgs Boson using the Cholesky Decomposition of an Indefinite Matrix}},
  year = {2012}
}

@article{Bengio00,
  address = {Los Alamitos, CA, USA},
  author = {Bengio, Yoshua},
  citeulike-article-id = {12077137},
  citeulike-linkout-0 = {http://dx.doi.org/http://doi.ieeecomputersociety.org/10.1109/IJCNN.2000.857853},
  journal = {IEEE - INNS - ENNS International Joint Conference on Neural Networks},
  pages = {1305},
  posted-at = {2013-02-26 12:39:03},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Continuous Optimization of Hyper-Parameters}},
  volume = {1},
  year = {2000}
}

@article{Toal09,
  abstract = {{The process of likelihood maximization can be found in many different areas of computational modelling. However, the construction of such models via likelihood maximization requires the solution of a difficult multi-modal optimization problem involving an expensive O(n3) factorization. The optimization techniques used to solve this problem may require many such factorizations and can result in a significant bottleneck. This article derives an adjoint formulation of the likelihood employed in the construction of a kriging model via reverse algorithmic differentiation. This adjoint is found to calculate the likelihood and all of its derivatives more efficiently than the standard analytical method and can therefore be used within a simple local search or within a hybrid global optimization to accelerate convergence and therefore reduce the cost of the likelihood optimization.}},
  author = {Toal, David J. J. and Forrester, Alexander I. J. and Bressloff, Neil W. and Keane, Andy J. and Holden, Carren},
  citeulike-article-id = {5871585},
  citeulike-linkout-0 = {http://dx.doi.org/10.1098/rspa.2009.0096},
  citeulike-linkout-1 = {http://rspa.royalsocietypublishing.org/content/465/2111/3267.abstract},
  citeulike-linkout-2 = {http://rspa.royalsocietypublishing.org/content/465/2111/3267.full.pdf},
  citeulike-linkout-3 = {http://rspa.royalsocietypublishing.org/cgi/content/abstract/465/2111/3267},
  day = {8},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science},
  month = nov,
  number = {2111},
  pages = {3267--3287},
  posted-at = {2013-02-26 12:35:39},
  priority = {2},
  title = {{An adjoint for likelihood maximization}},
  volume = {465},
  year = {2009}
}

@proceedings{DBLP:conf/icml/2009,
  booktitle = {ICML},
  citeulike-article-id = {12077121},
  editor = {Danyluk, Andrea P. and Bottou, L{\'{e}}on and Littman, Michael L.},
  posted-at = {2013-02-26 12:32:03},
  priority = {2},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  title = {{Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009}},
  volume = {382},
  year = {2009}
}

@inproceedings{Schmidt09,
  author = {Schmidt, Mikkel N.},
  booktitle = {ICML},
  citeulike-article-id = {12077120},
  editor = {Danyluk, Andrea P. and Bottou, L\'{e}on and Littman, Michael L.},
  pages = {116},
  posted-at = {2013-02-26 12:32:03},
  priority = {2},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  title = {{Function factorization using warped Gaussian processes}},
  volume = {382},
  year = {2009}
}

@proceedings{DBLP:conf/mm/2012,
  booktitle = {ACM Multimedia},
  citeulike-article-id = {12077113},
  editor = {Babaguchi, Noboru and Aizawa, Kiyoharu and Smith, John R. and Satoh, Shin'ichi and Plagemann, Thomas and Hua, Xian-Sheng and Yan, Rong},
  posted-at = {2013-02-26 12:29:53},
  priority = {2},
  publisher = {ACM},
  title = {{Proceedings of the 20th ACM Multimedia Conference, MM '12, Nara, Japan, October 29 - November 02, 2012}},
  year = {2012}
}

@inproceedings{Kim12,
  author = {Kim, Samuel and Filippone, Maurizio and Valente, Fabio and Vinciarelli, Alessandro},
  booktitle = {ACM Multimedia},
  citeulike-article-id = {12077112},
  editor = {Babaguchi, Noboru and Aizawa, Kiyoharu and Smith, John R. and Satoh, Shin'ichi and Plagemann, Thomas and Hua, Xian-Sheng and Yan, Rong},
  pages = {793--796},
  posted-at = {2013-02-26 12:29:53},
  priority = {2},
  publisher = {ACM},
  title = {{Predicting the conflict level in television political debates: an approach based on crowdsourcing, nonverbal communication and gaussian processes}},
  year = {2012}
}

@inproceedings{DBLP:conf/mm/MohammadiOFV12,
  author = {Mohammadi, Gelareh and Origlia, Antonio and Filippone, Maurizio and Vinciarelli, Alessandro},
  booktitle = {ACM Multimedia},
  citeulike-article-id = {12077109},
  editor = {Babaguchi, Noboru and Aizawa, Kiyoharu and Smith, John R. and Satoh, Shin'ichi and Plagemann, Thomas and Hua, Xian-Sheng and Yan, Rong},
  pages = {789--792},
  posted-at = {2013-02-26 12:28:10},
  priority = {2},
  publisher = {ACM},
  title = {{From speech to personality: mapping voice quality and intonation into personality differences}},
  year = {2012}
}

@article{FilipponeAOAS12,
  author = {Filippone, Maurizio and Marquand, Andre F. and Blain, Camilla R. V. and Williams, Steve C. R. and {Mour\~{a}o-Miranda}, Janaina and Girolami, Mark},
  citeulike-article-id = {12077094},
  citeulike-linkout-0 = {http://dx.doi.org/10.1214/12-AOAS562},
  journal = {Annals of Applied Statistics},
  keywords = {mcmc},
  number = {4},
  pages = {1883--1905},
  posted-at = {2013-02-26 12:24:47},
  priority = {2},
  title = {{Probabilistic Prediction of Neurological Disorders with a Statistical Assessment of Neuroimaging Data Modalities}},
  volume = {6},
  year = {2012}
}

@article{DeHoog11,
  author = {de Hoog, Frank R. and Anderssen, Robert S. and Lukas, Mark A.},
  citeulike-article-id = {12077071},
  journal = {Mathematics of Computation},
  keywords = {algebra},
  number = {275},
  pages = {1585--1600},
  posted-at = {2013-02-26 12:22:42},
  priority = {2},
  title = {{Differentiation of matrix functionals using triangular factorization}},
  volume = {80},
  year = {2011}
}

@article{Tierney94,
  abstract = {{Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.}},
  author = {Tierney, Luke},
  citeulike-article-id = {432149},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2242477},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2242477},
  issn = {00905364},
  journal = {The Annals of Statistics},
  keywords = {mcmc},
  number = {4},
  pages = {1701--1728},
  posted-at = {2013-02-08 09:18:20},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Markov Chains for Exploring Posterior Distributions}},
  volume = {22},
  year = {1994}
}

@article{Gibbs00,
  author = {Gibbs, Mark N. and MacKay, David J. C.},
  citeulike-article-id = {10902758},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {lgms},
  number = {6},
  pages = {1458--1464},
  posted-at = {2012-07-19 10:55:07},
  priority = {2},
  title = {{Variational Gaussian process classifiers}},
  volume = {11},
  year = {2000}
}

@book{Lawson01,
  author = {Lawson, A. B. and Williams, F. L. R. and Williams, F.},
  citeulike-article-id = {10898702},
  citeulike-linkout-0 = {http://books.google.co.uk/books?id=w\_NFAAAAYAAJ},
  keywords = {lgms},
  posted-at = {2012-07-18 10:20:53},
  priority = {2},
  publisher = {John Wiley},
  title = {{An introductory guide to disease mapping}},
  year = {2001}
}

@misc{Taylor12,
  author = {Taylor, M. Benjamin and Diggle, J. Peter},
  citeulike-article-id = {10853439},
  keywords = {lgms, mcmc},
  note = {arXiv:1202.1738},
  posted-at = {2012-07-03 14:27:10},
  priority = {2},
  title = {{INLA or MCMC? A Tutorial and Comparative Evaluation for Spatial Prediction in log-Gaussian Cox Processes}},
  year = {2012}
}

@article{Focke11,
  author = {Focke, Niels K. and Helms, Gunther and Scheewe, Sebstian and Pantel, Pia M. and Bachmann, Cornelius G. and Dechent, Peter and Ebentheuer, Jens and Mohr, Alexander and Paulus, Walter and Trenkwalder, Claudia},
  citeulike-article-id = {10853417},
  journal = {Human Brain Mapping},
  keywords = {parkinson},
  number = {11},
  pages = {1905--1915},
  posted-at = {2012-07-03 14:18:40},
  priority = {2},
  title = {Individual voxel-based subtype prediction can differentiate progressive supranuclear palsy from idiopathic {P}arkinson syndrome and healthy controls.},
  volume = {32},
  year = {2011}
}

@article{Moller98,
  abstract = {{Planar Cox processes directed by a log Gaussian intensity process are investigated in the univariate and multivariate cases. The appealing properties of such models are demonstrated theoretically as well as through data examples and simulations. In particular, the first, second and third-order properties are studied and utilized in the statistical analysis of clustered point patterns. Also empirical Bayesian inference for the underlying intensity surface is considered.}},
  author = {M{\o}ller, Jesper and Syversveen, Anne R. and Waagepetersen, Rasmus P.},
  citeulike-article-id = {10853159},
  citeulike-linkout-0 = {http://www.jstor.org/stable/4616515},
  journal = {Scandinavian Journal of Statistics},
  keywords = {lgms},
  number = {3},
  pages = {451--482},
  posted-at = {2012-07-03 11:22:05},
  priority = {2},
  publisher = {Blackwell Publishing on behalf of Board of the Foundation of the Scandinavian Journal of Statistics},
  title = {{Log Gaussian Cox Processes}},
  volume = {25},
  year = {1998}
}

@article{Flegal08,
  abstract = {{Current reporting of results based on Markov chain Monte Carlo computations
could be improved. In particular, a measure of the accuracy of the resulting
estimates is rarely reported in the literature. Thus the reader has little
ability to objectively assess the quality of the reported estimates. This paper
is an attempt to address this issue in that we discuss why Monte Carlo standard
errors are important, how they can be easily calculated in Markov chain Monte
Carlo and how they can be used to decide when to stop the simulation. We
compare their use to a popular alternative in the context of two examples.}},
  author = {Flegal, James M. and Haran, Murali and Jones, Galin L.},
  citeulike-article-id = {1189831},
  citeulike-linkout-0 = {http://arxiv.org/abs/math.ST/0703746},
  citeulike-linkout-1 = {http://arxiv.org/pdf/math.ST/0703746},
  day = {26},
  eprint = {math.ST/0703746},
  journal = {Statistical Science},
  keywords = {mcmc},
  month = mar,
  number = {2},
  pages = {250--260},
  posted-at = {2012-07-03 10:09:47},
  priority = {2},
  title = {{Markov Chain Monte Carlo: Can We Trust the Third Significant Figure?}},
  volume = {23},
  year = {2007}
}

@article{Jones06,
  author = {Jones and Galin, L. and Haran and Murali and Caffo and Brian, S. and Neath and Ronald},
  citeulike-article-id = {965796},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/016214506000000492},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/jasa/2006/00000101/00000476/art00021},
  issn = {0162-1459},
  journal = {Journal of the American Statistical Association},
  keywords = {mcmc},
  month = dec,
  number = {476},
  pages = {1537--1547},
  posted-at = {2012-07-03 10:07:53},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Fixed-Width Output Analysis for Markov Chain Monte Carlo}},
  volume = {101},
  year = {2006}
}

@article{Lindgren11,
  author = {Lindgren, Finn and Rue, H\r{a}vard and Lindstr\"{o}m, Johan},
  citeulike-article-id = {10852503},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2011.00777.x},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {approximate, bayesian, covariance, differential, equations, fields, functions, gaussian, inference, latent, lgms, markov, matrices, models, partial, random, sparse, stochastic},
  number = {4},
  posted-at = {2012-07-03 10:04:37},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  title = {{An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach}},
  volume = {73},
  year = {2011}
}

@article{Nickisch08,
  author = {Nickisch, Hannes and Rasmussen, Carl E.},
  citeulike-article-id = {10800827},
  journal = {Journal of Machine Learning Research},
  keywords = {classification, gaussianprocesses},
  month = oct,
  pages = {2035--2078},
  posted-at = {2012-06-18 13:23:03},
  priority = {2},
  title = {{Approximations for Binary Gaussian Process Classification}},
  volume = {9},
  year = {2008}
}

@proceedings{DBLP:conf/ecml/1998,
  booktitle = {ECML},
  citeulike-article-id = {10800746},
  editor = {Nedellec, Claire and Rouveirol, C{\'{e}}line},
  keywords = {classification},
  posted-at = {2012-06-18 11:35:18},
  priority = {2},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {{Machine Learning: ECML-98, 10th European Conference on Machine Learning, Chemnitz, Germany, April 21-23, 1998, Proceedings}},
  volume = {1398},
  year = {1998}
}

@inproceedings{Joachims98,
  author = {Joachims, Thorsten},
  booktitle = {ECML},
  citeulike-article-id = {10800745},
  editor = {Nedellec, Claire and Rouveirol, C{\'{e}}line},
  keywords = {classification},
  pages = {137--142},
  posted-at = {2012-06-18 11:35:18},
  priority = {2},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {{Text Categorization with Suport Vector Machines: Learning with Many Relevant Features}},
  volume = {1398},
  year = {1998}
}

@article{Bosch08,
  author = {Bosch, A. and Zisserman, A. and Muoz, X.},
  citeulike-article-id = {10800743},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TPAMI.2007.70716},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {analysisimage, analysisrelevance, and, automatedphotographyreproducibility, classification, classificationsupport, classificationtopic, classifierprobabilistic, computer-assistedpattern, distribution, enhancementimage, feedbackstatistical, generative, interpretation, latent, literaturesupervised, machinesalgorithmsdiscriminant, modelmultiway, of, recognition, recording, representationvisual, resultssensitivity, scene, semantic, specificitysubtraction, techniquevideo, text, vector, vectorimage, vectorvisual, word, words},
  number = {4},
  pages = {712--727},
  posted-at = {2012-06-18 11:30:15},
  priority = {2},
  title = {{Scene Classification Using a Hybrid Generative/Discriminative Approach}},
  volume = {30},
  year = {2008}
}

@article{OWilliams05,
  author = {Williams, O. and Blake, A. and Cipolla, R.},
  citeulike-article-id = {10800742},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TPAMI.2005.167},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {algorithmssupport, analysiscomputer, and, artificial, automatedsubtraction, bayesian, classification, computer-assistedimaging, distributionbelief, distributionsdisplacement, enhancementimage, estimationobject, fusionvisual, gaussian, intelligencebayes, intelligenceobject, interpretation, learning, learningspatial, localizationobject, machinesalgorithmsartificial, machinesparse, machinestemporal, networkslearning, perturbationstatistical, recognition, recognitionobject, recognitionsupport, retrievalmodels, simulationcomputer, statisticalmovementpattern, storage, systemsimage, technique, theoremcluster, three-dimensionalinformation, trackinggaussian, vector, verificationrelevance},
  month = aug,
  number = {8},
  pages = {1292--1304},
  posted-at = {2012-06-18 11:29:30},
  priority = {2},
  title = {{Sparse Bayesian learning for efficient visual tracking}},
  volume = {27},
  year = {2005}
}

@article{Jaakkola00b,
  author = {Jaakkola, Tommi and Diekhans, Mark and Haussler, David},
  citeulike-article-id = {10800737},
  journal = {Journal of Computational Biology},
  keywords = {classification},
  number = {1-2},
  pages = {95--114},
  posted-at = {2012-06-18 11:20:58},
  priority = {2},
  title = {{A Discriminative Framework for Detecting Remote Protein Homologies}},
  volume = {7},
  year = {2000}
}

@article{Ratsch06,
  author = {R{\"{a}}tsch, Gunnar and Sonnenburg, S{\"{o}}ren and Sch{\"{a}}fer, Christin},
  citeulike-article-id = {10800734},
  journal = {BMC Bioinformatics},
  keywords = {classification},
  number = {S-1},
  posted-at = {2012-06-18 11:18:41},
  priority = {2},
  title = {{Learning Interpretable SVMs for Biological Sequence Classification}},
  volume = {7},
  year = {2006}
}

@proceedings{DBLP:conf/rocai/2004,
  booktitle = {ROCAI},
  citeulike-article-id = {10780169},
  editor = {Hern{\'{a}}ndez-Orallo, Jos{\'{e}} and Ferri, C{\'{e}}sar and Lachiche, Nicolas and Flach, Peter A.},
  keywords = {classification},
  posted-at = {2012-06-11 17:39:14},
  priority = {2},
  title = {{ROC Analysis in Artificial Intelligence, 1st International Workshop, ROCAI-2004, Valencia, Spain, August 22, 2004}},
  year = {2004}
}

@inproceedings{Ferri04,
  author = {Ferri, C{\'{e}}sar and Hern{\'{a}}ndez-Orallo, Jos{\'{e}}},
  booktitle = {ROCAI},
  citeulike-article-id = {10780168},
  editor = {Hern{\'{a}}ndez-Orallo, Jos{\'{e}} and Ferri, C{\'{e}}sar and Lachiche, Nicolas and Flach, Peter A.},
  keywords = {classification},
  pages = {27--36},
  posted-at = {2012-06-11 17:39:14},
  priority = {2},
  title = {{Cautious Classifiers}},
  year = {2004}
}

@article{Kim06,
  address = {Washington, DC, USA},
  author = {Kim, Hyun-Chul and Ghahramani, Zoubin},
  citeulike-article-id = {10780153},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TPAMI.2006.238},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  keywords = {algorithm, bayesian, classification, em-ep, expectation, gaussian, kernel, methods, process, propagation},
  month = dec,
  number = {12},
  pages = {1948--1959},
  posted-at = {2012-06-11 17:35:07},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Bayesian Gaussian Process Classification with the EM-EP Algorithm}},
  volume = {28},
  year = {2006}
}

@article{Andrieu09,
  author = {Andrieu, Christophe and Roberts, Gareth O.},
  citeulike-article-id = {7606708},
  citeulike-linkout-0 = {http://dx.doi.org/10.1214/07-aos574},
  issn = {0090-5364},
  journal = {The Annals of Statistics},
  keywords = {mcmc},
  month = apr,
  number = {2},
  pages = {697--725},
  posted-at = {2012-06-11 17:29:09},
  priority = {2},
  title = {{The pseudo-marginal approach for efficient Monte Carlo computations}},
  volume = {37},
  year = {2009}
}

@article{Beaumont03,
  abstract = {{This article introduces a new general method for genealogical inference that samples independent genealogical histories using importance sampling (IS) and then samples other parameters with Markov chain Monte Carlo (MCMC). It is then possible to more easily utilize the advantages of importance sampling in a fully Bayesian framework. The method is applied to the problem of estimating recent changes in effective population size from temporally spaced gene frequency data. The method gives the posterior distribution of effective population size at the time of the oldest sample and at the time of the most recent sample, assuming a model of exponential growth or decline during the interval. The effect of changes in number of alleles, number of loci, and sample size on the accuracy of the method is described using test simulations, and it is concluded that these have an approximately equivalent effect. The method is used on three example data sets and problems in interpreting the posterior densities are highlighted and discussed.}},
  author = {Beaumont, Mark A.},
  citeulike-article-id = {7607289},
  citeulike-linkout-0 = {http://www.genetics.org/content/164/3/1139.abstract},
  citeulike-linkout-1 = {http://www.genetics.org/content/164/3/1139.full.pdf},
  citeulike-linkout-2 = {http://www.genetics.org/cgi/content/abstract/164/3/1139},
  citeulike-linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/12871921},
  citeulike-linkout-4 = {http://www.hubmed.org/display.cgi?uids=12871921},
  day = {1},
  journal = {Genetics},
  keywords = {mcmc},
  month = jul,
  number = {3},
  pages = {1139--1160},
  pmid = {12871921},
  posted-at = {2012-06-11 17:27:00},
  priority = {2},
  title = {{Estimation of Population Growth or Decline in Genetically Monitored Populations}},
  volume = {164},
  year = {2003}
}

@article{Roberts09,
  abstract = {{We investigate the use of adaptive MCMC algorithms to automatically tune the Markov chain parameters during a run. Examples include the Adaptive Metropolis (AM) multivariate algorithm of Haario, Saksman, and Tamminen (2001), Metropolis-within-Gibbs algorithms for nonconjugate hierarchical models, regionally adjusted Metropolis algorithms, and logarithmic scalings. Computer simulations indicate that the algorithms perform very well compared to nonadaptive algorithms, even in high dimension.}},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  booktitle = {Stat. and Comput},
  citeulike-article-id = {5727938},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/jcgs.2009.06134},
  citeulike-linkout-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.7198},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {mcmc},
  month = jun,
  number = {2},
  pages = {349--367},
  posted-at = {2012-05-09 13:45:35},
  priority = {2},
  title = {{Examples of adaptive MCMC}},
  volume = {18},
  year = {2009}
}

@article{Gneiting07,
  author = {Gneiting, Tilmann and Raftery, Adrian E.},
  citeulike-article-id = {10448268},
  journal = {Journal of the American Statistical Association},
  keywords = {classification},
  pages = {359--378},
  posted-at = {2012-03-13 12:12:02},
  priority = {2},
  title = {{Strictly proper scoring rules, prediction, and estimation}},
  volume = {102},
  year = {2007}
}

@article{Yu11,
  author = {Yu, Yaming and Meng, Xiao-Li},
  citeulike-article-id = {10408757},
  citeulike-linkout-0 = {http://amstat.tandfonline.com/doi/abs/10.1198/jcgs.2011.203main},
  citeulike-linkout-1 = {http://pubs.amstat.org/doi/abs/10.1198/jcgs.2011.203main},
  citeulike-linkout-2 = {http://dx.doi.org/10.1198/jcgs.2011.203main},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {mcmc, *file-import-13-04-14},
  number = {3},
  pages = {531--570},
  posted-at = {2012-03-03 18:10:07},
  priority = {2},
  title = {{To Center or Not to Center: That Is Not the Question--An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency}},
  volume = {20},
  year = {2011}
}

@article{Chu05,
  author = {Chu, Wei and Ghahramani, Zoubin},
  citeulike-article-id = {10109649},
  citeulike-linkout-0 = {http://dl.acm.org/citation.cfm?id=1046920.1088707},
  journal = {Journal of Machine Learning Research},
  keywords = {ordinal},
  month = dec,
  pages = {1019--1041},
  posted-at = {2011-12-09 11:23:32},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Gaussian Processes for Ordinal Regression}},
  volume = {6},
  year = {2005}
}

@article{Varin11,
  author = {Varin, Cristiano and Reid, Nancy and Firth, David},
  citeulike-article-id = {10109633},
  institution = {Ca' Foscari University of Venice, University of Toronto and University of Warwick},
  journal = {Statistica Sinica},
  keywords = {likelihood, likelihoodcomposite},
  pages = {5--42},
  posted-at = {2011-12-09 11:17:21},
  priority = {2},
  title = {{An Overview of Composite Likelihood Methods}},
  volume = {21},
  year = {2011}
}

@article{StathopoulosDiscussionRSSB11,
  author = {Stathopoulos, V. and Filippone, M.},
  citeulike-article-id = {10109618},
  journal = {Journal of the Royal Statistical Society, Series B (Statistical Methodology)},
  keywords = {mcmc},
  number = {2},
  pages = {123--214},
  posted-at = {2011-12-09 11:14:24},
  priority = {2},
  title = {Discussion of the paper ''{R}iemann manifold {L}angevin and {H}amiltonian {M}onte {C}arlo methods'' by {M}ark {G}irolami and {B}en {C}alderhead},
  volume = {73},
  year = {2011}
}

@article{FilipponeDiscussionRSSB11,
  author = {Filippone, M.},
  citeulike-article-id = {10109598},
  journal = {Journal of the Royal Statistical Society, Series B (Statistical Methodology)},
  keywords = {mcmc},
  number = {2},
  pages = {123--214},
  posted-at = {2011-12-09 11:07:24},
  priority = {2},
  title = {Discussion of the paper ''{R}iemann manifold {L}angevin and {H}amiltonian {M}onte {C}arlo methods'' by {M}ark {G}irolami and {B}en {C}alderhead},
  volume = {73},
  year = {2011}
}

@article{vanDyk01,
  author = {van Dyk, D. and Meng, X. L.},
  citeulike-article-id = {10109534},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {jabrefnokeywordassigned, likelihood},
  pages = {1--111},
  posted-at = {2011-12-09 10:51:56},
  priority = {2},
  title = {{The art of data augmentation (with discussion)}},
  volume = {10},
  year = {2001}
}

@article{Dempster77,
  abstract = {{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  citeulike-article-id = {117535},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2984875},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2984875},
  issn = {00359246},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  keywords = {likelihood},
  number = {1},
  pages = {1--38},
  posted-at = {2011-12-09 10:49:14},
  priority = {2},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  title = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
  volume = {39},
  year = {1977}
}

@misc{Neal10,
  author = {Neal, R. M.},
  citeulike-article-id = {9055009},
  howpublished = {in Handbook of Markov Chain Monte Carlo (eds S. Brooks, A. Gelman, G. Jones, XL Meng). Chapman and Hall/CRC Press},
  keywords = {mcmc},
  posted-at = {2011-12-09 10:43:30},
  priority = {2},
  title = {{MCMC using Hamiltonian dynamics}},
  year = {2010}
}

@book{Neal96,
  abstract = {{Artificial "neural networks" are widely used as flexible models for
classification and regression applications, but questions remain about how the
power of these models can be safely exploited when training data is limited.
This book demonstrates how Bayesian methods allow complex neural network
models to be used without fear of the "overfitting" that can occur with
traditional training methods. Insight into the nature of these complex
Bayesian models is provided by a theoretical investigation of the priors over
functions that underlie them. A practical implementation of Bayesian neural
network learning using Markov chain Monte Carlo methods is also described, and
software for it is freely available over the Internet. Presupposing only basic
knowledge of probability and statistics, this book should be of interest to
researchers in statistics, engineering, and artificial intelligence.}},
  author = {Neal, Radford M.},
  citeulike-article-id = {3212203},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387947248},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0387947248},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0387947248},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0387947248},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0387947248/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387947248},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0387947248},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0387947248},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0387947248\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0387947248},
  day = {09},
  edition = {1},
  howpublished = {Paperback},
  isbn = {0387947248},
  keywords = {neural},
  month = aug,
  posted-at = {2011-12-09 10:35:03},
  priority = {2},
  publisher = {Springer},
  title = {{Bayesian Learning for Neural Networks (Lecture Notes in Statistics)}},
  year = {1996}
}

@incollection{Mackay94,
  author = {Mac{k}ay, D. J. C.},
  booktitle = {Models of Neural Networks {III}},
  chapter = {6},
  citeulike-article-id = {819433},
  editor = {Domany, E. and van Hemmen, J. L. and Schulten, K.},
  keywords = {neural},
  pages = {211--254},
  posted-at = {2011-12-09 10:33:55},
  priority = {2},
  publisher = {Springer},
  title = {{B}ayesian methods for backpropagation networks},
  year = {1994}
}

@article{Duane87,
  author = {Duane, Simon and Kennedy, A. D. and Pendleton, Brian J. and Roweth, Duncan},
  citeulike-article-id = {10057689},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/pii/037026938791197X},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/0370-2693(87)91197-X},
  journal = {Physics Letters B},
  keywords = {mcmc},
  number = {2},
  pages = {216--222},
  posted-at = {2011-11-22 17:43:24},
  priority = {2},
  title = {{Hybrid Monte Carlo}},
  volume = {195},
  year = {1987}
}

@article{Kass89,
  author = {Kass, Robert E.},
  citeulike-article-id = {540270},
  journal = {Statistical Science},
  keywords = {statistics},
  month = aug,
  number = {3},
  pages = {188--219},
  posted-at = {2011-11-09 14:15:29},
  priority = {2},
  title = {{The Geometry of Asymptotic Inference}},
  volume = {4},
  year = {1989}
}

@article{Nelder72,
  abstract = {{The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  citeulike-article-id = {10007313},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2344614},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  keywords = {statistics},
  number = {3},
  posted-at = {2011-11-09 13:31:56},
  priority = {2},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  title = {{Generalized Linear Models}},
  volume = {135},
  year = {1972}
}

@article{FilipponeIEEETSP11,
  author = {Filippone, Maurizio and Sanguinetti, Guido},
  citeulike-article-id = {10007295},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TSP.2010.2094609},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {novelty},
  number = {3},
  pages = {1027--1036},
  posted-at = {2011-11-09 13:24:53},
  priority = {2},
  title = {{A Perturbative Approach to Novelty Detection in Autoregressive Models}},
  volume = {59},
  year = {2011}
}

@article{FilipponeCSDA11,
  author = {Filippone, Maurizio and Sanguinetti, Guido},
  citeulike-article-id = {10007294},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.csda.2011.05.023},
  journal = {Computational Statistics {\&} Data Analysis},
  keywords = {bayesian},
  number = {12},
  pages = {3104--3122},
  posted-at = {2011-11-09 13:24:30},
  priority = {2},
  title = {{Approximate inference of the bandwidth in multivariate kernel density estimation}},
  volume = {55},
  year = {2011}
}

@article{Cseke11,
  author = {Cseke, Botond and Heskes, Tom},
  citeulike-article-id = {9948670},
  journal = {Journal of Machine Learning Research},
  keywords = {bayesian},
  pages = {417--454},
  posted-at = {2011-10-26 18:26:05},
  priority = {2},
  title = {{Approximate Marginals in Latent Gaussian Models}},
  volume = {12},
  year = {2011}
}

@article{Neal03,
  author = {Neal, Radford M.},
  citeulike-article-id = {9947451},
  journal = {Annals of Statistics},
  keywords = {mcmc},
  pages = {705--767},
  posted-at = {2011-10-26 11:44:25},
  priority = {2},
  title = {{Slice Sampling}},
  volume = {31},
  year = {2003}
}

@article{Geyer92,
  abstract = {{Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo.}},
  author = {Geyer, Charles J.},
  citeulike-article-id = {4879195},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2246094},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2246094},
  issn = {08834237},
  journal = {Statistical Science},
  keywords = {statistics},
  number = {4},
  pages = {473--483},
  posted-at = {2011-08-16 15:17:26},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Practical Markov chain Monte Carlo}},
  volume = {7},
  year = {1992}
}

@article{Tierney86,
  abstract = {{This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal.}},
  author = {Tierney, Luke and Kadane, Joseph B.},
  citeulike-article-id = {9668876},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2287970},
  journal = {Journal of the American Statistical Association},
  keywords = {statistics},
  number = {393},
  pages = {82--86},
  posted-at = {2011-08-15 12:09:10},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Accurate Approximations for Posterior Moments and Marginal Densities}},
  volume = {81},
  year = {1986}
}

@article{Hastie86,
  author = {Hastie, Trevor and Tibshirani, Robert},
  citeulike-article-id = {9668857},
  journal = {Statistical Science},
  keywords = {generalized, linear, model, nonlinear, nonparametric, partial, regression, residual, smoothing, statistics},
  pages = {297--310},
  posted-at = {2011-08-15 12:05:42},
  priority = {2},
  title = {{Generalized additive models}},
  volume = {1},
  year = {1986}
}

@article{Goel81,
  author = {Goel, Prem K. and DeGroot, Morris H.},
  citeulike-article-id = {9608162},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2287059},
  journal = {Journal of the American Statistical Association},
  keywords = {information},
  month = mar,
  number = {373},
  pages = {140--147},
  posted-at = {2011-08-04 18:56:35},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Information about hyperparamters in hierarchical models}},
  volume = {76},
  year = {1981}
}

@article{Kuss05,
  author = {Kuss, Malte and Rasmussen, Carl E.},
  citeulike-article-id = {9445517},
  journal = {Journal of Machine Learning Research},
  pages = {1679--1704},
  posted-at = {2011-06-21 15:43:37},
  priority = {2},
  title = {{Assessing Approximate Inference for Binary Gaussian Process Classification}},
  volume = {6},
  year = {2005}
}

@article{FilipponeIEEETFS10,
  author = {Filippone, Maurizio and Masulli, Francesco and Rovetta, Stefano},
  citeulike-article-id = {9315336},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {clustering},
  month = jun,
  number = {3},
  pages = {572--584},
  posted-at = {2011-05-19 18:06:52},
  priority = {2},
  title = {{Applying the Possibilistic C-Means Algorithm in Kernel-Induced Spaces}},
  volume = {18},
  year = {2010}
}

@article{Hazelton07,
  author = {Hazelton, Martin L. and Turlach, Berwin A.},
  citeulike-article-id = {9315330},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-4JCCFBY-4/2/8709016f10b4de4a1fcb2fa08f3daf2a},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.csda.2006.02.002},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {biased, bootstrap, density},
  number = {6},
  pages = {3057--3069},
  posted-at = {2011-05-19 18:02:32},
  priority = {2},
  title = {{Reweighted kernel density estimation}},
  volume = {51},
  year = {2007}
}

@article{Taylor08,
  author = {Taylor, Charles C.},
  citeulike-article-id = {9315329},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-4R53W3W-1/2/c6fb5499edf19d9cedf2c0fa80985531},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.csda.2007.11.003},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {7},
  pages = {3493--3500},
  posted-at = {2011-05-19 18:01:47},
  priority = {2},
  title = {{Automatic bandwidth selection for circular density estimation}},
  volume = {52},
  year = {2008}
}

@article{Duong08,
  author = {Duong, Tarn and Cowling, Arianna and Koch, Inge and Wand, M. P.},
  citeulike-article-id = {9315313},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-4S0YXH6-1/2/1645b92059aaa26763a66df84ea7d880},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.csda.2008.02.035},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {9},
  pages = {4225--4242},
  posted-at = {2011-05-19 17:58:03},
  priority = {2},
  title = {{Feature significance for multivariate kernel density estimation}},
  volume = {52},
  year = {2008}
}

@article{Jones09,
  author = {Jones, M. C. and Henderson, D. A.},
  citeulike-article-id = {9315309},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-4W0SJVP-2/2/4402290179d7756a8ee02389522989f3},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.csda.2009.03.019},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {10},
  pages = {3726--3733},
  posted-at = {2011-05-19 17:56:15},
  priority = {2},
  title = {{Maximum likelihood kernel density estimation: On the potential of convolution sieves}},
  volume = {53},
  year = {2009}
}

@article{Chan10,
  author = {Chan, Ngai-Hang and Lee, Thomas C. M. and Peng, Liang},
  citeulike-article-id = {9315301},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-4XBG16P-4/2/cd22fa63170c58294b7843082a1dbe36},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.csda.2009.09.021},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {2},
  pages = {509--515},
  posted-at = {2011-05-19 17:54:44},
  priority = {2},
  title = {{On nonparametric local inference for density estimation}},
  volume = {54},
  year = {2010}
}

@article{Hirukawa10,
  author = {Hirukawa, Masayuki},
  citeulike-article-id = {9315296},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-4X97CSC-1/2/2376e98eb1e805af2a4f24f4b62519a6},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.csda.2009.09.017},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {2},
  pages = {473--495},
  posted-at = {2011-05-19 17:53:33},
  priority = {2},
  title = {{Nonparametric multiplicative bias correction for kernel-type density estimation on the unit interval}},
  volume = {54},
  year = {2010}
}

@article{Gyorfi92,
  author = {Gy\"{o}rfi, L\'{a}szl\'{o} and Lugosi, G\'{a}bor},
  citeulike-article-id = {9315227},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-45FSPRV-3/2/7a6209987ec74882d4cf9434d05dd26f},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/0167-9473(92)90059-O},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {4},
  pages = {437--442},
  posted-at = {2011-05-19 17:29:10},
  priority = {2},
  title = {{Kernel density estimation from ergodic sample is not universally consistent}},
  volume = {14},
  year = {1992}
}

@article{Tran06,
  author = {Tran, Thanh N. and Wehrens, Ron and Buydens, Lutgarde M. C.},
  citeulike-article-id = {9315219},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.csda.2005.10.001},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6V8V-4HD88MX-1/2/476bc2b27f48525d2988cab0ee5252f7},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {2},
  pages = {513--525},
  posted-at = {2011-05-19 17:27:08},
  priority = {2},
  title = {{KNN-kernel density-based clustering for high-dimensional multivariate data}},
  volume = {51},
  year = {2006}
}

@article{Cao94,
  author = {Cao, Ricardo and Cuevas, Antonio and Manteiga, Wensceslao G.},
  citeulike-article-id = {9315217},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V8V-45FCGS4-K/2/038366213176c6ce7493b415bb88c8e1},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/0167-9473(92)00066-Z},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {2},
  pages = {153--176},
  posted-at = {2011-05-19 17:25:38},
  priority = {2},
  title = {{A comparative study of several smoothing methods in density estimation}},
  volume = {17},
  year = {1994}
}

@article{Feluch92,
  author = {Feluch, W. and Koronacki, J.},
  citeulike-article-id = {9315212},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/0167-9473(92)90002-W},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6V8V-45FCGW7-2D/2/4e3fbec005ade29066835e2780d9b5da},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {cross-validation, density},
  number = {2},
  pages = {143--151},
  posted-at = {2011-05-19 17:23:51},
  priority = {2},
  title = {{A note on modified cross-validation in density estimation}},
  volume = {13},
  year = {1992}
}

@article{Jones91b,
  author = {Jones, M. C.},
  citeulike-article-id = {9315209},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/0167-9473(91)90049-8},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6V8V-463GR46-C/2/0dbb55f6f68462bb5d390097f2a5f1eb},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {density},
  number = {1},
  pages = {3--15},
  posted-at = {2011-05-19 17:21:30},
  priority = {2},
  title = {{On correcting for variance inflation in kernel density estimation}},
  volume = {11},
  year = {1991}
}

@article{Williams98,
  address = {Los Alamitos, CA, USA},
  author = {Williams, Christopher K. I. and Barber, David},
  citeulike-article-id = {9007615},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1342--1351},
  posted-at = {2011-03-17 10:52:53},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Bayesian classification with Gaussian processes}},
  volume = {20},
  year = {1998}
}

@article{Girolami11,
  abstract = {{Summary.  The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis–Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from http://www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.}},
  author = {Girolami, Mark and Calderhead, Ben},
  citeulike-article-id = {8952513},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2010.00765.x},
  day = {1},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  month = mar,
  number = {2},
  pages = {123--214},
  posted-at = {2011-03-14 10:58:52},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  title = {{Riemann manifold Langevin and Hamiltonian Monte Carlo methods}},
  volume = {73},
  year = {2011}
}

@book{Amari00,
  author = {Amari, S. and Nagaoka, H.},
  citeulike-article-id = {1615011},
  posted-at = {2011-03-14 10:51:49},
  priority = {2},
  publisher = {Oxford University Press},
  series = {Translations of Mathematical monographs},
  title = {{Methods of Information Geometry}},
  volume = {191},
  year = {2000}
}

@article{Rue09,
  author = {Rue, H\r{a}vard and Martino, Sara and Chopin, Nicolas},
  citeulike-article-id = {8988539},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2008.00700.x},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {additive, approximate, approximation, bayesian, computing, fields, gaussian, generalized, inference, laplace, markov, matrices, mixed, models, parallel, random, regression, sparse, structured},
  number = {2},
  pages = {319--392},
  posted-at = {2011-03-14 10:41:14},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  title = {{Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations}},
  volume = {71},
  year = {2009}
}

@article{Rue01,
  author = {Rue, Handaring;vard},
  citeulike-article-id = {8977371},
  citeulike-linkout-0 = {http://ideas.repec.org/a/bla/jorssb/v63y2001i2p325-338.html},
  journal = {Journal Of The Royal Statistical Society Series B},
  number = {2},
  pages = {325--338},
  posted-at = {2011-03-10 19:06:05},
  priority = {2},
  title = {{Fast sampling of Gaussian Markov random fields}},
  volume = {63},
  year = {2001}
}

@article{Roberts97,
  abstract = {{In this paper many convergence issues concerning the implementation of the Gibbs sampler are investigated. Exact computable rates of convergence for Gaussian target distributions are obtained. Different random and non-random updating strategies and blocking combinations are compared using the rates. The effect of dimensionality and correlation structure on the convergence rates are studied. Some examples are considered to demonstrate the results. For a Gaussian image analysis problem several updating strategies are described and compared. For problems in Bayesian linear models several possible parameterizations are analysed in terms of their convergence rates characterizing the optimal choice.}},
  author = {Roberts, G. O. and Sahu, S. K.},
  citeulike-article-id = {8977368},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2346048},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {2},
  posted-at = {2011-03-10 19:03:32},
  priority = {2},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  title = {{Updating Schemes, Correlation Structure, Blocking and Parameterization for the Gibbs Sampler}},
  volume = {59},
  year = {1997}
}

@inproceedings{Papaspiliopoulos03,
  author = {Papaspiliopolous, O. and Roberts, G. O. and Sk\^{o}ld, M.},
  booktitle = {Baysian Statistics 7},
  citeulike-article-id = {6128134},
  editor = {Bernardo, J. M. and Bayarri, M. J. and Berger, J. O. and Dawid, A. P. and Heckerman, D. and Smith, A. F. M. and West, M.},
  pages = {307--326},
  posted-at = {2011-03-10 19:00:49},
  priority = {2},
  publisher = {Oxford University Press},
  title = {{Non-Centered Parameterizations for Hierarchical Models and Data Augmentation}},
  year = {2003}
}

@article{Meng99,
  author = {Meng, X. L. and van Dyk, D. A.},
  citeulike-article-id = {3852263},
  journal = {Biometrika},
  pages = {301--320},
  posted-at = {2011-03-10 18:59:52},
  priority = {2},
  title = {{Seeking efficient data augmentation schemes via conditional and marginal augmentation}},
  volume = {86},
  year = {1999}
}

@article{Papaspiliopoulos07,
  abstract = {{In this paper, we describe centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models. We give a clear qualitative understanding as to when centering and noncentering work well, and introduce theory concerning the convergence time complexity of Gibbs samplers using centered and noncentered parametrizations. We give general recipes for the construction of noncentered parametrizations, including an auxiliary variable technique called the state-space expansion technique. We also describe partially noncentered methods, and demonstrate their use in constructing robust Gibbs sampler algorithms whose convergence properties are not overly sensitive to the data.}},
  author = {Papaspiliopoulos, Omiros and Roberts, Gareth O. and Sk\"{o}ld, Martin},
  citeulike-article-id = {8977350},
  citeulike-linkout-0 = {http://www.jstor.org/stable/27645805},
  journal = {Statistical Science},
  number = {1},
  pages = {59--73},
  posted-at = {2011-03-10 18:55:50},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{A general framework for the parametrization of hierarchical models}},
  volume = {22},
  year = {2007}
}

@article{Geweke04,
  abstract = {{Analytical or coding errors in posterior simulators can produce reasonable but incorrect approximations of posterior moments. This article develops simple tests of posterior simulators that detect both kinds of errors, and uses them to detect and correct errors in two previously published articles. The tests exploit the fact that a Bayesian model specifies the joint distribution of observables (data) and unobservables (parameters). There are two joint distribution simulators. The marginal-conditional simulator draws unobservables from the prior and then observables conditional on unobservables. The successive-conditional simulator alternates between the posterior simulator and an observables simulator. Formal comparison of moment approximations of the two simulators reveals existing analytical or coding errors in the posterior simulator.}},
  author = {Geweke, John},
  citeulike-article-id = {8901401},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/27590449},
  citeulike-linkout-1 = {http://www.jstor.org/stable/27590449},
  issn = {01621459},
  journal = {Journal of the American Statistical Association},
  number = {467},
  pages = {799--804},
  posted-at = {2011-03-10 18:51:38},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Getting it right: joint distribution tests of posterior simulators}},
  volume = {99},
  year = {2004}
}

@article{Smith95,
  abstract = {{One way to estimate variance components is by restricted maximum likelihood. The log-likelihood function is fully defined by the Cholesky factor of a matrix that is usually large and sparse. In this article forward and backward differentiation methods are developed for calculating the first and second derivatives of the Cholesky factor and its functions. These differentiation methods are general and can be applied to either a full or a sparse matrix. Moreover, these methods can be used to calculate the derivatives that are needed for restricted maximum likelihood, resulting in substantial savings in computation.}},
  author = {Smith, S. P.},
  citeulike-article-id = {8878046},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/1390762},
  citeulike-linkout-1 = {http://www.jstor.org/stable/1390762},
  issn = {10618600},
  journal = {Journal of Computational and Graphical Statistics},
  number = {2},
  pages = {134--147},
  posted-at = {2011-03-10 18:51:24},
  priority = {2},
  publisher = {American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of America},
  title = {{Differentiation of the Cholesky Algorithm}},
  volume = {4},
  year = {1995}
}

@article{KnorrHeld02,
  abstract = {{			Gaussian Markov random field (GMRF) models are commonly used to model spatial correlation in disease mapping applications. For Bayesian inference by MCMC, so far mainly single\&\#150;site updating algorithms have been considered. However, convergence and mixing properties of such algorithms can be extremely poor due to strong dependencies of parameters in the posterior distribution. In this paper, we propose various block sampling algorithms in order to improve the MCMC performance. The methodology is rather general, allows for non\&\#150;standard full conditionals, and can be applied in a modular fashion in a large number of different scenarios. For illustration we consider three different applications: two formulations for spatial modelling of a single disease (with and without additional unstructured parameters respectively), and one formulation for the joint analysis of two diseases. The results indicate that the largest benefits are obtained if parameters and the corresponding hyperparameter are updated jointly in one large block. Implementation of such block algorithms is relatively easy using methods for fast sampling of Gaussian Markov random fields (Rue, 2001). By comparison, Monte Carlo estimates based on single\&\#150;site updating can be rather misleading, even for very long runs. Our results may have wider relevance for efficient MCMC simulation in hierarchical models with Markov random field components.}},
  author = {Knorr-Held, L. and Rue, H.},
  citeulike-article-id = {399820},
  citeulike-linkout-0 = {http://www.ingentaconnect.com/content/bpl/sjos/2002/00000029/00000004/art00308},
  issn = {0303-6898},
  journal = {Scandinavian Journal of Statistics},
  keywords = {bayesian},
  month = dec,
  number = {4},
  pages = {597--614},
  posted-at = {2011-03-10 18:46:03},
  priority = {2},
  title = {{On Block Updating in Markov Random Field Models for Disease Mapping}},
  volume = {29},
  year = {2002}
}

@article{Blackwell73,
  author = {Blackwell, D. and Macqueen, J. B.},
  citeulike-article-id = {1495499},
  journal = {The Annals of Statistics},
  keywords = {bayesian},
  pages = {353--355},
  posted-at = {2011-03-10 18:29:22},
  priority = {2},
  title = {{Ferguson distributions via P\'{o}lya urn schemes}},
  volume = {1},
  year = {1973}
}

@article{Neal00,
  abstract = {{This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.}},
  author = {Neal, Radford M.},
  citeulike-article-id = {634913},
  citeulike-linkout-0 = {http://www-clmc.usc.edu/\~{}cs599\_ct/neal-TR1998.pdf},
  citeulike-linkout-1 = {http://dx.doi.org/10.2307/1390653},
  citeulike-linkout-2 = {http://www.jstor.org/stable/1390653},
  issn = {10618600},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {bayesian},
  month = jun,
  number = {2},
  pages = {249--265},
  posted-at = {2011-03-10 18:28:02},
  priority = {2},
  publisher = {American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of America},
  title = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
  volume = {9},
  year = {2000}
}

@article{Guha08,
  author = {Guha and Subharup},
  citeulike-article-id = {2852209},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/106186008x319854},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/jcgs/2008/00000017/00000002/art00009},
  issn = {1061-8600},
  journal = {Journal of Computational \&\#38; Graphical Statistics},
  keywords = {bayesian},
  month = jun,
  number = {2},
  pages = {410--425},
  posted-at = {2011-03-10 18:26:57},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Posterior Simulation in the Generalized Linear Mixed Model With Semiparametric Random Effects}},
  volume = {17},
  year = {2008}
}

@article{Brooks00,
  abstract = {{In this article we discuss the problem of assessing the performance of Markov chain Monte Carlo (MCMC) algorithms on the basis of simulation output. In essence, we extend the original ideas of Gelman and Rubin and, more recently, Brooks and Gelman, to problems where we are able to split the variation inherent within the MCMC simulation output into two distinct groups. We show how such a diagnostic may be useful in assessing the performance of MCMC samplers addressing model choice problems, such as the reversible jump MCMC algorithm. In the model choice context, we show how the reversible jump MCMC simulation output for parameters that retain a coherent interpretation throughout the simulation, can be used to assess convergence. By considering various decompositions of the sampling variance of this parameter, we can assess the performance of our MCMC sampler in terms of its mixing properties both within and between models and we illustrate our approach in both the graphical Gaussian models and normal mixtures context. Finally, we provide an example of the application of our diagnostic to the assessment of the influence of different starting values on MCMC simulation output, thereby illustrating the wider utility of our method beyond the Bayesian model choice and reversible jump MCMC context.}},
  author = {Brooks, S. P. and Giudici, P.},
  citeulike-article-id = {8823369},
  citeulike-linkout-0 = {http://www.jstor.org/stable/1390654},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {*file-import-11-02-15},
  number = {2},
  posted-at = {2011-02-15 15:13:11},
  priority = {2},
  publisher = {American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of America},
  title = {{Markov Chain Monte Carlo Convergence Assessment via Two-Way Analysis of Variance}},
  volume = {9},
  year = {2000}
}

@electronic{Pasarica03,
  abstract = {{Using existing theory on efficient jumping rules and on adaptive MCMC, we construct and demonstrate the effectiveness of a workable scheme for improving the efficiency of Metropolis algorithms. A good choice of the proposal distribution is crucial for the rapid convergence of the Metropolis algorithm. In this paper, given a family of parametric Markovian kernels, we develop an algorithm for optimizing the kernel by maximizing the expected squared jumped distance, an objective function that characterizes the Markov chain under its d-dimensional stationary distribution. The algorithm uses the information accumulated by a single path and adapts the choice of the parametric kernel in the direction of the local maximum of the objective function using multiple importance sampling techniques. We follow a two-stage approach: a series of adaptive optimization steps followed by an MCMC run with fixed kernel. It is not necessary for the adaptation itself to converge. Using several examples, we demonstrate the effectiveness of our method, even for cases in which the Metropolis transition kernel is initialized at very poor values.}},
  author = {Pasarica, Cristian and Gelman, Andrew},
  citeulike-article-id = {8823359},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.2435},
  keywords = {mcmc},
  posted-at = {2011-02-15 15:11:45},
  priority = {2},
  title = {{Adaptively scaling the Metropolis algorithm using expected squared jumped distance}},
  year = {2003}
}

@article{Rinaldo10,
  abstract = {{Summary: We study generalized density-based clustering in which sharply defined clusters such as clusters on lower-dimensional manifolds are allowed. We show that accurate clustering is possible even in high dimensions. We propose two data-based methods for choosing the bandwidth and we study the stability properties of density clusters. We show that a simple graph-based algorithm successfully approximates the high density clusters.}},
  author = {Rinaldo, Alessandro and Wasserman, Larry},
  citeulike-article-id = {8819542},
  citeulike-linkout-0 = {http://dx.doi.org/10.1214/10-AOS797},
  journal = {Annals of Statistics},
  keywords = {clustering, density, estimation, kernel},
  number = {5},
  pages = {2678--2722},
  posted-at = {2011-02-14 18:16:46},
  priority = {2},
  title = {{Generalized density clustering}},
  volume = {38},
  year = {2010}
}

@article{Friel08,
  author = {Friel, N. and Pettitt, A. N.},
  citeulike-article-id = {8819522},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2007.00650.x},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {analysis, bayes, bayesian, choice, factor, hidden, markov, mcmc, model, regression, survival},
  number = {3},
  pages = {589--607},
  posted-at = {2011-02-14 18:07:33},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  title = {{Marginal likelihood estimation via power posteriors}},
  volume = {70},
  year = {2008}
}

@article{Ferguson73,
  author = {Ferguson, T. S.},
  citeulike-article-id = {936707},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2958008},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2958008},
  issn = {00905364},
  journal = {The Annals of Statistics},
  keywords = {bayesian},
  number = {2},
  pages = {209--230},
  posted-at = {2011-02-14 18:01:53},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{A Bayesian Analysis of Some Nonparametric Problems}},
  volume = {1},
  year = {1973}
}

@article{Skilling06,
  author = {Skilling, J.},
  citeulike-article-id = {7510481},
  journal = {Bayesian Analysis},
  keywords = {bayesian},
  number = {4},
  pages = {833--860},
  posted-at = {2011-02-01 18:16:42},
  priority = {2},
  title = {{Nested sampling for general Bayesian computation}},
  volume = {1},
  year = {2006}
}

@article{Calderhead09,
  abstract = {{A Bayesian approach to model comparison based on the integrated or marginal likelihood is considered, and applications to linear regression models and nonlinear ordinary differential equation (ODE) models are used as the setting in which to elucidate and further develop existing statistical methodology. The focus is on two methods of marginal likelihood estimation. First, a statistical failure of the widely employed Posterior Harmonic Mean estimator is highlighted. It is demonstrated that there is a systematic bias capable of significantly skewing Bayes factor estimates, which has not previously been highlighted in the literature. Second, a detailed study of the recently proposed Thermodynamic Integral estimator is presented, which characterises the error associated with its discrete form. An experimental study using analytically tractable linear regression models highlights substantial differences with recently published results regarding optimal discretisation. Finally, with the insights gained, it is demonstrated how Population MCMC and thermodynamic integration methods may be elegantly combined to estimate Bayes factors accurately enough to discriminate between nonlinear models based on systems of ODEs, which has important application in describing the behaviour of complex processes arising in a wide variety of research areas, such as Systems Biology, Computational Ecology and Chemical Engineering.}},
  author = {Calderhead, Ben and Girolami, Mark},
  citeulike-article-id = {5381884},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.csda.2009.07.025},
  day = {01},
  issn = {01679473},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {bayesian},
  month = oct,
  number = {12},
  pages = {4028--4045},
  posted-at = {2011-02-01 18:15:41},
  priority = {2},
  title = {{Estimating Bayes factors via thermodynamic integration and population MCMC}},
  volume = {53},
  year = {2009}
}

@techreport{Minka05,
  address = {Whistler, BC Canada},
  author = {Minka, T. P. and Ghahramani, Z.},
  citeulike-article-id = {8682563},
  institution = {NIPS'03 Workshop on Nonparametric Bayesian Methods and Infinite Models, 13},
  keywords = {bayesian},
  posted-at = {2011-01-23 13:01:57},
  priority = {2},
  title = {{Expectation propagation for infinite mixtures}},
  year = {2003}
}

@article{Kass95,
  abstract = {{In a 1935 paper and in his book Theory of probability, Jeffresy developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpies was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: From Jeffrey's Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. Bayes factors are very general and do not require alternative models to be nested. Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. In "non-Bayesian significance tests. The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. Bayes factors are useful for guiding an evolutionary model-building process. It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.}},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  citeulike-article-id = {1778922},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2291091},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2291091},
  issn = {01621459},
  journal = {Journal of the American Statistical Association},
  keywords = {bayesian},
  month = jun,
  number = {430},
  pages = {773--795},
  posted-at = {2011-01-22 17:57:03},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Bayes factors}},
  volume = {90},
  year = {1995}
}

@article{deLima10,
  abstract = {{The estimation of multivariate densities using the kernel method has wide applicability. However, this problem has received less attention than the univariate case. This is mainly due to the increasing difficulty in estimating the optimal smoothing matrix, especially when the components are correlated. To overcome this difficulty, we propose in this work a Bayesian method to estimate the optimal smoothing matrix H. A loss function is defined and the estimator of H is the matrix minimising the loss function. We carried out simulations with a mixture of multivariate densities with correlation and the results were highly satisfactory.}},
  author = {de Lima, Max S. and Atuncar, Gregorio S.},
  citeulike-article-id = {8676931},
  citeulike-linkout-0 = {http://dx.doi.org/10.1080/10485252.2010.485200},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/tandf/gnst/2011/00000023/00000001/art00010},
  issn = {1048-5252},
  journal = {Journal of Nonparametric Statistics},
  keywords = {density},
  month = mar,
  pages = {137--148},
  posted-at = {2011-01-22 17:31:17},
  priority = {2},
  publisher = {Taylor and Francis Ltd},
  title = {A {B}ayesian method to estimate the optimal bandwidth for multivariate kernel estimator},
  year = {2011}
}

@article{Christensen06,
  author = {Christensen, Ole F. and Roberts, Gareth O. and Sk\"{o}ld, Martin},
  citeulike-article-id = {512348},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/106186006x100470},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/jcgs/2006/00000015/00000001/art00001},
  issn = {1061-8600},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {mcmc},
  month = mar,
  number = {1},
  pages = {1--17},
  posted-at = {2010-12-07 11:54:15},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Robust Markov chain Monte Carlo Methods for Spatial Generalized Linear Mixed Models}},
  volume = {15},
  year = {2006}
}

@article{Neal99,
  author = {Neal, R. M.},
  citeulike-article-id = {7952161},
  journal = {Bayesian Statistics},
  keywords = {mcmc},
  pages = {475--501},
  posted-at = {2010-10-06 11:57:26},
  priority = {2},
  title = {{Regression and classification using Gaussian process priors (with discussion)}},
  volume = {6},
  year = {1999}
}

@article{Tipping01,
  author = {Tipping, Michael E.},
  citeulike-article-id = {7890542},
  journal = {Journal of Machine Learning Research},
  keywords = {gp},
  pages = {211--244},
  posted-at = {2010-09-23 17:36:48},
  priority = {2},
  publisher = {JMLR.org},
  title = {{Sparse bayesian learning and the relevance vector machine}},
  volume = {1},
  year = {2001}
}

@inproceedings{Rasmussen05,
  abstract = {{The Relevance Vector Machine (RVM) is a sparse approximate Bayesian kernel method. It provides full predictive distributions for test cases. However, the predictive uncertainties have the unintuitive property, that they get smaller the further you move away from the training cases. We give a thorough analysis. Inspired by the analogy to non-degenerate Gaussian Processes, we suggest augmentation to solve the problem. The purpose of the resulting model, RVM*, is primarily to corroborate the theoretical and experimental analysis. Although RVM* could be used in practical applications, it is no longer a truly sparse model. Experiments show that sparsity comes at the expense of worse predictive. distributions.}},
  address = {New York, NY, USA},
  author = {Rasmussen, Carl E. and Candela, Joaquin Q.},
  booktitle = {Proceedings of the 22nd international conference on Machine learning},
  citeulike-article-id = {7316755},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1102438},
  citeulike-linkout-1 = {http://dx.doi.org/10.1145/1102351.1102438},
  isbn = {1-59593-180-5},
  keywords = {gp},
  location = {Bonn, Germany},
  pages = {689--696},
  posted-at = {2010-09-23 17:25:46},
  priority = {2},
  publisher = {ACM},
  series = {ICML '05},
  title = {{Healing the relevance vector machine through augmentation}},
  year = {2005}
}

@article{Green95,
  abstract = {{This article proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments. Some key words: Change-point analysis, Image...}},
  author = {Green, P.},
  citeulike-article-id = {524309},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.5225},
  journal = {Biometrika},
  keywords = {mcmc},
  pages = {711--732},
  posted-at = {2010-09-23 17:08:00},
  priority = {2},
  title = {Reversible jump {M}arkov chain {M}onte {C}arlo computation and {B}ayesian model determination},
  volume = {82},
  year = {1995}
}

@incollection{Seeger03,
  author = {Seeger, M. and Williams, C.},
  booktitle = {Workshop on AI and Statistics 9},
  citeulike-article-id = {3152311},
  keywords = {gp},
  posted-at = {2010-09-23 16:45:03},
  priority = {2},
  title = {Fast forward selection to speed up sparse {G}aussian process regression},
  year = {2003}
}

@article{Cressie08,
  abstract = {{Spatial statistics for very large spatial data sets is challenging. The size of the data set, \&quot;n\&quot;, causes problems in computing optimal spatial predictors such as kriging, since its computational cost is of order <formula format=\&quot;inline\&quot;><file name=\&quot;rssb\_633\_mu1.gif\&quot; type=\&quot;gif\&quot; /></formula>. In addition, a large data set is often defined on a large spatial domain, so the spatial process of interest typically exhibits non-stationary behaviour over that domain. A flexible family of non-stationary covariance functions is defined by using a set of basis functions that is fixed in number, which leads to a spatial prediction method that we call fixed rank kriging. Specifically, fixed rank kriging is kriging within this class of non-stationary covariance functions. It relies on computational simplifications when \&quot;n\&quot; is very large, for obtaining the spatial best linear unbiased predictor and its mean-squared prediction error for a hidden spatial process. A method based on minimizing a weighted Frobenius norm yields best estimators of the covariance function parameters, which are then substituted into the fixed rank kriging equations. The new methodology is applied to a very large data set of total column ozone data, observed over the entire globe, where \&quot;n\&quot; is of the order of hundreds of thousands. Copyright 2008 Royal Statistical Society.}},
  author = {Cressie, Noel and Johannesson, Gardar},
  citeulike-article-id = {7890311},
  citeulike-linkout-0 = {http://ideas.repec.org/a/bla/jorssb/v70y2008i1p209-226.html},
  journal = {Journal Of The Royal Statistical Society Series B},
  keywords = {gp},
  number = {1},
  pages = {209--226},
  posted-at = {2010-09-23 16:39:27},
  priority = {2},
  title = {{Fixed rank kriging for very large spatial data sets}},
  volume = {70},
  year = {2008}
}

@book{Rasmussen06,
  author = {Rasmussen, Carl E. and Williams, Christopher},
  booktitle = {Gaussian Processes for Machine Learning},
  citeulike-article-id = {4015194},
  citeulike-linkout-0 = {http://www.gaussianprocess.org/gpml/},
  keywords = {gp},
  posted-at = {2010-09-23 16:07:47},
  priority = {2},
  publisher = {MIT Press},
  title = {{Gaussian Processes for Machine Learning}},
  year = {2006}
}

@article{Jones98,
  address = {Hingham, MA, USA},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  citeulike-article-id = {7889958},
  journal = {Journal of Global Optimization},
  keywords = {gp},
  number = {4},
  pages = {455--492},
  posted-at = {2010-09-23 15:54:24},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
  volume = {13},
  year = {1998}
}

@proceedings{DBLP:conf/nips/2005,
  booktitle = {NIPS},
  citeulike-article-id = {7869094},
  keywords = {gp},
  posted-at = {2010-09-21 15:47:26},
  priority = {2},
  title = {{Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada]}},
  year = {2005}
}

@inproceedings{Snelson05,
  author = {Snelson, Edward and Ghahramani, Zoubin},
  booktitle = {NIPS},
  citeulike-article-id = {7869093},
  keywords = {gp},
  posted-at = {2010-09-21 15:47:26},
  priority = {2},
  title = {{Sparse Gaussian Processes using Pseudo-inputs}},
  year = {2005}
}

@article{Candela05,
  author = {Candela, Joaquin Q. and Rasmussen, Carl E.},
  citeulike-article-id = {7869038},
  journal = {Journal of Machine Learning Research},
  keywords = {gp},
  pages = {1939--1959},
  posted-at = {2010-09-21 15:34:42},
  priority = {2},
  title = {{A Unifying View of Sparse Approximate Gaussian Process Regression}},
  volume = {6},
  year = {2005}
}

@article{Csato02,
  address = {Cambridge, MA, USA},
  author = {Csat\'{o}, Lehel and Opper, Manfred},
  citeulike-article-id = {7869006},
  journal = {Neural Comput.},
  keywords = {gp},
  number = {3},
  pages = {641--668},
  posted-at = {2010-09-21 15:32:18},
  priority = {2},
  publisher = {MIT Press},
  title = {{Sparse on-line Gaussian processes}},
  volume = {14},
  year = {2002}
}

@misc{gp,
  abstract = {{We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N , where N is the number of real data points, and hence obtain a sparse regression method which has N) training cost and ) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian...}},
  author = {Snelson, E. and Ghahramani, Z.},
  booktitle = {NIPS 18},
  citeulike-article-id = {1846806},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.2209},
  journal = {Neural Information Processing Systems 18},
  keywords = {snelson05},
  posted-at = {2010-09-21 15:31:12},
  priority = {2},
  title = {{Sparse Gaussian Processes using Pseudo-inputs}},
  year = {2005}
}

@proceedings{DBLP:conf/nips/2002b,
  booktitle = {NIPS},
  citeulike-article-id = {7867817},
  editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
  keywords = {gp},
  posted-at = {2010-09-21 12:33:52},
  priority = {2},
  publisher = {MIT Press},
  title = {{Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]}},
  year = {2003}
}

@inproceedings{Lawrence02,
  author = {Lawrence, Neil D. and Seeger, Matthias and Herbrich, Ralf},
  booktitle = {NIPS},
  citeulike-article-id = {7867816},
  editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
  keywords = {gp},
  pages = {609--616},
  posted-at = {2010-09-21 12:33:51},
  priority = {2},
  publisher = {MIT Press},
  title = {{Fast Sparse Gaussian Process Methods: The Informative Vector Machine}},
  year = {2002}
}

@article{Gredilla10,
  abstract = {{We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.}},
  author = {L\'{a}zaro-Gredilla, M. and Quinonero-Candela, J. and Rasmussen, C. E. and Figueiras-Vidal, A. R.},
  citeulike-article-id = {7867753},
  citeulike-linkout-0 = {http://www.jmlr.org/papers/volume11/lazaro-gredilla10a/lazaro-gredilla10a.pdf},
  journal = {Journal of Machine Learning Research},
  keywords = {gp},
  pages = {1865--1881},
  posted-at = {2010-09-21 12:27:36},
  priority = {2},
  title = {{Sparse Spectrum Gaussian Process Regression}},
  volume = {11},
  year = {2010}
}

@article{Qi09,
  author = {Qi, Yuan and Abdel-Gawad, Ahmed H. and Minka, Thomas P.},
  citeulike-article-id = {7867681},
  journal = {CoRR},
  keywords = {gp},
  posted-at = {2010-09-21 12:14:03},
  priority = {2},
  title = {{Variable sigma Gaussian processes: An expectation propagation perspective}},
  volume = {abs/0910.0668},
  year = {2009}
}

@proceedings{DBLP:conf/icml/2010,
  booktitle = {ICML},
  citeulike-article-id = {7867649},
  editor = {F{\"{u}}rnkranz, Johannes and Joachims, Thorsten},
  keywords = {gp},
  posted-at = {2010-09-21 12:09:35},
  priority = {2},
  publisher = {Omnipress},
  title = {{Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel}},
  year = {2010}
}

@inproceedings{Yan10,
  author = {Yan, Feng and Qi, Yuan A.},
  booktitle = {ICML},
  citeulike-article-id = {7867648},
  editor = {F{\"{u}}rnkranz, Johannes and Joachims, Thorsten},
  keywords = {gp},
  pages = {1183--1190},
  posted-at = {2010-09-21 12:09:35},
  priority = {2},
  publisher = {Omnipress},
  title = {{Sparse Gaussian Process Regression via L1 Penalization}},
  year = {2010}
}

@inproceedings{Vanhatalo10,
  author = {Vanhatalo, Jarno and Vehtari, Aki},
  booktitle = {UAI '10: Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
  citeulike-article-id = {7867539},
  keywords = {gp},
  posted-at = {2010-09-21 12:02:39},
  priority = {2},
  title = {Speeding up the binary {G}aussian process classification},
  year = {2010}
}

@inproceedings{Minka09,
  address = {Arlington, Virginia, United States},
  author = {Minka, Thomas P. and Xiang, Rongjing and Qi, Yuan A.},
  booktitle = {UAI '09: Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
  citeulike-article-id = {7867457},
  keywords = {gp},
  location = {Montreal, Quebec, Canada},
  pages = {411--418},
  posted-at = {2010-09-21 11:53:09},
  priority = {2},
  publisher = {AUAI Press},
  title = {{Virtual vector machine for Bayesian online classification}},
  year = {2009}
}

@article{McCulloch00,
  abstract = {{We empirically test existing theories on the provision of public goods, in particular air quality, using data on sulfur dioxide (SO2) concentrations from the Global Environment Monitoring Projects for 107 cities in 42 countries from 1971 to 1996. The results are as follows: First, we provide additional support for the claim that the degree of democracy has an independent positive effect on air quality. Second, we find that among democracies, presidential systems are more conducive to air quality than parliamentary ones. Third, in testing competing claims about the effect of interest groups on public goods provision in democracies we establish that labor union strength contributes to lower environmental quality, whereas the strength of green parties has the opposite effect.}},
  author = {McCulloch, Robert E. and Polson, Nicholas G. and Rossi, Peter E.},
  citeulike-article-id = {7745439},
  citeulike-linkout-0 = {http://ideas.repec.org/a/eee/econom/v99y2000i1p173-193.html},
  journal = {Journal of Econometrics},
  keywords = {air, and, civil, democracy, groups, interest, liberties, mcmc, parliamentary, pollution, presidential, sulf, systems},
  month = nov,
  number = {1},
  pages = {173--193},
  posted-at = {2010-08-30 16:02:55},
  priority = {2},
  title = {{A Bayesian analysis of the multinomial probit model with fully identified parameters}},
  volume = {99},
  year = {2000}
}

@article{Nobile00,
  author = {Nobile, Agostino},
  citeulike-article-id = {7745436},
  citeulike-linkout-0 = {http://ideas.repec.org/a/eee/econom/v99y2000i2p335-345.html},
  journal = {Journal of Econometrics},
  keywords = {mcmc},
  month = dec,
  number = {2},
  pages = {335--345},
  posted-at = {2010-08-30 16:01:12},
  priority = {2},
  title = {{Comment: Bayesian multinomial probit models with a normalization constraint}},
  volume = {99},
  year = {2000}
}

@article{Imai05,
  author = {Imai, Kosuke and van Dyk, David A.},
  citeulike-article-id = {7745432},
  citeulike-linkout-0 = {http://ideas.repec.org/a/eee/econom/v124y2005i2p311-334.html},
  journal = {Journal of Econometrics},
  keywords = {mcmc},
  month = feb,
  number = {2},
  pages = {311--334},
  posted-at = {2010-08-30 15:59:41},
  priority = {2},
  title = {{A Bayesian analysis of the multinomial probit model using marginal data augmentation}},
  volume = {124},
  year = {2005}
}

@article{Nobile98,
  address = {Hingham, MA, USA},
  author = {Nobile, Agostino},
  citeulike-article-id = {7745430},
  journal = {Statistics and Computing},
  keywords = {mcmc},
  number = {3},
  pages = {229--242},
  posted-at = {2010-08-30 15:57:52},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{A hybrid Markov chain for the Bayesian analysis of the multinomial probit model}},
  volume = {8},
  year = {1998}
}

@article{Roberts03,
  abstract = {{We consider a class of Langevin diffusions with state-dependent volatility. The volatility of the diffusion is chosen so as to make the stationary distribution of the diffusion with respect to its natural clock, a heated version of the stationary density of interest. The motivation behind this construction is the desire to construct uniformly ergodic diffusions with required stationary densities. Discrete time algorithms constructed by Hastings accept reject mechanisms are constructed from discretisations of the algorithms, and the properties of these algorithms are investigated.}},
  author = {Roberts, G. O. and Stramer, O.},
  citeulike-article-id = {7353479},
  citeulike-linkout-0 = {http://dx.doi.org/10.1023/a:1023562417138},
  citeulike-linkout-1 = {http://www.springerlink.com/content/nv57r5gq18088257},
  day = {1},
  issn = {13875841},
  journal = {Methodology and Computing in Applied Probability},
  keywords = {mcmc},
  month = dec,
  number = {4},
  pages = {337--357},
  posted-at = {2010-06-23 16:55:08},
  priority = {2},
  publisher = {Springer U.S.},
  title = {{Langevin Diffusions and Metropolis-Hastings Algorithms}},
  volume = {4},
  year = {2002}
}

@article{Roberts98,
  abstract = {{We consider the optimal scaling problem for proposal distributions in Hastings--Metropolis algorithms derived from Langevin diffusions. We propose an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterized by its overall acceptance rate, independently of the target distribution. The asymptotically optimal acceptance rate is 0.574. We show that, as a function of dimension n, the complexity of the algorithm is O(n<sup>1/3</sup>), which compares favourably with the O(n) complexity of random walk Metropolis algorithms. We illustrate this comparison with some example simulations.}},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  citeulike-article-id = {7351424},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2985986},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2985986},
  issn = {13697412},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  keywords = {mcmc},
  number = {1},
  pages = {255--268},
  posted-at = {2010-06-23 11:48:50},
  priority = {2},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  title = {{Optimal Scaling of Discrete Approximations to Langevin Diffusions}},
  volume = {60},
  year = {1998}
}

@book{Gilks96,
  author = {Gilks, W. R. and Spiegelhalter, D. J.},
  citeulike-article-id = {7348691},
  citeulike-linkout-0 = {http://scholar.google.de/scholar.bib?q=info:AN5YKWErdFAJ:scholar.google.com/\&\#38;output=citation\&\#38;hl=de\&\#38;ct=citation\&\#38;cd=0},
  keywords = {carlo, chain, gibbs, learning, markov, mchine, mcmc, ml, monte},
  posted-at = {2010-06-22 11:46:36},
  priority = {2},
  publisher = {Chapman \& Hall/CRC},
  title = {{Markov chain Monte Carlo in practice}},
  year = {1996}
}

@article{Stein87,
  abstract = {{Latin hypercube sampling (McKay, Conover, and Beckman 1979) is a method of sampling that can be used to produce input values for estimation of expectations of functions of output variables. The asymptotic variance of such an estimate is obtained. The estimate is also shown to be asymptotically normal. Asymptotically, the variance is less than that obtained using simple random sampling, with the degree of variance reduction depending on the degree of additivity in the function being integrated. A method for producing Latin hypercube samples when the components of the input variables are statistically dependent is also described. These techniques are applied to a simulation of the performance of a printer actuator.}},
  author = {Stein, Michael},
  citeulike-article-id = {7348683},
  citeulike-linkout-0 = {http://www.jstor.org/stable/1269769},
  journal = {Technometrics},
  keywords = {design},
  number = {2},
  pages = {143--151},
  posted-at = {2010-06-22 11:42:01},
  priority = {2},
  publisher = {American Statistical Association and American Society for Quality},
  title = {{Large Sample Properties of Simulations Using Latin Hypercube Sampling}},
  volume = {29},
  year = {1987}
}

@article{Izenman91,
  abstract = {{Advances in computation and the fast and cheap computational facilities now available to statisticians have had a significant impact upon statistical research, and especially the development of nonparametric data analysis procedures. In particular, theoretical and applied research on nonparametric density estimation has had a noticeable influence on related topics, such as nonparametric regression, nonparametric discrimination, and nonparametric pattern recognition. This article reviews recent developments in nonparametric density estimation and includes topics that have been omitted from review articles and books on the subject. The early density estimation methods, such as the histogram, kernel estimators, and orthogonal series estimators are still very popular, and recent research on them is described. Different types of restricted maximum likelihood density estimators, including order-restricted estimators, maximum penalized likelihood estimators, and sieve estimators, are discussed, where restrictions are imposed upon the class of densities or on the form of the likelihood function. Nonparametric density estimators that are data-adaptive and lead to locally smoothed estimators are also discussed; these include variable partition histograms, estimators based on statistically equivalent blocks, nearest-neighbor estimators, variable kernel estimators, and adaptive kernel estimators. For the multivariate case, extensions of methods of univariate density estimation are usually straightforward but can be computationally expensive. A method of multivariate density estimation that did not spring from a univariate generalization is described, namely, projection pursuit density estimation, in which both dimensionality reduction and density estimation can be pursued at the same time. Finally, some areas of related research are mentioned, such as nonparametric estimation of functionals of a density, robust parametric estimation, semiparametric models, and density estimation for censored and incomplete data, directional and spherical data, and density estimation for dependent sequences of observations.}},
  author = {Izenman, Alan J.},
  citeulike-article-id = {7348679},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2289732},
  journal = {Journal of the American Statistical Association},
  keywords = {density},
  number = {413},
  pages = {205--224},
  posted-at = {2010-06-22 11:37:38},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Recent Developments in Nonparametric Density Estimation}},
  volume = {86},
  year = {1991}
}

@book{Robert05,
  address = {Secaucus, NJ, USA},
  author = {Robert, Christian P. and Casella, George},
  citeulike-article-id = {7348677},
  keywords = {mcmc},
  posted-at = {2010-06-22 11:33:32},
  priority = {2},
  publisher = {Springer-Verlag New York, Inc.},
  title = {{Monte Carlo Statistical Methods (Springer Texts in Statistics)}},
  year = {2005}
}

@book{Baker77,
  author = {Baker, Christopher T. H.},
  citeulike-article-id = {7348675},
  keywords = {algebra},
  posted-at = {2010-06-22 11:32:31},
  priority = {2},
  publisher = {Clarendon Press},
  title = {{The numerical treatment of integral equations}},
  year = {1977}
}

@article{Murthy88,
  abstract = {{A survey of methods for sensitivity analysis of the algebraic eigenvalue problem for non-Hermitian matrices is presented. In addition, a modification of one method based on a better normalizing condition is proposed. Methods are classified as Direct or Adjoint and are evaluated for efficiency. Operation counts are presented in terms of matrix size, number of design variables and number of eigenvalues and eigenvectors of interest. The effect of the sparsity of the matrix and its derivatives is also considered, and typical solution times are given. General guidelines are established for the selection of the most efficient method.}},
  address = {Department of Aerospace and Ocean Engineering, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061, U.S.A.},
  author = {Murthy, Durbha V. and Haftka, Raphael T.},
  citeulike-article-id = {7348107},
  citeulike-linkout-0 = {http://dx.doi.org/10.1002/nme.1620260202},
  citeulike-linkout-1 = {http://www3.interscience.wiley.com/cgi-bin/abstract/110547344/ABSTRACT},
  issn = {1097-0207},
  journal = {International Journal for Numerical Methods in Engineering},
  keywords = {algebra},
  number = {2},
  pages = {293--311},
  posted-at = {2010-06-22 10:53:03},
  priority = {2},
  title = {{Derivatives of eigenvalues and eigenvectors of a general complex matrix}},
  volume = {26},
  year = {1988}
}

@article{Peskun73,
  abstract = {The sampling method proposed by Metropolis et al. (1953) requires the simulation of a Markov chain with a specified {pi} as its stationary distribution. Hastings (1970) outlined a general procedure for constructing and simulating such a Markov chain. The matrix P of transition probabilities is constructed using a defined symmetric function sij and an arbitrary transition matrix Q. Here, for a given Q, the relative merits of the two simple choices for sij suggested by Hastings (1970) are discussed. The optimum choice for sij is shown to be one of these. For the other choice, those Q are given which are known to make the sampling method based on P asymptotically less precise than independent sampling. 10.1093/biomet/60.3.607},
  author = {Peskun, P. H.},
  citeulike-article-id = {4595220},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/biomet/60.3.607},
  citeulike-linkout-1 = {http://biomet.oxfordjournals.org/cgi/content/abstract/60/3/607},
  day = {1},
  journal = {Biometrika},
  keywords = {mcmc},
  month = dec,
  number = {3},
  pages = {607--612},
  posted-at = {2010-06-22 10:44:42},
  priority = {2},
  title = {{Optimum Monte-Carlo sampling using Markov chains}},
  volume = {60},
  year = {1973}
}

@article{Diaconis91,
  abstract = {{We develop bounds for the second largest eigenvalue and spectral gap of a reversible Markov chain. The bounds depend on geometric quantities such as the maximum degree, diameter and covering number of associated graphs. The bounds compare well with exact answers for a variety of simple chains and seem better than bounds derived through Cheeger-like inequalities. They offer improved rates of convergence for the random walk associated to approximate computation of the permanent.}},
  author = {Diaconis, Persi and Stroock, Daniel},
  citeulike-article-id = {7348086},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2959624},
  journal = {The Annals of Applied Probability},
  keywords = {mcmc},
  number = {1},
  pages = {36--61},
  posted-at = {2010-06-22 10:42:52},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Geometric Bounds for Eigenvalues of Markov Chains}},
  volume = {1},
  year = {1991}
}

@article{Roberts01,
  abstract = {{We review and extend results related to optimal scaling of Metropolis-Hastings algorithms. We present various theoretical results for the high-dimensional limit. We also present simulation studies which confirm the theoretical results in finite-dimensional contexts.}},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  citeulike-article-id = {7348072},
  citeulike-linkout-0 = {http://www.jstor.org/stable/3182776},
  journal = {Statistical Science},
  keywords = {mcmc},
  number = {4},
  pages = {351--367},
  posted-at = {2010-06-22 10:36:12},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Optimal Scaling for Various Metropolis-Hastings Algorithms}},
  volume = {16},
  year = {2001}
}

@article{Bedard08,
  author = {B\'{e}dard, Myl\`{e}ne},
  citeulike-article-id = {7348040},
  citeulike-linkout-0 = {http://www.sciencedirect.com/science/article/B6V1B-4RGFD5G-1/2/1effaf5f33016ede8fed135190698967},
  citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.spa.2007.12.005},
  journal = {Stochastic Processes and their Applications},
  keywords = {convergence, mcmc, weak},
  number = {12},
  posted-at = {2010-06-22 10:23:56},
  priority = {2},
  title = {{Optimal acceptance rates for Metropolis algorithms: Moving beyond 0.234}},
  volume = {118},
  year = {2008}
}

@incollection{Gelman96,
  address = {New York},
  author = {Gelman, A. and Roberts, G. O. and Gilks, W. R.},
  booktitle = {Bayesian statistics, 5 (Alicante, 1994)},
  citeulike-article-id = {7348001},
  keywords = {mcmc},
  pages = {599--607},
  posted-at = {2010-06-22 09:56:01},
  priority = {2},
  publisher = {Oxford Univ. Press},
  series = {Oxford Sci. Publ.},
  title = {Efficient {M}etropolis jumping rules},
  year = {1996}
}

@inproceedings{Williams00b,
  author = {Williams, Christopher K. I. and Seeger, Matthias},
  booktitle = {NIPS},
  citeulike-article-id = {4694084},
  citeulike-linkout-0 = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/WilliamsS00},
  editor = {Leen, Todd K. and Dietterich, Thomas G. and Tresp, Volker and Leen, Todd K. and Dietterich, Thomas G. and Tresp, Volker},
  keywords = {kernel},
  pages = {682--688},
  posted-at = {2010-06-22 09:50:42},
  priority = {2},
  publisher = {MIT Press},
  title = {Using the {N}ystr{\"o}m Method to Speed Up Kernel Machines},
  year = {2000}
}

@proceedings{DBLP:conf/nips/2002,
  booktitle = {NIPS},
  citeulike-article-id = {7347987},
  editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
  keywords = {kernel},
  posted-at = {2010-06-22 09:48:56},
  priority = {2},
  publisher = {MIT Press},
  title = {{Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]}},
  year = {2003}
}

@inproceedings{Shawe-Taylor02,
  author = {Shawe-Taylor, John and Williams, Christopher K. I.},
  booktitle = {NIPS},
  citeulike-article-id = {7347986},
  editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
  keywords = {kernel},
  pages = {367--374},
  posted-at = {2010-06-22 09:48:56},
  priority = {2},
  publisher = {MIT Press},
  title = {{The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum}},
  year = {2002}
}

@inproceedings{Williams00,
  address = {San Francisco, CA, USA},
  author = {Williams, Christopher K. I. and Seeger, Matthias},
  booktitle = {ICML '00: Proceedings of the Seventeenth International Conference on Machine Learning},
  citeulike-article-id = {7347982},
  keywords = {kernel},
  pages = {1159--1166},
  posted-at = {2010-06-22 09:45:46},
  priority = {2},
  publisher = {Morgan Kaufmann Publishers Inc.},
  title = {{The Effect of the Input Density Distribution on Kernel-based Classifiers}},
  year = {2000}
}

@article{Andrieu08,
  address = {Hingham, MA, USA},
  author = {Andrieu, Christophe and Thoms, Johannes},
  citeulike-article-id = {7347978},
  journal = {Statistics and Computing},
  keywords = {mcmc},
  number = {4},
  pages = {343--373},
  posted-at = {2010-06-22 09:43:26},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{A tutorial on adaptive MCMC}},
  volume = {18},
  year = {2008}
}

@article{Kivinen04,
  author = {Kivinen, J. and Smola, A. J. and Williamson, R. C.},
  citeulike-article-id = {7201125},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/TSP.2004.830991},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {algorithms, analysis, artificial, bounds, classification, classifiers, convergence, descent, detection, gradient, hilbert, intelligence, kernel, kernels, kernels-based, large, learning, loss, machines, margin, novelty, online, operating, operation, processes, regression, signal, space, spaces, stochastic, support, system, vector, worst-case},
  month = aug,
  number = {8},
  posted-at = {2010-05-19 17:45:54},
  priority = {2},
  title = {{Online learning with kernels}},
  volume = {52},
  year = {2004}
}

@proceedings{DBLP:conf/his/2003,
  booktitle = {HIS},
  citeulike-article-id = {7201074},
  editor = {Abraham, Ajith and K{\"{o}}ppen, Mario and Franke, Katrin},
  keywords = {novelty},
  posted-at = {2010-05-19 17:17:36},
  priority = {2},
  publisher = {IOS Press},
  series = {Frontiers in Artificial Intelligence and Applications},
  title = {{Design and Application of Hybrid Intelligent Systems, HIS03, the Third International Conference on Hybrid Intelligent Systems, Melbourne, Australia, December 14-17, 2003}},
  volume = {105},
  year = {2003}
}

@inproceedings{Oliveira03,
  author = {Oliveira, Adriano L. I. and de Lima Neto, Fernando B. and de Lemos Meira, Silvio R.},
  booktitle = {HIS},
  citeulike-article-id = {7201073},
  editor = {Abraham, Ajith and K\"{o}ppen, Mario and Franke, Katrin},
  keywords = {novelty},
  pages = {66--76},
  posted-at = {2010-05-19 17:17:36},
  priority = {2},
  publisher = {IOS Press},
  series = {Frontiers in Artificial Intelligence and Applications},
  title = {{Novelty Detection for Short Time Series with Neural Networks}},
  volume = {105},
  year = {2003}
}

@article{FilipponePR09,
  author = {Filippone, Maurizio and Sanguinetti, Guido},
  citeulike-article-id = {7201064},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.patcog.2009.07.002},
  journal = {Pattern Recognition},
  keywords = {novelty},
  month = mar,
  number = {3},
  pages = {805--814},
  posted-at = {2010-05-19 17:12:27},
  priority = {2},
  title = {{Information Theoretic Novelty Detection}},
  volume = {43},
  year = {2010}
}

@article{Jones91,
  abstract = {{The asymptotically best bandwidth selectors for a kernel density estimator currently require the use of either unappealing higher order kernel pilot estimators or related Fourier transform methods. The point of this paper is to present a methodology which allows the fastest possible rate of convergence with the use of only nonnegative kernel estimators at all stages of the selection process. The essential idea is derived through careful study of factorizations of the pilot bandwidth in terms of the original bandwidth.}},
  author = {Jones, M. C. and Marron, J. S. and Park, B. U.},
  citeulike-article-id = {7127355},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2241911},
  journal = {The Annals of Statistics},
  keywords = {density},
  number = {4},
  pages = {1919--1932},
  posted-at = {2010-05-05 10:45:06},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{A simple root n bandwidth selector}},
  volume = {19},
  year = {1991}
}

@article{Bowman84,
  author = {Bowman, Adrian W.},
  citeulike-article-id = {7127335},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/biomet/71.2.353},
  journal = {Biometrika},
  keywords = {density},
  mrnumber = {MR767163 (86b:62061)},
  number = {2},
  pages = {353--360},
  posted-at = {2010-05-05 10:37:39},
  priority = {2},
  title = {{An alternative method of cross-validation for the smoothing of density estimates}},
  volume = {71},
  year = {1984}
}

@article{Wand94,
  author = {Wand, M. P. and Jones, M. C.},
  citeulike-article-id = {7127308},
  journal = {Computational Statistics},
  keywords = {density},
  pages = {97--116},
  posted-at = {2010-05-05 10:24:55},
  priority = {2},
  title = {{Multivariate plug-in bandwidth selection}},
  volume = {9},
  year = {1994}
}

@article{vanderLaan04,
  abstract = {{Likelihood-based cross-validation is a statistical tool for selecting a density estimate based on n i.i.d. observations from the true density among a collection of candidate density estimators. General examples are the selection of a model indexing a maximum likelihood estimator, and the selection of a bandwidth indexing a nonparametric (e.g. kernel) density estimator. In this article, we establish a finite sample result for a general class of likelihood-based cross-validation procedures (as indexed by the type of sample splitting used, e.g. V-fold cross-validation). This result implies that the cross-validation selector performs asymptotically as well (w.r.t. to the Kullback-Leibler distance to the true density) as a benchmark model selector which is optimal for each given dataset and depends on the true density. Crucial conditions of our theorem are that the size of the validation sample converges to infinity, which excludes leave-one-out cross-validation, and that the candidate density estimates are bounded away from zero and infinity. We illustrate these asymptotic results and the practical performance of likelihood-based cross-validation for the purpose of bandwidth selection with a simulation study. Moreover, we use likelihood-based cross-validation in the context of regulatory motif detection in DNA sequences.}},
  author = {van der Laan, Mark J. and Dudoit, Sandrine and Keles, Sunduz},
  citeulike-article-id = {7013867},
  citeulike-linkout-0 = {http://ideas.repec.org/a/bpj/sagmbi/v3y2004i1n4.html},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  keywords = {and, cross-validation, density, estimation, kullb, likelihood, maximum, methods, statistical, theory},
  number = {1},
  posted-at = {2010-04-13 10:15:44},
  priority = {2},
  title = {{Asymptotic optimality of likelihood-based cross-validation}},
  volume = {3},
  year = {2004}
}

@article{Kulasekera06,
  abstract = {{Problems with censored data arise frequently in survival analyses and reliability applications. The estimation of the density function of the failure times is often of interest. Two inherent problems in density estimation for lifetime data are the <i>spill-over</i> at the origin and the smoothing parameter selection. To address these issues, we propose the use of asymmetric kernels (like inverse Gaussian) with bandwidths selected by a Bayes criterion. We show strong pointwise consistency of the density estimator, and for suitable choices of the prior, we show that one can obtain meaningful bandwidths with the same rates of convergence as for the classical asymptotically optimal bandwidths.}},
  author = {Kulasekera, K. B. and Padgett, W. J.},
  citeulike-article-id = {7008268},
  citeulike-linkout-0 = {http://dx.doi.org/10.1080/10485250600556744},
  journal = {Journal of Nonparametric Statistics},
  keywords = {density},
  number = {2},
  pages = {129--143},
  posted-at = {2010-04-12 15:23:31},
  priority = {2},
  publisher = {Taylor \& Francis},
  title = {{Bayes bandwidth selection in kernel density estimation with censored data}},
  volume = {18},
  year = {2006}
}

@article{Gangopadhyay02,
  author = {Gangopadhyay, Ashis and Cheung, Kin},
  citeulike-article-id = {7008266},
  journal = {Journal of Nonparametric Statistics},
  keywords = {density},
  number = {6},
  pages = {655--664},
  posted-at = {2010-04-12 15:22:15},
  priority = {2},
  title = {BAYESIAN APPROACH TO THE CHOICE OF SMOOTHING PARAMETER IN KERNEL DENSITY ESTIMATION},
  volume = {14},
  year = {2002}
}

@article{Zhang09,
  abstract = {{This paper presents a Bayesian approach to bandwidth selection for multivariate kernel regression. A Monte Carlo study shows that under the average squared error criterion, the Bayesian bandwidth selector is comparable to the cross-validation method and clearly outperforms the bootstrapping and rule-of-thumb bandwidth selectors. The Bayesian bandwidth selector is applied to a multivariate kernel regression model that is often used to estimate the state-price density of Arrow-Debreu securities with the S\&P 500 index options data and the DAX index options data. The proposed Bayesian bandwidth selector represents a data-driven solution to the problem of choosing bandwidths for the multivariate kernel regression involved in the nonparametric estimation of the state-price density pioneered by A\"{i}t-Sahalia and Lo [A\"{i}t-Sahalia, Y., Lo, A.W., 1998. Nonparametric estimation of state-price densities implicit in financial asset prices. The Journal of Finance, 53, 499, 547.]}},
  author = {Zhang, Xibin and Brooks, Robert D. and King, Maxwell L.},
  citeulike-article-id = {7008220},
  citeulike-linkout-0 = {http://econpapers.repec.org/RePEc:eee:econom:v:153:y:2009:i:1:p:21-32},
  journal = {Journal of Econometrics},
  keywords = {black-scholes, bootstrapping, carlo, chain, cross-validation, density, formula, markov, maturity, monte, time, to},
  number = {1},
  pages = {21--32},
  posted-at = {2010-04-12 15:09:18},
  priority = {2},
  title = {{A Bayesian approach to bandwidth selection for multivariate kernel regression with an application to state-price density estimation}},
  volume = {153},
  year = {2009}
}

@techreport{Neal93,
  author = {Neal, Radford M.},
  citeulike-article-id = {7004423},
  comment = {144pp.},
  institution = {Dept. of Computer Science, University of Toronto},
  keywords = {carlo, gibbs, markov, monte, probabilistic, sampling},
  month = sep,
  number = {CRG-TR-93-1},
  posted-at = {2010-04-12 11:49:58},
  priority = {2},
  title = {Probabilistic Inference using {M}arkov chain {M}onte {C}arlo Methods},
  year = {1993}
}

@article{Hastings70,
  abstract = {{SUMMARY A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}},
  author = {Hastings, W. K.},
  citeulike-article-id = {1015842},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/biomet/57.1.97},
  citeulike-linkout-1 = {http://biomet.oxfordjournals.org/content/57/1/97.abstract},
  citeulike-linkout-2 = {http://biomet.oxfordjournals.org/content/57/1/97.full.pdf},
  citeulike-linkout-3 = {http://biomet.oxfordjournals.org/cgi/content/abstract/57/1/97},
  day = {01},
  issn = {1464-3510},
  journal = {Biometrika},
  keywords = {sampling},
  month = apr,
  number = {1},
  pages = {97--109},
  posted-at = {2010-04-12 11:45:27},
  priority = {2},
  publisher = {Oxford University Press},
  title = {Monte {C}arlo sampling methods using {M}arkov chains and their applications},
  volume = {57},
  year = {1970}
}

@article{Metropolis53,
  abstract = {{A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  citeulike-article-id = {531300},
  citeulike-linkout-0 = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=JCPSA6000021000006001087000001\&idtype=cvips\&gifs=yes},
  citeulike-linkout-1 = {http://link.aip.org/link/?JCP/21/1087},
  citeulike-linkout-2 = {http://dx.doi.org/10.1063/1.1699114},
  day = {01},
  issn = {0021-9606},
  journal = {The Journal of Chemical Physics},
  keywords = {sampling},
  month = jun,
  number = {6},
  pages = {1087--1092},
  posted-at = {2010-04-12 11:43:59},
  priority = {2},
  publisher = {AIP},
  title = {{Equation of state calculations by fast computing machines}},
  volume = {21},
  year = {1953}
}

@article{Gelman92,
  abstract = {{The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.}},
  author = {Gelman, Andrew and Rubin, Donald B.},
  citeulike-article-id = {1911404},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2246093},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2246093},
  issn = {08834237},
  journal = {Statistical Science},
  keywords = {sampling},
  number = {4},
  pages = {457--472},
  posted-at = {2010-04-12 11:40:09},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Inference from iterative simulation using multiple sequences}},
  volume = {7},
  year = {1992}
}

@article{Kass98,
  abstract = {{Markov chain Monte Carlo (MCMC) methods make possible the use of flexible Bayesian models that would otherwise be computationally infeasible. In recent years, a great variety of such applications have been described in the literature. Applied statisticians who are new to these methods may have several questions and concerns, however: How much effort and expertise are needed to design and use a Markov chain sampler? How much confidence can one have in the answers that MCMC produces? How does the use of MCMC affect the rest of the model-building process? At the Joint Statistical Meetings in August, 1996, a panel of experienced MCMC users discussed these and other issues, as well as various "tricks of the trade". This article is an edited recreation of that discussion. Its purpose is to offer advice and guidance to novice users of MCMC-and to not-so-novice users as well. Topics include building confidence in simulation results, methods for speeding and assessing convergence, estimating standard errors, identification of models for which good MCMC algorithms exist, and the current state of software development.}},
  author = {Kass, Robert E. and Carlin, Bradley P. and Gelman, Andrew and Neal, Radford M.},
  citeulike-article-id = {828996},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2685466},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2685466},
  issn = {00031305},
  journal = {The American Statistician},
  keywords = {sampling},
  number = {2},
  pages = {93--100},
  posted-at = {2010-04-12 11:38:53},
  priority = {2},
  publisher = {American Statistical Association},
  title = {Markov chain {M}onte {C}arlo in practice: A roundtable discussion},
  volume = {52},
  year = {1998}
}

@article{Zychaluk08,
  author = {\.{Z}ychaluk, Kamila and Patil, Prakash},
  citeulike-article-id = {7004334},
  citeulike-linkout-0 = {http://econpapers.repec.org/RePEc:spr:aistmt:v:60:y:2008:i:1:p:21-44},
  journal = {Annals of the Institute of Statistical Mathematics},
  keywords = {cross-validation, density, estimator, kernel, ties},
  number = {1},
  pages = {21--44},
  posted-at = {2010-04-12 11:27:19},
  priority = {2},
  title = {{A cross-validation method for data with ties in kernel density estimation}},
  volume = {60},
  year = {2008}
}

@book{Cover91,
  abstract = {{Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.}},
  author = {Cover, T. M. and Thomas, Joy A.},
  citeulike-article-id = {308696},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0471062596},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0471062596},
  citeulike-linkout-10 = {http://www.worldcat.org/oclc/22305429},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0471062596},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0471062596},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0471062596/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0471062596},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0471062596},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0471062596},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0471062596\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0471062596},
  day = {26},
  edition = {99},
  howpublished = {Hardcover},
  isbn = {0471062596},
  keywords = {information\_theory},
  month = aug,
  posted-at = {2009-12-18 14:14:45},
  priority = {2},
  publisher = {Wiley},
  title = {{Elements of information theory}},
  year = {1991}
}

@article{Hall87,
  author = {Hall, Peter and Watson, G. S. and Cabrera, Javier},
  citeulike-article-id = {6368455},
  journal = {Biometrika},
  keywords = {density},
  number = {4},
  pages = {751--762},
  posted-at = {2009-12-12 16:45:58},
  priority = {2},
  publisher = {Biometrika Trust},
  title = {{Kernel Density Estimation with Spherical Data}},
  volume = {74},
  year = {1987}
}

@inproceedings{Attias99,
  author = {Attias, H.},
  booktitle = {Proc. 15th Conf. on Uncertainty in Artificial Intelligence},
  citeulike-article-id = {6080990},
  keywords = {bayesian},
  number = {1},
  posted-at = {2009-11-06 14:29:32},
  priority = {2},
  title = {{Inferring parameters and structure of latent variable models by variational Bayes}},
  volume = {2},
  year = {1999}
}

@article{Brewer00,
  address = {Hingham, MA, USA},
  author = {Brewer, Mark J.},
  citeulike-article-id = {6080223},
  journal = {Statistics and Computing},
  keywords = {density},
  number = {4},
  pages = {299--309},
  posted-at = {2009-11-06 09:00:23},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {A {B}ayesian model for local smoothing in kernel density estimation},
  volume = {10},
  year = {2000}
}

@article{Opper00,
  author = {Opper, M. and Winther, O.},
  citeulike-article-id = {6074086},
  journal = {Neural Computation},
  keywords = {gaussian-processes},
  number = {11},
  pages = {2655--2684},
  posted-at = {2009-11-05 12:26:53},
  priority = {2},
  publisher = {MIT Press},
  title = {{Gaussian processes for classification: Mean-field algorithms}},
  volume = {12},
  year = {2000}
}

@article{Marron92,
  author = {Marron, J. S. and Wand, M. P.},
  citeulike-article-id = {6070018},
  journal = {The Annals of Statistics},
  keywords = {density},
  number = {2},
  pages = {712--736},
  posted-at = {2009-11-04 16:13:19},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Exact mean integrated squared error}},
  volume = {20},
  year = {1992}
}

@article{Duong05,
  author = {Duong, T. and Hazelton, M. L.},
  citeulike-article-id = {6068162},
  journal = {Scandinavian Journal of Statistics Theory and Applications},
  keywords = {density},
  number = {3},
  pages = {485--506},
  posted-at = {2009-11-04 14:21:51},
  priority = {2},
  publisher = {Stockholm, Almqvist \& Wiksell Periodical Co.},
  title = {{Cross-validation bandwidth matrices for multivariate kernel density estimation}},
  volume = {32},
  year = {2005}
}

@article{Zhang06,
  author = {Zhang, X. and King, M. L. and Hyndman, R. J.},
  citeulike-article-id = {6068151},
  journal = {Computational Statistics and Data Analysis},
  keywords = {bayesian},
  number = {11},
  pages = {3009--3031},
  posted-at = {2009-11-04 14:20:26},
  priority = {2},
  publisher = {Elsevier},
  title = {{A Bayesian approach to bandwidth selection for multivariate kernel density estimation}},
  volume = {50},
  year = {2006}
}

@article{Jaakkola00,
  author = {Jaakkola, T. S.},
  citeulike-article-id = {5973486},
  journal = {Advanced Mean Field Methods: Theory and Practice},
  keywords = {bayesian},
  pages = {129--159},
  posted-at = {2009-10-20 10:51:27},
  priority = {2},
  publisher = {Citeseer},
  title = {{Tutorial on Variational Approximation Methods}},
  year = {2000}
}

@article{Chiu91,
  author = {Chiu, S. T.},
  citeulike-article-id = {5973368},
  journal = {The Annals of Statistics},
  keywords = {density},
  number = {4},
  pages = {1883--1905},
  posted-at = {2009-10-20 09:40:00},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Bandwidth selection for kernel density estimation}},
  volume = {19},
  year = {1991}
}

@article{Terrell90,
  author = {Terrell, G. R.},
  citeulike-article-id = {5973324},
  journal = {Journal of the American Statistical Association},
  keywords = {density},
  number = {410},
  pages = {470--477},
  posted-at = {2009-10-20 09:10:24},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{The maximal smoothing principle in density estimation}},
  volume = {85},
  year = {1990}
}

@article{Hall82,
  author = {Hall, P.},
  citeulike-article-id = {5973310},
  journal = {Biometrika},
  keywords = {density},
  number = {2},
  pages = {383--390},
  posted-at = {2009-10-20 09:00:06},
  priority = {2},
  publisher = {Biometrika Trust},
  title = {{Cross-validation in density estimation}},
  volume = {69},
  year = {1982}
}

@article{Loader99,
  author = {Loader, C. R.},
  citeulike-article-id = {5973302},
  journal = {The Annals of Statistics},
  keywords = {density},
  number = {2},
  pages = {415--438},
  posted-at = {2009-10-20 08:55:02},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  title = {{Bandwidth selection: classical or plug-in?}},
  volume = {27},
  year = {1999}
}

@article{Jones96,
  author = {Jones, M. C. and Marron, J. S. and Sheather, S. J.},
  citeulike-article-id = {5973293},
  journal = {Journal of the American Statistical Association},
  keywords = {density},
  number = {433},
  pages = {401--407},
  posted-at = {2009-10-20 08:47:20},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{A brief survey of bandwidth selection for density estimation}},
  volume = {91},
  year = {1996}
}

@article{Sheather91,
  author = {Sheather, S. J. and Jones, M. C.},
  citeulike-article-id = {5973289},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  keywords = {density},
  number = {3},
  pages = {683--690},
  posted-at = {2009-10-20 08:44:33},
  priority = {2},
  publisher = {Blackwell Publishers},
  title = {{A reliable data-based bandwidth selection method for kernel density estimation}},
  volume = {53},
  year = {1991}
}

@article{Barni96,
  author = {Barni, M. and Cappellini, V. and Mecocci, A.},
  citeulike-article-id = {5865784},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {clustering},
  number = {3},
  pages = {393--396},
  posted-at = {2009-10-01 08:20:05},
  priority = {2},
  title = {{Comments on  ” a possibilistic approach to clustering”}},
  volume = {4},
  year = {1996}
}

@misc{ChangXX,
  abstract = {{LIBSVM is a library for support vector machines (SVM). Its goal is to help users can easily use SVM as a tool. In this document, we present all its implementation details. 1}},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  citeulike-article-id = {169847},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.9020},
  keywords = {classification},
  posted-at = {2009-09-30 09:30:11},
  priority = {2},
  title = {{LIBSVM: a Library for Support Vector Machines (Version 2.31)}}
}

@book{Brockwell02,
  author = {Brockwell, Peter J. and Davis, Richard A.},
  citeulike-article-id = {191329},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387953515},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0387953515},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0387953515},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0387953515},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0387953515/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387953515},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0387953515},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0387953515},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0387953515\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0387953515},
  day = {08},
  edition = {2nd},
  howpublished = {Hardcover},
  isbn = {0387953515},
  keywords = {timeseries},
  month = mar,
  posted-at = {2009-07-22 10:31:25},
  priority = {2},
  publisher = {Springer},
  title = {{Introduction to Time Series and Forecasting}},
  year = {2002}
}

@article{Akaike74,
  abstract = {{The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.}},
  author = {Akaike, H.},
  citeulike-article-id = {849862},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/tac.1974.1100705},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1100705},
  day = {06},
  institution = {Institute of Statistical Mathematics, Minato-ku, Tokyo, Japan},
  issn = {0018-9286},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {statistics},
  month = dec,
  number = {6},
  pages = {716--723},
  posted-at = {2009-07-20 15:56:14},
  priority = {2},
  publisher = {IEEE},
  title = {{A new look at the statistical model identification}},
  volume = {19},
  year = {1974}
}

@article{Jefferson30,
  author = {Jefferson, Mark},
  citeulike-article-id = {5207234},
  citeulike-linkout-0 = {http://www.jstor.org/stable/209130},
  journal = {Geographical Review},
  keywords = {novelty\_detection},
  number = {1},
  pages = {133--137},
  posted-at = {2009-07-20 12:26:32},
  priority = {2},
  publisher = {American Geographical Society},
  title = {{Variation in Lake Huron Levels and the Chicago Drainage Canal}},
  volume = {20},
  year = {1930}
}

@article{Hau89,
  author = {Hau, M. C. and Tong, H.},
  citeulike-article-id = {4894715},
  journal = {Stochastic Environmental Research and Risk Assessment (SERRA)},
  keywords = {autoregressive},
  number = {4},
  pages = {241--260},
  posted-at = {2009-06-18 18:17:13},
  priority = {2},
  publisher = {Springer},
  title = {{A practical method for outlier detection in autoregressive time series modelling}},
  volume = {3},
  year = {1989}
}

@inproceedings{Lee08,
  author = {Lee, H. and Roberts, S. J.},
  booktitle = {Pattern Recognition, 2008. ICPR 2008. 19th International Conference on},
  citeulike-article-id = {4894709},
  keywords = {novelty\_detection},
  pages = {1--4},
  posted-at = {2009-06-18 18:16:24},
  priority = {2},
  title = {{On-line Novelty Detection Using the Kalman Filter and Extreme Value Theory}},
  year = {2008}
}

@inproceedings{Penny00b,
  author = {Penny, W. D. and Roberts, S. J.},
  booktitle = {Neural Networks for Signal Processing X, 2000. Proceedings of the 2000 IEEE Signal Processing Society Workshop},
  citeulike-article-id = {4894702},
  keywords = {autoregressive},
  posted-at = {2009-06-18 18:15:30},
  priority = {2},
  title = {{Bayesian methods for autoregressive models}},
  volume = {1},
  year = {2000}
}

@inproceedings{Penny00,
  author = {Penny, W. D. and Roberts, S. J.},
  booktitle = {Neural Networks for Signal Processing X, 2000. Proceedings of the 2000 IEEE Signal Processing Society Workshop},
  citeulike-article-id = {4894696},
  keywords = {autoregressive},
  posted-at = {2009-06-18 18:14:35},
  priority = {2},
  title = {{Variational Bayes for non-Gaussian autoregressive models}},
  volume = {1},
  year = {2000}
}

@article{Choy01,
  author = {Choy, K.},
  citeulike-article-id = {4894690},
  journal = {Journal of Statistical Planning and Inference},
  keywords = {novelty\_detection},
  number = {2},
  pages = {111--127},
  posted-at = {2009-06-18 18:13:46},
  priority = {2},
  publisher = {Elsevier},
  title = {{Outlier detection for stationary time series}},
  volume = {99},
  year = {2001}
}

@article{Monahan83,
  author = {Monahan, J. F.},
  citeulike-article-id = {4894641},
  journal = {Journal of Econometrics},
  keywords = {autoregressive},
  number = {3},
  pages = {307--31},
  posted-at = {2009-06-18 18:07:34},
  priority = {2},
  title = {{Fully Bayesian analysis of ARMA time series models}},
  volume = {21},
  year = {1983}
}

@inproceedings{Roberts00,
  author = {Roberts, S. J.},
  booktitle = {Advances in Medical Signal and Information Processing, 2000. First International Conference on (IEE Conf. Publ. No. 476)},
  citeulike-article-id = {4894623},
  keywords = {novelty\_detection},
  pages = {166--172},
  posted-at = {2009-06-18 18:05:40},
  priority = {2},
  title = {{Extreme value statistics for novelty detection in biomedical signalprocessing}},
  year = {2000}
}

@article{McQuarrie03,
  author = {Mc{q}uarrie, A. D. and Tsai, C. L.},
  citeulike-article-id = {4894139},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {autoregressive},
  number = {2},
  pages = {450--471},
  posted-at = {2009-06-18 16:55:41},
  priority = {2},
  publisher = {ASA},
  title = {{Outlier detections in autoregressive models}},
  volume = {12},
  year = {2003}
}

@article{Justel01,
  author = {Justel, A. and Pena, D. and Tsay, R. S.},
  citeulike-article-id = {4894126},
  journal = {Statistica Sinica},
  keywords = {autoregressive},
  number = {3},
  pages = {651--674},
  posted-at = {2009-06-18 16:53:53},
  priority = {2},
  publisher = {Citeseer},
  title = {{Detection of outlier patches in autoregressive time series}},
  volume = {11},
  year = {2001}
}

@article{Louini08,
  author = {Louni, Hamid},
  citeulike-article-id = {4894083},
  citeulike-linkout-0 = {http://ideas.repec.org/a/bla/jtsera/v29y2008i6p1057-1065.html},
  journal = {Journal of Time Series Analysis},
  keywords = {autoregressive},
  month = nov,
  number = {6},
  pages = {1057--1065},
  posted-at = {2009-06-18 16:48:45},
  priority = {2},
  title = {{Outlier detection in ARMA models}},
  volume = {29},
  year = {2008}
}

@article{Zhan05,
  abstract = {{Non-parametric time–frequency techniques are increasingly developed and employed to process non-stationary vibration signals of rotating machinery in a great deal of condition monitoring literature. However, their capacity to reveal power variations in the time–frequency space as precisely as possible becomes a hard constraint when the aim is that of monitoring the occurrence of mechanical faults. Therefore, for an early diagnosis, it is imperative to utilize methods with high temporal resolution, aiming at detecting spectral variations occurring in a very short time. This paper proposes three new adaptive parametric models transformed from time-varying vector-autoregressive model with their parameters estimated by means of noise-adaptive Kalman filter, extended Kalman filter and modified extended Kalman filter, respectively, on the basis of different assumptions. The performance analysis of the proposed adaptive parametric models is demonstrated using numerically generated non-stationary test signals. The results suggest that the proposed models possess appealing advantages in processing non-stationary signals and thus are able to provide reliable time–frequency domain information for condition monitoring.}},
  author = {Zhan, Y. and Jardine, A.},
  citeulike-article-id = {4894061},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.jsv.2004.10.024},
  day = {06},
  issn = {0022460X},
  journal = {Journal of Sound and Vibration},
  keywords = {autoregressive},
  month = sep,
  number = {3},
  pages = {429--450},
  posted-at = {2009-06-18 16:46:55},
  priority = {2},
  title = {{Adaptive autoregressive modeling of non-stationary vibration signals under distinct gear states. Part 1: modeling}},
  volume = {286},
  year = {2005}
}

@article{Broersen93,
  author = {Broersen, P. M. and Wensink, H. E.},
  citeulike-article-id = {4894026},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {autoregressive},
  number = {1},
  posted-at = {2009-06-18 16:42:56},
  priority = {2},
  title = {{On finite sample theory for autoregressive model order selection}},
  volume = {41},
  year = {1993}
}

@inproceedings{Dijkhof00,
  address = {Tampere, Finland},
  author = {Dijkhof, Wilbert and Einar, Wensink},
  booktitle = {10th European Signal Processing Conference ({EUSIPCO})},
  citeulike-article-id = {4893943},
  keywords = {autoregressive},
  month = sep,
  posted-at = {2009-06-18 16:32:57},
  priority = {2},
  title = {Small sample statistics of the {Y}ule-{W}alker method for autoregressive parameter estimation},
  year = {2000}
}

@inproceedings{Keogh02,
  address = {New York, NY, USA},
  author = {Keogh, Eamonn and Lonardi, Stefano and Bill},
  booktitle = {KDD '02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  citeulike-article-id = {4884672},
  keywords = {novelty\_detection},
  location = {Edmonton, Alberta, Canada},
  pages = {550--556},
  posted-at = {2009-06-17 22:53:30},
  priority = {2},
  publisher = {ACM},
  title = {{Finding surprising patterns in a time series database in linear time and space}},
  year = {2002}
}

@inproceedings{Dasgupta95,
  author = {Dasgupta, Dipankar and Forrest, Stephanie},
  booktitle = {In Proceedings of The International Conference on Intelligent Systems},
  citeulike-article-id = {4884641},
  keywords = {novelty\_detection},
  posted-at = {2009-06-17 22:49:59},
  priority = {2},
  title = {{Novelty Detection in Time Series Data using Ideas from Immunology}},
  year = {1995}
}

@techreport{Adams07,
  address = {Cambridge, UK},
  author = {Adams, Ryan P. and Mackay, David J. C.},
  citeulike-article-id = {4884618},
  comment = {arXiv:0710.3742v1 [stat.ML]},
  institution = {University of Cambridge},
  keywords = {file-import-09-06-17},
  posted-at = {2009-06-17 22:47:26},
  priority = {2},
  title = {{B}ayesian Online Changepoint Detection},
  year = {2007}
}

@inproceedings{Ma03b,
  abstract = {{Time-series novelty detection, or anomaly detection, refers to the automatic identification of novel or abnormal events embedded in normal time-series points. Although it is a challenging topic in data mining, it has been acquiring increasing attention due to its huge potential for immediate applications. In this paper, a new algorithm for time-series novelty detection based on one-class support vector machines (SVMs) is proposed. The concepts of phase and projected phase spaces are first introduced, which allows us to convert a time-series into a set of vectors in the (projected) phase spaces. Then we interpret novel events in time-series as outliers of the "normal" distribution of the converted vectors in the (projected) phase spaces. One-class SVMs are employed as the outlier detectors. In order to obtain robust detection results, a technique to combine intermediate results at different phase spaces is also proposed. Experiments on both synthetic and measured data are presented to demonstrate the promising performance of the new algorithm.}},
  author = {Ma, Junshui and Perkins, Simon},
  booktitle = {Neural Networks, 2003. Proceedings of the International Joint Conference on},
  citeulike-article-id = {1157642},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/ijcnn.2003.1223670},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1223670},
  institution = {Space \& Remote Sensing Sci., Los Alamos Nat. Lab., NM, USA},
  isbn = {0-7803-7898-9},
  issn = {1098-7576},
  journal = {Neural Networks, 2003. Proceedings of the International Joint Conference on},
  keywords = {novelty\_detection},
  month = jul,
  pages = {1741--1745 vol.3},
  posted-at = {2009-06-17 22:43:36},
  priority = {2},
  publisher = {IEEE},
  title = {{Time-series novelty detection using one-class support vector machines}},
  volume = {3},
  year = {2003}
}

@inproceedings{Kawahara09,
  author = {Kawahara, Yoshinobu and Sugiyama, Masashi},
  booktitle = {Proceedings of the SIAM International Conference on Data Mining, SDM 2009, April 30 - May 2, 2009, Sparks, Nevada, USA},
  citeulike-article-id = {4884563},
  keywords = {novelty\_detection},
  pages = {389--400},
  posted-at = {2009-06-17 22:40:24},
  priority = {2},
  publisher = {SIAM},
  title = {{Change-Point Detection in Time-Series Data by Direct Density-Ratio Estimation}},
  year = {2009}
}

@incollection{Archer04,
  address = {Cambridge, MA},
  author = {Archer, Cynthia and Leen, Todd K. and Baptista, Antonio},
  booktitle = {Advances in Neural Information Processing Systems 16},
  citeulike-article-id = {4884530},
  editor = {Thrun, Sebastian and Saul, Lawrence and {sch\"{o}lkopf}, Bernhard},
  keywords = {anomaly, degradation, detection, estimation, fault, likelihood, maximum, novelty, novelty\_detection, online, sensor},
  posted-at = {2009-06-17 22:36:40},
  priority = {2},
  publisher = {MIT Press},
  title = {{Parameterized Novelty Detectors for Environmental Sensor Monitoring}},
  year = {2004}
}

@inproceedings{Ma03,
  address = {New York, NY, USA},
  author = {Ma, Junshui and Perkins, Simon},
  booktitle = {KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining},
  citeulike-article-id = {4884507},
  keywords = {novelty\_detection},
  location = {Washington, D.C.},
  pages = {613--618},
  posted-at = {2009-06-17 22:32:59},
  priority = {2},
  publisher = {ACM},
  title = {{Online novelty detection on temporal sequences}},
  year = {2003}
}

@inproceedings{Hershey07,
  abstract = {{The Kullback Leibler (KL) divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaussian mixture models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments}},
  author = {Hershey, John R. and Olsen, Peder A.},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing, 2007. ICASSP 2007.},
  citeulike-article-id = {1813428},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/icassp.2007.366913},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4218101},
  isbn = {1-4244-0727-3},
  journal = {IEEE International Conference on Acoustics, Speech and Signal Processing, 2007. ICASSP 2007.},
  keywords = {information\_theory},
  location = {Honolulu, HI, USA},
  pages = {IV-317--IV-320},
  posted-at = {2009-05-07 12:43:53},
  priority = {2},
  publisher = {IEEE},
  title = {{Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models}},
  volume = {4},
  year = {2007}
}

@article{Scholkopf99,
  author = {Sch{\"{o}}lkopf, Bernhard and Mika, Sebastian and Burges, Christopher J. C. and Knirsch, Phil and M{\"{u}}ller, Klaus R. and R{\"{a}}tsch, Gunnar and Smola, Alexander J.},
  citeulike-article-id = {4270019},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {kernel},
  number = {5},
  pages = {1000--1017},
  posted-at = {2009-04-03 15:28:12},
  priority = {2},
  title = {{Input space versus feature space in kernel-based methods}},
  volume = {10},
  year = {1999}
}

@article{FilipponeIJAR09,
  author = {Filippone, Maurizio},
  citeulike-article-id = {4270009},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.ijar.2008.08.006},
  journal = {International Journal of Approximate Reasoning},
  keywords = {clustering},
  month = feb,
  number = {2},
  pages = {363--384},
  posted-at = {2009-04-03 15:26:01},
  priority = {2},
  title = {{Dealing with Non-Metric Dissimilarities in Fuzzy Central Clustering Algorithms}},
  volume = {50},
  year = {2009}
}

@book{Silverman86,
  abstract = {{Although there has been a surge of interest in density estimation in recent years, much of the published research has been concerned with purely technical matters with insufficient emphasis given to the technique's practical value. Furthermore, the subject has been rather inaccessible to the general statistician.The account presented in this book places emphasis on topics of methodological importance, in the hope that this will facilitate broader practical application of density estimation and also encourage research into relevant theoretical work. The book also provides an introduction to the subject for those with general interests in statistics. The important role of density estimation as a graphical technique is reflected by the inclusion of more than 50 graphs and figures throughout the text.Several contexts in which density estimation can be used are discussed, including the exploration and presentation of data, nonparametric discriminant analysis, cluster analysis, simulation and the bootstrap, bump hunting, projection pursuit, and the estimation of hazard rates and other quantities that depend on the density. This book includes general survey of methods available for density estimation. The Kernel method, both for univariate and multivariate data, is discussed in detail, with particular emphasis on ways of deciding how much to smooth and on computation aspects. Attention is also given to adaptive methods, which smooth to a greater degree in the tails of the distribution, and to methods based on the idea of penalized likelihood.}},
  author = {Silverman, Bernard W.},
  citeulike-article-id = {300228},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0412246201},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0412246201},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0412246201},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0412246201},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0412246201/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0412246201},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0412246201},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0412246201},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0412246201\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0412246201},
  day = {01},
  edition = {1},
  howpublished = {Hardcover},
  isbn = {0412246201},
  keywords = {density},
  month = apr,
  posted-at = {2009-04-03 15:17:38},
  priority = {2},
  publisher = {Chapman and Hall/CRC},
  title = {{Density Estimation for Statistics and Data Analysis (Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability)}},
  year = {1986}
}

@inproceedings{Tarassenko95,
  abstract = {{Breast cancer is the major cause of death amongst women in the 35 to 55 age group. Mammography is the only feasible imaging modality for screening large numbers of women. With the present screening policy, there are three million mammograms to be analysed each year in the UK; there is therefore a need (as yet unmet) for an automated analysis system which could highlight areas of interest. In the first instance, the areas of interest might simply be any mass-like structures and this is indeed the approach reported on in this paper. Mammography is typical of many problems in medicine: the class of real interest is under-represented in the database of available examples and hence its prior probability will be very low. As a result of this, there are very few examples of abnormalities in any of the existing databases. If a neural network classifier is trained using the standard approach of minimising the mean-squared error (MSE) at the output, the under-represented class will be ignored. We have been exploring an alternative approach in which we attempt to learn a description of normality using the large number of available mammograms which do not show any evidence of mass-like structures. The idea is then to test for novelty against this description in order to try and identify candidate masses in previously unseen images analysis and interpretation and present a sample of the results which we have so far obtained on a standard database}},
  author = {Tarassenko, L. and Hayton, P. and Cerneaz, N. and Brady, M.},
  booktitle = {Fourth International Conference on Artificial Neural Networks},
  citeulike-article-id = {1113191},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=497860},
  journal = {Artificial Neural Networks, 1995., Fourth International Conference on},
  keywords = {novelty\_detection},
  pages = {442--447},
  posted-at = {2009-02-25 10:32:15},
  priority = {2},
  title = {{Novelty detection for the identification of masses in mammograms}},
  year = {1995}
}

@techreport{FilipponeTECH_REP_INFO_THEO09,
  author = {Filippone, Maurizio and Sanguinetti, Guido},
  citeulike-article-id = {4097096},
  institution = {Department of Computer Science, University of Sheffield},
  keywords = {novelty\_detection},
  month = feb,
  number = {CS-09-02},
  posted-at = {2009-02-25 09:31:01},
  priority = {2},
  title = {{Information Theoretic Novelty Detection}},
  year = {2009}
}

@inproceedings{Cox82,
  author = {Cox, L. H. and Johnson, M. M. and Kafadar, K.},
  booktitle = {ASA Proceedings of the Statistical Computation Section},
  citeulike-article-id = {4064542},
  keywords = {signal},
  posted-at = {2009-02-17 11:40:02},
  priority = {2},
  title = {{Exposition of statistical graphics technology}},
  year = {1982}
}

@article{Scholkopf01b,
  author = {Sch\"{o}lkopf, Bernhard and Platt, John C. and Taylor, John S. and Smola, Alex J. and Williamson, Robert C.},
  citeulike-article-id = {4028440},
  journal = {Neural Computation},
  keywords = {novelty\_detection},
  number = {7},
  pages = {1443--1471},
  posted-at = {2009-02-10 10:17:14},
  priority = {2},
  title = {{Estimating the Support of a High-Dimensional Distribution}},
  volume = {13},
  year = {2001}
}

@article{He04,
  author = {He, Chao and Girolami, Mark and Ross, Gary},
  citeulike-article-id = {4015133},
  journal = {Pattern Recognition},
  keywords = {novelty\_detection},
  number = {6},
  pages = {1085--1096},
  posted-at = {2009-02-06 09:50:11},
  priority = {2},
  title = {{Employing optimized combinations of one-class classifiers for automated currency validation}},
  volume = {37},
  year = {2004}
}

@inproceedings{Scholkopf00,
  author = {Sch\"{o}lkopf, Bernhard and Williamson, Robert C. and Smola, Alex J. and Taylor, John S. and Platt, John C.},
  booktitle = {Advances in Neural Information Processing Systems 12, NIPS 1999},
  citeulike-article-id = {4014160},
  citeulike-linkout-0 = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ScholkopfWSSP99},
  editor = {Solla, Sara A. and Leen, Todd K. and M\"{u}ller, Klaus R. and Solla, Sara A. and Leen, Todd K. and M\"{u}ller, Klaus R.},
  isbn = {0-262-19450-3},
  keywords = {novelty\_detection},
  location = {Denver, Colorado, USA},
  pages = {582--588},
  posted-at = {2009-02-05 23:25:33},
  priority = {2},
  publisher = {The MIT Press},
  title = {{Support Vector Method for Novelty Detection}},
  year = {1999}
}

@article{Markou03,
  author = {Markou, Markos and Singh, Sameer},
  citeulike-article-id = {4014142},
  journal = {Signal Processing},
  keywords = {novelty\_detection},
  number = {12},
  pages = {2481--2497},
  posted-at = {2009-02-05 23:12:38},
  priority = {2},
  title = {{Novelty detection: a review - part 1: statistical approaches}},
  volume = {83},
  year = {2003}
}

@inproceedings{Zhang04c,
  author = {Zhang, Jian and Ghahramani, Zoubin and Yang, Yiming},
  booktitle = {Advances in Neural Information Processing Systems 17, NIPS 2004},
  citeulike-article-id = {4014126},
  citeulike-linkout-0 = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ZhangGY04},
  day = {13-18},
  keywords = {novelty\_detection},
  month = dec,
  posted-at = {2009-02-05 22:55:20},
  priority = {2},
  title = {{A Probabilistic Model for Online Document Clustering with Application to Novelty Detection}},
  year = {2004}
}

@inproceedings{Williams05,
  author = {Williams, Christopher K. I. and Quinn, John A. and Mc{i}ntosh, Neil},
  booktitle = {Advances in Neural Information Processing Systems 18, NIPS 2005},
  citeulike-article-id = {4014110},
  day = {5-8},
  keywords = {novelty\_detection},
  location = {Vancouver, British Columbia, Canada},
  month = dec,
  posted-at = {2009-02-05 22:42:45},
  priority = {2},
  title = {{Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care}},
  year = {2005}
}

@article{Peel00,
  address = {Hingham, MA, USA},
  author = {Peel, D. and Mclachlan, G. J.},
  citeulike-article-id = {4000717},
  journal = {Statistics and Computing},
  keywords = {statistics},
  number = {4},
  pages = {339--348},
  posted-at = {2009-02-03 12:39:15},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{Robust mixture modelling using the t distribution}},
  volume = {10},
  year = {2000}
}

@inproceedings{Singer98,
  author = {Singer, Yoram and Warmuth, Manfred K.},
  booktitle = {Advances in Neural Information Processing Systems 11, NIPS 1998},
  citeulike-article-id = {4000703},
  editor = {Kearns, Michael J. and Solla, Sara A. and Cohn, David A.},
  keywords = {statistics},
  location = {Denver, Colorado, USA},
  pages = {578--584},
  posted-at = {2009-02-03 12:30:49},
  priority = {2},
  publisher = {The MIT Press},
  title = {{Batch and On-Line Parameter Estimation of Gaussian Mixtures Based on the Joint Entropy}},
  year = {1998}
}

@article{Eguchi06,
  address = {Orlando, FL, USA},
  author = {Eguchi, Shinto and Copas, John},
  citeulike-article-id = {4000683},
  journal = {Journal of Multivariate Analysis},
  keywords = {statistics},
  number = {9},
  pages = {2034--2040},
  posted-at = {2009-02-03 12:22:46},
  priority = {2},
  publisher = {Academic Press, Inc.},
  title = {Interpreting {K}ullback-{L}eibler divergence with the {N}eyman-{P}earson lemma},
  volume = {97},
  year = {2006}
}

@inproceedings{Itti05,
  author = {Itti, Laurent and Baldi, Pierre},
  booktitle = {Advances in Neural Information Processing Systems 18, NIPS 2005},
  citeulike-article-id = {4000654},
  day = {5-8},
  keywords = {information\_theory},
  location = {Vancouver, British Columbia, Canada},
  month = dec,
  posted-at = {2009-02-03 12:11:04},
  priority = {2},
  title = {{Bayesian Surprise Attracts Human Attention}},
  year = {2005}
}

@book{Anderson84,
  abstract = {{Multivariate Statistical Simulation Mark E. Johnson For the researcher instatistics, probability, and operations research involved in the design andexecution of a computer-aided simulation study utilizing continuousmultivariate distributions, this book considers the properties of suchdistributions from a unique perspective. With enhancing graphics (three-dimensional and contour plots), it presents generation algorithms revealingfeatures of the distribution undisclosed in preliminary algebraicmanipulations. Well-known multivariate distributions covered include normalmixtures, elliptically assymmetric, Johnson translation, Khintine, and Burr-Pareto-logistic. 1987 (0 471-82290-6) 230 pp. Aspects of MultivariateStatistical Theory Robb J. Muirhead A classical mathematical treatment of thetechniques, distributions, and inferences based on the multivariate normaldistributions. The main focus is on distribution theory—both exact andasymptotic. Introduces three main areas of current activity overlooked orinadequately covered in existing texts: noncentral distribution theory,decision theoretic estimation of the parameters of a multivariate normaldistribution, and the uses of spherical and elliptical distributions inmultivariate analysis. 1982 (0 471-09442-0) 673 pp. Multivariate ObservationsG. A. F. Seber This up-to-date, comprehensive sourcebook treats data-orientedtechniques and classical methods. It concerns the external analysis ofdifferences among objects, and the internal analysis of how the variablesmeasured relate to one another within objects. The scope ranges from thepractical problems of graphically representing high dimensional data to thetheoretical problems relating to matrices of random variables. 1984 (0471-88104-X) 686 pp.}},
  author = {Anderson, T. W. and Anderson, Theodore W.},
  citeulike-article-id = {3125604},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0471889873},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0471889873},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0471889873},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0471889873},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0471889873/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0471889873},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0471889873},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0471889873},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0471889873\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0471889873},
  day = {14},
  edition = {2},
  howpublished = {Hardcover},
  isbn = {0471889873},
  keywords = {statistics},
  month = sep,
  posted-at = {2009-02-03 11:30:01},
  priority = {2},
  publisher = {Wiley},
  title = {{An Introduction to Multivariate Statistical Analysis, 2nd Edition}},
  year = {1984}
}

@inproceedings{Quinn07b,
  author = {Quinn, John A. and Williams, Christopher K. I.},
  booktitle = {Pattern Recognition and Image Analysis, Third Iberian Conference, IbPRIA 2007},
  citeulike-article-id = {4000513},
  day = {6-8},
  editor = {Mart\'{i}, Joan and Bened\'{i}, Jos\'{e} M. and Mendon\c{c}a, Ana M. and Serrat, Joan},
  keywords = {novelty\_detection},
  location = {Girona, Spain},
  month = jun,
  pages = {1--6},
  posted-at = {2009-02-03 11:10:42},
  priority = {2},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {{Known Unknowns: Novelty Detection in Condition Monitoring}},
  volume = {4477},
  year = {2007}
}

@book{Barnett94,
  author = {Barnett, Vic and Lewis, Toby},
  citeulike-article-id = {3973331},
  keywords = {novelty\_detection},
  month = apr,
  posted-at = {2009-01-28 14:20:40},
  priority = {2},
  publisher = {Wiley},
  series = {Wiley Series in Probability \& Statistics},
  title = {{Outliers in Statistical Data}},
  year = {1994}
}

@article{Martinez98,
  author = {Martinez, D.},
  citeulike-article-id = {3946030},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/72.661127},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {adaptive, competitive, data, density, detection, distributions, entropy, equiprobable, estimation, histogram, learning, learningadaptive, mathematics, maximum, methods, multivariable, nets, neural, novelty, novelty\_detection, quantisation, quantization, rule, sequential, signal, structure, structures, time-varying, tree, trees, unsupervised},
  number = {2},
  pages = {330--338},
  posted-at = {2009-01-26 11:10:26},
  priority = {2},
  title = {{Neural tree density estimation for novelty detection}},
  volume = {9},
  year = {Mar 1998}
}

@inproceedings{Clifton08,
  author = {Clifton, David A. and Mcgrogan, Nicholas and Tarassenko, Lionel and King, Dennis and King, Steve and Anuzis, Paul},
  booktitle = {Proceedings of IEEE Aerospace},
  citeulike-article-id = {3946028},
  keywords = {novelty\_detection},
  posted-at = {2009-01-26 11:09:43},
  priority = {2},
  title = {{Bayesian Extreme Value Statistics for Novelty Detection in Gas-Turbine Engines}},
  year = {2008}
}

@inproceedings{Archer03,
  author = {Archer, Cynthia and Leen, Todd K. and Baptista, Ant\'{o}nio M.},
  booktitle = {Advances in Neural Information Processing Systems 16, NIPS 2003},
  citeulike-article-id = {3946022},
  day = {8-13},
  keywords = {novelty\_detection},
  location = {Vancouver and Whistler, British Columbia, Canada},
  month = dec,
  posted-at = {2009-01-26 11:07:05},
  priority = {2},
  title = {{Parameterized Novelty Detectors for Environmental Sensor Monitoring}},
  year = {2003}
}

@inproceedings{Lanckriet02,
  author = {Lanckriet, Gert R. G. and Ghaoui, Laurent E. and Jordan, Michael I.},
  booktitle = {Advances in Neural Information Processing Systems 15, NIPS 2002},
  citeulike-article-id = {3946021},
  day = {9-14},
  keywords = {novelty\_detection},
  location = {Vancouver, British Columbia, Canada},
  month = dec,
  pages = {905--912},
  posted-at = {2009-01-26 11:07:05},
  priority = {2},
  title = {{Robust Novelty Detection with Single-Class MPM}},
  year = {2002}
}

@inproceedings{Hayton00,
  author = {Hayton, Paul and Sch\"{o}lkopf, Bernhard and Tarassenko, Lionel and Anuzis, Paul},
  booktitle = {Advances in Neural Information Processing Systems 13, NIPS 2000},
  citeulike-article-id = {3946019},
  editor = {Leen, Todd K. and Dietterich, Thomas G. and Tresp, Volker},
  keywords = {novelty\_detection},
  location = {Denver, CO, USA},
  pages = {946--952},
  posted-at = {2009-01-26 11:05:43},
  priority = {2},
  publisher = {MIT Press},
  title = {{Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra}},
  year = {2000}
}

@inproceedings{Campbell00,
  author = {Campbell, Colin and Bennett, Kristin P.},
  booktitle = {Advances in Neural Information Processing Systems 13, NIPS 2000},
  citeulike-article-id = {3945734},
  editor = {Leen, Todd K. and Dietterich, Thomas G. and Tresp, Volker},
  keywords = {novelty\_detection},
  location = {Denver, CO, USA},
  pages = {395--401},
  posted-at = {2009-01-26 11:02:07},
  priority = {2},
  title = {{A Linear Programming Approach to Novelty Detection}},
  year = {2000}
}

@article{Roberts99,
  author = {Roberts, Stephen J.},
  citeulike-article-id = {3945733},
  journal = {IEE Proceedings on Vision, Image and Signal Processing},
  keywords = {novelty\_detection},
  number = {3},
  pages = {124--129},
  posted-at = {2009-01-26 11:00:59},
  priority = {2},
  title = {{Novelty Detection using Extreme Value Statistics}},
  volume = {146},
  year = {1999}
}

@article{Bishop94,
  author = {Bishop, Christopher M.},
  citeulike-article-id = {3945732},
  journal = {IEE Proceedings on Vision, Image and Signal processing},
  keywords = {novelty\_detection},
  number = {4},
  pages = {217--222},
  posted-at = {2009-01-26 10:59:37},
  priority = {2},
  title = {{Novelty detection and neural network validation}},
  volume = {141},
  year = {1994}
}

@article{Denoeux04,
  author = {Denoeux, Thierry and Masson, Marie H.},
  citeulike-article-id = {3005773},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B},
  keywords = {clustering},
  number = {1},
  pages = {95--109},
  posted-at = {2008-07-15 15:08:00},
  priority = {0},
  title = {{EVCLUS: evidential clustering of proximity data}},
  volume = {34},
  year = {2004}
}

@proceedings{Kim07b,
  abstract = {{In this paper we present a kernel method for data clustering, where the soft k-means is carried out in a feature space, instead of input data space, leading to soft kernel k-means. We also incorporate a geodesic kernel into the soft kernel k-means, in order to take the data manifold structure into account. The method is referred to as soft geodesic kernel k-means. In contrast to k-means, our method is able to identify clusters that are not linearly separable. In addition, soft responsibilities as well as geodesic kernel, improve the clustering performance, compared to kernel k-means. Numerical experiments with toy data sets and real-world data sets (UCI and document clustering), confirm the useful behavior of the proposed method}},
  author = {Kim, Jaehwan and Shim, Kwang-Hyun and Choi, Seungjin},
  booktitle = {Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on},
  citeulike-article-id = {2925578},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/icassp.2007.366264},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4217437},
  journal = {IEEE International Conference on Acoustics, Speech and Signal Processing, 2007. ICASSP 2007.},
  keywords = {clustering},
  pages = {II-429--II-432},
  posted-at = {2008-06-25 10:49:06},
  priority = {2},
  title = {{Soft Geodesic Kernel K-Means}},
  volume = {2},
  year = {2007}
}

@incollection{Fischer04b,
  address = {Cambridge, MA},
  author = {Fischer, Bernd and Roth, Volker and Buhmann, Joachim M.},
  booktitle = {Advances in Neural Information Processing Systems 16},
  citeulike-article-id = {2925570},
  editor = {Thrun, Sebastian and Saul, Lawrence and {sch\"{o}lkopf}, Bernhard},
  keywords = {clustering, computational-complexity, digit-recognition, dimensionality-reduction, energy-function, kernel-pca, unsupervised-learning},
  posted-at = {2008-06-25 10:43:38},
  priority = {2},
  publisher = {MIT Press},
  title = {{Clustering with the Connectivity Kernel}},
  year = {2004}
}

@article{Kim07,
  address = {Cambridge, MA, USA},
  author = {Kim, Hyun C. and Lee, Jaewook},
  citeulike-article-id = {2925551},
  journal = {Neural Computation},
  keywords = {clustering, gaussian-processes},
  number = {11},
  pages = {3088--3107},
  posted-at = {2008-06-25 10:30:59},
  priority = {2},
  publisher = {MIT Press},
  title = {{Clustering Based on Gaussian Processes}},
  volume = {19},
  year = {2007}
}

@book{Bishop06,
  abstract = {{The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.}},
  author = {Bishop, Christopher M.},
  citeulike-article-id = {873540},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387310738},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0387310738},
  citeulike-linkout-10 = {http://www.worldcat.org/oclc/71008143},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0387310738},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0387310738},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0387310738/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387310738},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0387310738},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0387310738},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0387310738\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0387310738},
  day = {01},
  edition = {1st ed. 2006. Corr. 2nd printing 2011},
  howpublished = {Hardcover},
  isbn = {0387310738},
  keywords = {machine-learning},
  month = aug,
  posted-at = {2008-06-25 10:21:44},
  priority = {2},
  publisher = {Springer},
  title = {{Pattern recognition and machine learning}},
  year = {2013}
}

@article{Banfield93,
  author = {Banfield, J. D. and Raftery, A. E.},
  citeulike-article-id = {2925448},
  journal = {Biometrics},
  keywords = {clustering, imported},
  pages = {803--821},
  posted-at = {2008-06-25 10:20:29},
  priority = {2},
  title = {Model-based {Gaussian} and non-{Gaussian} clustering},
  volume = {49},
  year = {1993}
}

@article{Raftery06,
  author = {Raftery, Adrian E. and Dean, Nema},
  citeulike-article-id = {510438},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/016214506000000113},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/jasa/2006/00000101/00000473/art00019},
  issn = {0162-1459},
  journal = {Journal of the American Statistical Association},
  keywords = {clustering},
  month = mar,
  number = {473},
  pages = {168--178},
  posted-at = {2008-06-25 10:17:50},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Variable Selection for Model-Based Clustering}},
  volume = {101},
  year = {2006}
}

@article{Lau07,
  author = {Lau and John, W. and Green and Peter, J.},
  citeulike-article-id = {1631419},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/106186007x238855},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/jcgs/2007/00000016/00000003/art00002},
  issn = {1061-8600},
  journal = {Journal of Computational \&\#38; Graphical Statistics},
  keywords = {clustering},
  month = sep,
  number = {3},
  pages = {526--558},
  posted-at = {2008-06-25 10:15:48},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Bayesian Model-Based Clustering Procedures}},
  volume = {16},
  year = {2007}
}

@article{ManSuk07,
  author = {Oh and Man-Suk and Raftery and Adrian, E.},
  citeulike-article-id = {1631418},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/106186007x236127},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/asa/jcgs/2007/00000016/00000003/art00003},
  issn = {1061-8600},
  journal = {Journal of Computational \& Graphical Statistics},
  keywords = {clustering},
  month = sep,
  number = {3},
  pages = {559--585},
  posted-at = {2008-06-25 10:11:49},
  priority = {2},
  publisher = {American Statistical Association},
  title = {{Model-Based Clustering With Dissimilarities}},
  volume = {16},
  year = {2007}
}

@book{MacKay03,
  abstract = {{Information theory and inference, often taught separately, are here united in
one entertaining textbook. These topics lie at the heart of many exciting
areas of contemporary science and engineering - communication, signal
processing, data mining, machine learning, pattern recognition, computational
neuroscience, bioinformatics, and cryptography. This textbook introduces
theory in tandem with applications. Information theory is taught alongside
practical communication systems, such as arithmetic coding for data
compression and sparse-graph codes for error-correction. A toolbox of
inference techniques, including message-passing algorithms, Monte Carlo
methods, and variational approximations, are developed alongside applications
of these tools to clustering, convolutional codes, independent component
analysis, and neural networks. The final part of the book describes the state
of the art in error-correcting codes, including low-density parity-check
codes, turbo codes, and digital fountain codes -- the twenty-first century
standards for satellite communications, disk drives, and data broadcast.
Richly illustrated, filled with worked examples and over 400 exercises, some
with detailed solutions, David MacKay's groundbreaking book is ideal for self-
learning and for undergraduate or graduate courses. Interludes on crosswords,
evolution, and sex provide entertainment along the way. In sum, this is a
textbook on information, communication, and coding for a new generation of
students, and an unparalleled entry point into these subjects for
professionals in areas as diverse as computational biology, financial
engineering, and machine learning.}},
  author = {Mackay, David J. C.},
  citeulike-article-id = {141092},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0521642981},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0521642981},
  citeulike-linkout-10 = {http://www.worldcat.org/oclc/52377690},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0521642981},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0521642981},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0521642981/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0521642981},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0521642981},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0521642981},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0521642981\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0521642981},
  day = {06},
  edition = {First Edition},
  howpublished = {Hardcover},
  isbn = {0521642981},
  keywords = {clustering},
  month = jun,
  posted-at = {2008-04-20 15:12:47},
  priority = {2},
  publisher = {Cambridge University Press},
  title = {{Information Theory, Inference and Learning Algorithms}},
  year = {2003}
}

@manual{R,
  address = {Vienna, Austria},
  author = {{R Development Core Team}},
  citeulike-article-id = {2396563},
  citeulike-linkout-0 = {http://www.R-project.org},
  comment = {{ISBN} 3-900051-07-0},
  keywords = {program},
  organization = {R Foundation for Statistical Computing},
  posted-at = {2008-02-18 22:35:36},
  priority = {2},
  title = {{R: A Language and Environment for Statistical Computing}},
  year = {2006}
}

@inproceedings{Fern03,
  author = {Fern, Xiaoli Z. and Brodley, Carla E.},
  booktitle = {ICML},
  citeulike-article-id = {2396543},
  editor = {Fawcett, Tom and Mishra, Nina},
  keywords = {clustering},
  pages = {186--193},
  posted-at = {2008-02-18 22:27:42},
  priority = {2},
  publisher = {AAAI Press},
  title = {{Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach}},
  year = {2003}
}

@article{Alon99,
  author = {Alon, U. and Barkai, N. and Notterman, D. A. and Gishdagger, K. and Ybarradagger, S. and Mackdagger, D. and Levine, A. J.},
  citeulike-article-id = {2396520},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  keywords = {data},
  month = jun,
  number = {12},
  pages = {6745--6750},
  posted-at = {2008-02-18 22:18:23},
  priority = {2},
  title = {{Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays}},
  volume = {96},
  year = {1999}
}

@misc{Asuncion07,
  author = {Asuncion, A. and Newman, D. J.},
  citeulike-article-id = {2396508},
  citeulike-linkout-0 = {http://www.ics.uci.edu/\$\&\#8764;\$mlearn/MLRepository.html},
  institution = {University of California, Irvine, School of Information and Computer Sciences},
  keywords = {data},
  posted-at = {2008-02-18 22:12:24},
  priority = {2},
  title = {{UCI} Machine Learning Repository},
  year = {2007}
}

@article{Platt99,
  address = {Cambridge, MA, USA},
  author = {Platt, John C.},
  booktitle = {Advances in kernel methods: support vector learning},
  citeulike-article-id = {2396463},
  keywords = {optimization},
  pages = {185--208},
  posted-at = {2008-02-18 21:46:58},
  priority = {2},
  publisher = {MIT Press},
  title = {{Fast training of support vector machines using sequential minimal optimization}},
  year = {1999}
}

@article{Roubens78,
  author = {Roubens, M.},
  citeulike-article-id = {2326507},
  journal = {Fuzzy Sets and Systems},
  keywords = {clustering},
  month = oct,
  number = {4},
  pages = {239--253},
  posted-at = {2008-02-03 16:19:03},
  priority = {2},
  title = {{Pattern classification problems and fuzzy sets}},
  volume = {1},
  year = {1978}
}

@article{Diday71,
  author = {Diday, E.},
  citeulike-article-id = {2326493},
  journal = {Revue de Stat Appliqu\'ee},
  keywords = {clustering},
  number = {2},
  pages = {19--34},
  posted-at = {2008-02-03 16:10:16},
  priority = {2},
  title = {{La m\'ethode des nu\'ees dynamiques}},
  volume = {19},
  year = {1971}
}

@article{Lange04,
  address = {Cambridge, MA, USA},
  author = {Lange, Tilman and Roth, Volker and Braun, Mikio L. and Buhmann, Joachim M.},
  citeulike-article-id = {2250724},
  journal = {Neural Computation},
  keywords = {clustering},
  number = {6},
  pages = {1299--1323},
  posted-at = {2008-01-18 11:45:54},
  priority = {0},
  publisher = {MIT Press},
  title = {{Stability-based validation of clustering solutions}},
  volume = {16},
  year = {2004}
}

@article{Windham85,
  author = {Windham, Michael},
  citeulike-article-id = {2250278},
  comment = {available at http://ideas.repec.org/a/spr/jclass/v2y1985i1p157-172.html},
  journal = {Journal of Classification},
  keywords = {clustering},
  month = dec,
  number = {1},
  pages = {157--172},
  posted-at = {2008-01-18 09:27:39},
  priority = {2},
  title = {{Numerical classification of proximity data with assignment measures}},
  volume = {2},
  year = {1985}
}

@incollection{Ruspini93,
  address = {San Mateo, CA},
  author = {Ruspini, E. H.},
  booktitle = {Readings in Fuzzy Sets for Intelligent Systems},
  citeulike-article-id = {2250222},
  editor = {Dubois, D. and Prade, H. and Yager, R. R.},
  keywords = {clustering},
  pages = {599--614},
  posted-at = {2008-01-18 09:12:12},
  priority = {2},
  publisher = {Kaufmann},
  title = {{Numerical Methods for Fuzzy Clustering}},
  year = {1993}
}

@article{Ng2002,
  address = {Los Alamitos, CA, USA},
  author = {Ng, Raymond T. and Han, Jiawei},
  citeulike-article-id = {2250194},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {clustering},
  number = {5},
  pages = {1003--1016},
  posted-at = {2008-01-18 09:05:02},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{CLARANS: A Method for Clustering Objects for Spatial Data Mining}},
  volume = {14},
  year = {2002}
}

@book{Kaufman90,
  author = {Kaufman, L. and Rousseeuw, P. J.},
  citeulike-article-id = {2250188},
  keywords = {clustering},
  posted-at = {2008-01-18 09:03:10},
  priority = {2},
  publisher = {John Wiley},
  title = {{Finding Groups in Data: An Introduction to Cluster Analysis}},
  year = {1990}
}

@article{Kurgan01,
  author = {Kurgan, Lukasz A. and Cios, Krzysztof J. and Tadeusiewicz, Ryszard and Ogiela, Marek R. and Goodenday, Lucy S.},
  citeulike-article-id = {2245526},
  journal = {Artificial Intelligence in Medicine},
  keywords = {classification},
  number = {2},
  pages = {149--169},
  posted-at = {2008-01-17 14:41:38},
  priority = {2},
  title = {{Knowledge discovery approach to automated cardiac SPECT diagnosis}},
  volume = {23},
  year = {2001}
}

@article{Gorman88,
  author = {Gorman, Paul R. and Sejnowski, Terrence J.},
  citeulike-article-id = {2245520},
  journal = {Neural Networks},
  keywords = {classification},
  number = {1},
  pages = {75--89},
  posted-at = {2008-01-17 14:39:43},
  priority = {2},
  title = {{Analysis of hidden units in a layered network trained to classify sonar targets}},
  volume = {1},
  year = {1988}
}

@article{Hong91,
  address = {New York, NY, USA},
  author = {Hong, Zi Q. and Yang, Jing Y.},
  citeulike-article-id = {2245492},
  journal = {Pattern Recognition},
  keywords = {classification},
  number = {4},
  pages = {317--324},
  posted-at = {2008-01-17 14:33:01},
  priority = {2},
  publisher = {Elsevier Science Inc.},
  title = {{Optimal discriminant plane for a small number of samples and design method of classifier on the plane}},
  volume = {24},
  year = {1991}
}

@article{Frey91,
  address = {Hingham, MA, USA},
  author = {Frey, Peter W. and Slate, David J.},
  citeulike-article-id = {2245480},
  journal = {Mach. Learn.},
  keywords = {classification},
  number = {2},
  pages = {161--182},
  posted-at = {2008-01-17 14:31:16},
  priority = {2},
  publisher = {Kluwer Academic Publishers},
  title = {{Letter Recognition Using Holland-Style Adaptive Classifiers}},
  volume = {6},
  year = {1991}
}

@inproceedings{Horton96,
  author = {Horton, Paul and Nakai, Kenta},
  booktitle = {Proceedings of the Fourth International Conference on Intelligent Systems for Molecular Biology},
  citeulike-article-id = {2245452},
  keywords = {classification},
  pages = {109--115},
  posted-at = {2008-01-17 14:23:58},
  priority = {2},
  publisher = {AAAI Press},
  title = {{A Probabilistic Classification System for Predicting the Cellular Localization Sites of Proteins}},
  year = {1996}
}

@article{Krishnapuram01,
  abstract = {{This paper presents new algorithms-fuzzy c-medoids (FCMdd) and robust fuzzy c-medoids (RFCMdd)-for fuzzy clustering of relational data. The objective functions are based on selecting c representative objects (medoids) from the data set in such a way that the total fuzzy dissimilarity within each cluster is minimized. A comparison of FCMdd with the well-known relational fuzzy c-means algorithm (RFCM) shows that FCMdd is more efficient. We present several applications of these algorithms to Web mining, including Web document clustering, snippet clustering, and Web access log analysis}},
  author = {Krishnapuram, R. and Joshi, A. and Nasraoui, O. and Yi, L.},
  booktitle = {IEEE Transactions on Fuzzy Systems},
  citeulike-article-id = {1815590},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=940971},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {clustering},
  number = {4},
  pages = {595--607},
  posted-at = {2007-10-24 15:34:04},
  priority = {0},
  title = {{Low-complexity fuzzy relational clustering algorithms for Web mining}},
  volume = {9},
  year = {2001}
}

@article{Fisher03,
  address = {Washington, DC, USA},
  author = {Fischer, Bernd and Buhmann, Joachim M.},
  citeulike-article-id = {1756971},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {clustering},
  number = {11},
  pages = {1411--1415},
  posted-at = {2007-10-11 20:29:18},
  priority = {0},
  publisher = {IEEE Computer Society},
  title = {{Bagging for Path-Based Clustering}},
  volume = {25},
  year = {2003}
}

@inproceedings{FilipponeWILF07,
  author = {Filippone, Maurizio and Masulli, Francesco and Rovetta, Stefano},
  booktitle = {WILF},
  citeulike-article-id = {1702891},
  comment = {(private-note)in press},
  keywords = {clustering, kernel},
  posted-at = {2007-09-27 21:43:48},
  priority = {0},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {{Possibilistic Clustering in Feature Space}},
  year = {2007}
}

@article{FilipponePR08,
  author = {Filippone, Maurizio and Camastra, Francesco and Masulli, Francesco and Rovetta, Stefano},
  citeulike-article-id = {1702888},
  journal = {Pattern Recognition},
  keywords = {clustering, kernel, spectral},
  month = jan,
  number = {1},
  pages = {176--190},
  posted-at = {2007-09-27 21:42:36},
  priority = {0},
  title = {{A Survey of Kernel and Spectral Methods for Clustering}},
  volume = {41},
  year = {2008}
}

@article{Hoppner03,
  abstract = {{In this paper, we revisit the convergence and optimization properties of fuzzy clustering algorithms, in general, and the fuzzy c-means (FCM) algorithm, in particular. Our investigation includes probabilistic and (a slightly modified implementation of) possibilistic memberships, which will be discussed under a unified view. We give a convergence proof for the axis-parallel variant of the algorithm by Gustafson and Kessel, that can be generalized to other algorithms more easily than in the usual approach. Using reformulated fuzzy clustering algorithms, we apply Banach's classical contraction principle and establish a relationship between saddle points and attractive fixed points. For the special case of FCM we derive a sufficient condition for fixed points to be attractive, allowing identification of them as (local) minima of the objective function (excluding the possibility of a saddle point).}},
  author = {H\"{o}ppner, F. and Klawonn, F.},
  booktitle = {IEEE Transactions on Fuzzy Systems},
  citeulike-article-id = {1702814},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1235994},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {clustering},
  number = {5},
  pages = {682--694},
  posted-at = {2007-09-27 21:34:25},
  priority = {0},
  title = {{A contribution to convergence theory of fuzzy c-means and derivatives}},
  volume = {11},
  year = {2003}
}

@article{Beni94,
  address = {Washington, DC, USA},
  author = {Beni, G. and Liu, X.},
  citeulike-article-id = {1682737},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {clustering},
  number = {9},
  pages = {954--960},
  posted-at = {2007-09-21 16:24:24},
  priority = {0},
  publisher = {IEEE Computer Society},
  title = {{A Least Biased Fuzzy Clustering Method}},
  volume = {16},
  year = {1994}
}

@inproceedings{Roth02,
  author = {Roth, Volker and Laub, Julian and Buhmann, Joachim M. and M{\"u}ller, Klaus R.},
  booktitle = {NIPS},
  citeulike-article-id = {1682693},
  keywords = {clustering},
  pages = {817--824},
  posted-at = {2007-09-21 16:04:33},
  priority = {0},
  title = {{Going Metric: Denoising Pairwise Data}},
  year = {2002}
}

@article{Hathaway89,
  author = {Hathaway, Richard J. and Davenport, John W. and Bezdek, James C.},
  citeulike-article-id = {1682666},
  journal = {Pattern Recognition},
  keywords = {clustering},
  number = {2},
  pages = {205--212},
  posted-at = {2007-09-21 15:52:28},
  priority = {0},
  title = {{Relational duals of the c-means clustering algorithms}},
  volume = {22},
  year = {1989}
}

@article{Laub06,
  address = {New York, NY, USA},
  author = {Laub, Julian and Roth, Volker and Buhmann, Joachim M. and M\"{u}ller, Klaus R.},
  citeulike-article-id = {1682648},
  journal = {Pattern Recognition},
  keywords = {clustering},
  number = {10},
  pages = {1815--1826},
  posted-at = {2007-09-21 15:48:23},
  priority = {0},
  publisher = {Elsevier Science Inc.},
  title = {{On the information and representation of non-Euclidean pairwise data}},
  volume = {39},
  year = {2006}
}

@article{Laub04,
  address = {Cambridge, MA, USA},
  author = {Laub, Julian and M{\"u}ller, Klaus R.},
  citeulike-article-id = {1682608},
  journal = {Journal of Machine Learning Research},
  keywords = {clustering},
  pages = {801--818},
  posted-at = {2007-09-21 15:45:21},
  priority = {2},
  publisher = {MIT Press},
  title = {{Feature Discovery in Non-Metric Pairwise Data}},
  volume = {5},
  year = {2004}
}

@article{Caceres06,
  author = {de C\'aceres, Miquel and Oliva, Francesc and Font, Xavier},
  citeulike-article-id = {1682599},
  journal = {Pattern Recognition},
  keywords = {clustering},
  number = {11},
  pages = {2010--2024},
  posted-at = {2007-09-21 15:42:32},
  priority = {0},
  title = {{On relational possibilistic clustering.}},
  volume = {39},
  year = {2006}
}

@article{Hathaway94,
  author = {Hathaway, Richard J. and Bezdek, James C.},
  citeulike-article-id = {1682579},
  journal = {Pattern Recognition},
  keywords = {clustering},
  number = {3},
  pages = {429--437},
  posted-at = {2007-09-21 15:36:20},
  priority = {0},
  title = {{Nerf c-means: Non-Euclidean relational fuzzy clustering.}},
  volume = {27},
  year = {1994}
}

@article{Roth03,
  address = {Washington, DC, USA},
  author = {Roth, Volker and Laub, Julian and Kawanabe, Motoaki and Buhmann, Joachim M.},
  citeulike-article-id = {1202380},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {clustering, kernel},
  number = {12},
  pages = {1540--1551},
  posted-at = {2007-04-02 05:05:02},
  priority = {0},
  publisher = {IEEE Computer Society},
  title = {{Optimal Cluster Preserving Embedding of Nonmetric Proximity Data}},
  volume = {25},
  year = {2003}
}

@book{Scholkopf01,
  address = {Cambridge, MA, USA},
  author = {Sch\"{o}lkopf, Bernhard and Smola, Alexander J.},
  citeulike-article-id = {1190617},
  keywords = {kernel},
  posted-at = {2007-03-28 02:31:30},
  priority = {3},
  publisher = {MIT Press},
  title = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
  year = {2001}
}

@article{Dhillon07,
  author = {Dhillon, Inderjit S. and Guan, Yuqiang and Kulis, Brian},
  citeulike-article-id = {1190080},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {clustering},
  month = nov,
  number = {11},
  pages = {1944--1957},
  posted-at = {2007-03-27 21:35:28},
  priority = {0},
  title = {{Weighted Graph Cuts without Eigenvectors A Multilevel Approach}},
  volume = {29},
  year = {2007}
}

@article{Lloyd82,
  author = {Lloyd, S.},
  citeulike-article-id = {1190072},
  comment = {(private-note)Reprinted from Bell Laboratories technical note, 1957},
  journal = {IEEE Transactions on Information Theory},
  keywords = {clustering},
  pages = {129--137},
  posted-at = {2007-03-27 21:23:51},
  priority = {2},
  title = {{Least squares quantization in PCM}},
  volume = {28},
  year = {1982}
}

@book{Ritter91,
  address = {M\"unchen, Germany},
  author = {Ritter, H. J. and Martinetz, T. M. and Schulten, K. J.},
  citeulike-article-id = {1190069},
  keywords = {machine-learning},
  posted-at = {2007-03-27 21:23:08},
  priority = {1},
  publisher = {Addison-Wesley},
  title = {{Neuronale Netze}},
  year = {1991}
}

@article{Mercer1909,
  author = {Mercer, John},
  citeulike-article-id = {1190035},
  journal = {Proceedings of the Royal Society of London},
  keywords = {kernel},
  pages = {415--446},
  posted-at = {2007-03-27 21:18:51},
  priority = {2},
  title = {{Functions of positive and negative type and their connection with the theory of integral equations}},
  volume = {209},
  year = {1909}
}

@inproceedings{Wagner93,
  author = {Wagner, Dorothea and Wagner, Frank},
  booktitle = {Mathematical Foundations of Computer Science},
  citeulike-article-id = {1190034},
  citeulike-linkout-0 = {\#},
  keywords = {spectral},
  pages = {744--750},
  posted-at = {2007-03-27 21:18:02},
  priority = {2},
  title = {{Between Min Cut and Graph Bisection}},
  year = {1993}
}

@article{Belkin03,
  abstract = {{One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.}},
  address = {Cambridge, MA, USA},
  author = {Belkin, Mikhail and Niyogi, Partha},
  citeulike-article-id = {885290},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=795528},
  citeulike-linkout-1 = {http://dx.doi.org/10.1162/089976603321780317},
  citeulike-linkout-2 = {http://www.mitpressjournals.org/doi/abs/10.1162/089976603321780317},
  day = {1},
  issn = {0899-7667},
  journal = {Neural Computation},
  keywords = {dimensionality-reduction},
  month = jun,
  number = {6},
  pages = {1373--1396},
  posted-at = {2007-03-16 15:47:13},
  priority = {2},
  publisher = {MIT Press},
  title = {{Laplacian Eigenmaps for Dimensionality Reduction and Data Representation}},
  volume = {15},
  year = {2003}
}

@article{Roweis00,
  abstract = {{Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.}},
  author = {Roweis, Sam T. and Saul, Lawrence K.},
  citeulike-article-id = {108703},
  citeulike-linkout-0 = {http://dx.doi.org/10.1126/science.290.5500.2323},
  citeulike-linkout-1 = {http://www.sciencemag.org/content/290/5500/2323.abstract},
  citeulike-linkout-2 = {http://www.sciencemag.org/content/290/5500/2323.full.pdf},
  citeulike-linkout-3 = {http://www.sciencemag.org/cgi/content/abstract/290/5500/2323},
  citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/11125150},
  citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=11125150},
  citeulike-linkout-6 = {http://www.sciencemag.org/cgi/content/full/290/5500/2323},
  day = {22},
  issn = {00368075},
  journal = {Science},
  keywords = {dimensionality-reduction},
  month = dec,
  number = {5500},
  pages = {2323--2326},
  pmid = {11125150},
  posted-at = {2007-03-16 15:46:19},
  priority = {2},
  publisher = {American Association for the Advancement of Science},
  title = {{Nonlinear Dimensionality Reduction by Locally Linear Embedding}},
  volume = {290},
  year = {2000}
}

@article{Muller01,
  author = {M\"{u}ller, Klaus R. and Mika, Sebastian and R\"{a}tsch, Gunnar and Tsuda, Koji and Sch\"{o}lkopf, Bernhard},
  citeulike-article-id = {210165},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {kernel},
  number = {2},
  pages = {181--202},
  posted-at = {2007-03-16 15:42:47},
  priority = {0},
  title = {{An introduction to kernel-based learning algorithms}},
  volume = {12},
  year = {2001}
}

@article{Masulli06,
  abstract = {{In the fuzzy clustering literature, two main types of membership are usually considered: A relative type, termed probabilistic, and an absolute or possibilistic type, indicating the strength of the attribution to any cluster independent from the rest. There are works addressing the unification of the two schemes. Here, we focus on providing a model for the transition from one schema to the other, to exploit the dual information given by the two schemes, and to add flexibility for the interpretation of results. We apply an uncertainty model based on interval values to memberships in the clustering framework, obtaining a framework that we term graded possibility. We outline a basic example of graded possibilistic clustering algorithm and add some practical remarks about its implementation. The experimental demonstrations presented highlight the different properties attainable through appropriate implementation of a suitable graded possibilistic model. An interesting application is found in automated segmentation of diagnostic medical images, where the model provides an interactive visualization tool for this task.}},
  author = {Masulli, F. and Rovetta, S.},
  citeulike-article-id = {965347},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1664887},
  journal = {Fuzzy Systems, IEEE Transactions on},
  keywords = {clustering},
  number = {4},
  pages = {516--527},
  posted-at = {2006-11-28 15:18:32},
  priority = {0},
  title = {{Soft transition from probabilistic to possibilistic fuzzy clustering}},
  volume = {14},
  year = {2006}
}

@inproceedings{MacQueen67,
  author = {Macqueen, J. B.},
  booktitle = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
  citeulike-article-id = {903715},
  keywords = {clustering},
  pages = {281--297},
  posted-at = {2006-10-18 14:32:45},
  priority = {2},
  title = {{Some methods of classification and analysis of multivariate observations}},
  year = {1967}
}

@book{Huber81,
  address = {New York},
  author = {Huber, P. J.},
  citeulike-article-id = {903600},
  keywords = {statistics},
  posted-at = {2006-10-18 11:53:40},
  priority = {2},
  publisher = {John Wiley and Sons},
  title = {{Robust Statistics}},
  year = {1981}
}

@inproceedings{Nasraoui96,
  address = {Berkeley, California},
  author = {Nasraoui, O. and Krishnapuram, R.},
  booktitle = {North American Fuzzy Information Processing Society Conference},
  citeulike-article-id = {903598},
  keywords = {clustering},
  month = jun,
  posted-at = {2006-10-18 11:50:27},
  priority = {2},
  title = {{An improved possibilistic c-means algorithm with finite rejection and robust scale estimation}},
  year = {1996}
}

@article{Kluger03,
  abstract = {{
                Global analyses of RNA expression levels are useful for classifying genes and overall phenotypes. Often these classification problems are linked, and one wants to find "marker genes" that are differentially expressed in particular sets of "conditions." We have developed a method that simultaneously clusters genes and conditions, finding distinctive "checkerboard" patterns in matrices of gene expression data, if they exist. In a cancer context, these checkerboards correspond to genes that are markedly up- or downregulated in patients with particular types of tumors. Our method, spectral biclustering, is based on the observation that checkerboard structures in matrices of expression data can be found in eigenvectors corresponding to characteristic expression patterns across genes or conditions. In addition, these eigenvectors can be readily identified by commonly used linear algebra approaches, in particular the singular value decomposition (SVD), coupled with closely integrated normalization steps. We present a number of variants of the approach, depending on whether the normalization over genes and conditions is done independently or in a coupled fashion. We then apply spectral biclustering to a selection of publicly available cancer expression data sets, and examine the degree to which the approach is able to identify checkerboard structures. Furthermore, we compare the performance of our biclustering methods against a number of reasonable benchmarks (e.g., direct application of SVD or normalized cuts to raw data).
            }},
  address = {Department of Genetics, Yale University, New Haven, Connecticut 06520, USA.},
  author = {Kluger, Y. and Basri, R. and Chang, J. T. and Gerstein, M.},
  citeulike-article-id = {79882},
  citeulike-linkout-0 = {http://dx.doi.org/10.1101/gr.648603},
  citeulike-linkout-1 = {http://genome.cshlp.org/content/13/4/703.abstract},
  citeulike-linkout-2 = {http://genome.cshlp.org/content/13/4/703.full.pdf},
  citeulike-linkout-3 = {http://www.genome.org/cgi/content/abstract/13/4/703},
  citeulike-linkout-4 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC430175/},
  citeulike-linkout-5 = {http://view.ncbi.nlm.nih.gov/pubmed/12671006},
  citeulike-linkout-6 = {http://www.hubmed.org/display.cgi?uids=12671006},
  day = {01},
  issn = {1088-9051},
  journal = {Genome Research},
  keywords = {biclustering},
  month = apr,
  number = {4},
  pages = {703--716},
  pmcid = {PMC430175},
  pmid = {12671006},
  posted-at = {2006-10-13 14:18:46},
  priority = {2},
  publisher = {Cold Spring Harbor Laboratory Press},
  title = {{Spectral biclustering of microarray data: coclustering genes and conditions.}},
  volume = {13},
  year = {2003}
}

@inproceedings{Chan93,
  address = {Cambridge, MA, USA},
  author = {Chan, Pak K. and Schlag, Martine and Zien, Jason Y.},
  booktitle = {Proceeding of the 1993 symposium on Research on integrated systems},
  citeulike-article-id = {883687},
  keywords = {spectral},
  pages = {123--142},
  posted-at = {2006-10-04 14:37:05},
  priority = {2},
  publisher = {MIT Press},
  title = {{Spectral k-way ratio-cut partitioning and clustering}},
  year = {1993}
}

@article{Kernighan70,
  author = {Kernighan, B. W. and Lin, S.},
  citeulike-article-id = {883684},
  journal = {The {B}ell system technical journal},
  keywords = {spectral},
  number = {1},
  pages = {291--307},
  posted-at = {2006-10-04 14:32:42},
  priority = {2},
  title = {{An Efficient Heuristic Procedure for Partitioning Graphs}},
  volume = {49},
  year = {1970}
}

@inproceedings{Yu03,
  address = {Washington DC, USA},
  author = {Yu, Stella X. and Shi, Jianbo},
  booktitle = {ICCV '03: Proceedings of the Ninth IEEE International Conference on Computer Vision},
  citeulike-article-id = {883599},
  keywords = {spectral},
  posted-at = {2006-10-04 11:01:53},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Multiclass Spectral Clustering}},
  year = {2003}
}

@book{Apostol61,
  author = {Apostol, T. M.},
  citeulike-article-id = {883597},
  edition = {2},
  keywords = {algebra},
  posted-at = {2006-10-04 10:57:11},
  priority = {2},
  publisher = {Wiley},
  title = {{Calculus, 2 vols}},
  year = {1967}
}

@article{Bengio04,
  author = {Bengio, Y. and Delalleau, O. and Le Roux, N. and Paiement, J. F. and Vincent, P. and Ouimet, M.},
  citeulike-article-id = {883575},
  journal = {Neural Computation},
  keywords = {spectral},
  number = {10},
  pages = {2197--2219},
  posted-at = {2006-10-04 10:42:03},
  priority = {2},
  title = {Learning eigenfunctions links spectral embedding and kernel {PCA}},
  volume = {16},
  year = {2004}
}

@article{Fisher36,
  author = {Fisher, Ronald A.},
  citeulike-article-id = {764226},
  journal = {Annals Eugenics},
  keywords = {classification},
  pages = {179--188},
  posted-at = {2006-07-19 11:00:57},
  priority = {2},
  title = {{The use of multiple measurements in taxonomic problems}},
  volume = {7},
  year = {1936}
}

@article{Wolberg90,
  author = {Wolberg, W. H. and Mangasarian, O. L.},
  citeulike-article-id = {764209},
  journal = {Proceedings of the National Academy of Sciences,U.S.A.},
  keywords = {classification},
  pages = {9193--9196},
  posted-at = {2006-07-19 10:55:27},
  priority = {2},
  title = {{Multisurface Method of Pattern Separation for Medical Diagnosis Applied to Breast Cytology}},
  volume = {87},
  year = {1990}
}

@techreport{Bach03,
  author = {Bach, Francis R. and Jordan, Michael I.},
  citeulike-article-id = {761836},
  institution = {EECS Department, University of California, Berkeley},
  keywords = {spectral},
  number = {UCB/CSD-03-1249},
  posted-at = {2006-07-17 13:11:41},
  priority = {2},
  title = {{Learning Spectral Clustering}},
  year = {2003}
}

@book{Bishop95,
  abstract = {{This book provides a solid statistical foundation for neural networks from a pattern recognition perspective. The focus is on the types of neural nets that are most widely used in practical applications, such as the multi-layer perceptron and radial basis function networks. Rather than trying to cover many different types of neural networks, Bishop thoroughly covers topics such as density estimation, error functions, parameter optimization algorithms, data pre-processing, and Bayesian methods. All topics are organized well and all mathematical foundations are explained before being applied to neural networks. The text is suitable for a graduate or advanced undergraduate level course on neural networks or for practitioners interested in applying neural networks to real-world problems. The reader is assumed to have the level of math knowledge necessary for an undergraduate science degree. This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.}},
  author = {Bishop, Christopher M.},
  citeulike-article-id = {308856},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0198538642},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0198538642},
  citeulike-linkout-10 = {http://www.worldcat.org/oclc/33101074},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0198538642},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0198538642},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0198538642/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0198538642},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0198538642},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0198538642},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0198538642\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0198538642},
  day = {18},
  edition = {1},
  howpublished = {Paperback},
  isbn = {0198538642},
  keywords = {machine-learning},
  month = jan,
  posted-at = {2006-07-17 13:03:15},
  priority = {2},
  publisher = {Oxford University Press},
  title = {{Neural networks for pattern recognition}},
  year = {1995}
}

@inproceedings{Kohonen90,
  abstract = {{Among the architectures and algorithms suggested for artificial neural networks, the Self-Organizing Map has the special property of effectively creating spatially organized "internal representations" or various features of input signals and their abstractions. One novel result is that the self-organization process can also discover semantic relationships in sentences. The Self-Organizing Map has been particularly successful in various pattern recognition tasks involving very noisy signals. In particular, these maps have been used in practical speech recognition, and work is in progress on their application to robotics, process control, telecommunications, etc. This paper contains a survey of several basic facts and results.}},
  author = {Kohonen, T.},
  booktitle = {Proceedings of the IEEE},
  citeulike-article-id = {717091},
  keywords = {clustering},
  pages = {1464--1480},
  posted-at = {2006-06-30 09:12:53},
  priority = {2},
  title = {{The Self-Organizing Map.}},
  volume = {78},
  year = {1990}
}

@article{Qinand04,
  address = {Los Alamitos, CA, USA},
  author = {Qinand, A. K. and Suganthan, P. N.},
  citeulike-article-id = {717088},
  journal = {ICPR},
  keywords = {clustering, kernel},
  pages = {617--620},
  posted-at = {2006-06-30 09:06:47},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Kernel Neural Gas Algorithms with Application to Cluster Analysis}},
  volume = {04},
  year = {2004}
}

@article{Wu03,
  address = {Los Alamitos, CA, USA},
  author = {Wu, Zhong D. and Xie, Wei X. and Yu, Jian P.},
  booktitle = {ICCIMA 2003},
  citeulike-article-id = {717086},
  journal = {Computational Intelligence and Multimedia Applications},
  keywords = {clustering, kernel},
  posted-at = {2006-06-30 09:05:08},
  priority = {2},
  publisher = {IEEE Computer Society},
  title = {{Fuzzy C-Means Clustering Algorithm Based on Kernel Method}},
  year = {2003}
}

@manual{Rcran,
  address = {Vienna, Austria},
  author = {{r}},
  citeulike-article-id = {708579},
  citeulike-linkout-0 = {http://www.R-project.org},
  comment = {(private-note){ISBN} 3-900051-07-0},
  organization = {R Foundation for Statistical Computing},
  posted-at = {2006-06-23 16:01:52},
  priority = {2},
  title = {{R: A language and environment for statistical computing}},
  year = {2005}
}

@article{Weston05,
  abstract = {{MOTIVATION: Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful. RESULTS: In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency. AVAILABILITY: Source code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider. SUPPLEMENTARY INFORMATION: www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot.}},
  address = {NEC Research Institute, 4 Independence Way, Princeton, NJ 08540, USA. jasonw@nec-labs.com},
  author = {Weston, J. and Leslie, C. and Ie, E. and Zhou, D. and Elisseeff, A. and Noble, W. S.},
  citeulike-article-id = {666065},
  citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/15905279},
  citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=15905279},
  day = {1},
  issn = {1367-4803},
  journal = {Bioinformatics},
  keywords = {kernel},
  month = aug,
  number = {15},
  pages = {3241--3247},
  pmid = {15905279},
  posted-at = {2006-06-13 10:20:00},
  priority = {2},
  title = {{Semi-supervised protein classification using cluster kernels.}},
  volume = {21},
  year = {2005}
}

@inproceedings{Paccanaro03,
  abstract = {{A major challenge in bioinformatics is the grouping together of protein sequences into functionally similar families. Large scale clustering of protein sequences may help to identify novel relationships and may also be of use in structural genomics. This paper explores the use of graph-theoretic spectral methods for clustering protein sequences. Using the leading eigenvectors of a matrix derived from similarity information between protein sequences, we were able to obtain meaningful clusters on quite diverse sets of proteins. The results presented show how this method is often able to identify correctly the superfamilies to which the sequences belong.}},
  author = {Paccanaro, A. and Chennubhotla, C. and Casbon, J. A. and Saqi, M. A. S.},
  booktitle = {International Joint Conference on Neural Networks},
  citeulike-article-id = {694514},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/ijcnn.2003.1224064},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1224064},
  day = {26},
  journal = {Neural Networks, 2003. Proceedings of the International Joint Conference on},
  keywords = {spectral},
  month = aug,
  pages = {3083--3088},
  posted-at = {2006-06-13 10:15:14},
  priority = {2},
  title = {{Spectral clustering of protein sequences}},
  volume = {4},
  year = {2003}
}

@inproceedings{Awan06,
  author = {Awan, Majid A. and Mohd},
  booktitle = {PAKDD},
  citeulike-article-id = {694503},
  editor = {Ng, Wee K. and Kitsuregawa, Masaru and Li, Jianzhong and Chang, Kuiyu},
  keywords = {kernel},
  pages = {841--846},
  posted-at = {2006-06-13 10:05:14},
  priority = {2},
  series = {Lecture Notes in Computer Science},
  title = {{An Intelligent System Based on Kernel Methods for Crop Yield Prediction.}},
  volume = {3918},
  year = {2006}
}

@article{Pothen90,
  abstract = {{An abstract is not available.}},
  address = {Philadelphia, PA, USA},
  author = {Pothen, Alex and Simon, Horst D. and Liou, Kan-Pu},
  citeulike-article-id = {674647},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=84521},
  citeulike-linkout-1 = {http://dx.doi.org/10.1137/0611030},
  issn = {0895-4798},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  keywords = {spectral},
  month = may,
  number = {3},
  pages = {430--452},
  posted-at = {2006-06-08 14:45:52},
  priority = {2},
  publisher = {Society for Industrial and Applied Mathematics},
  title = {{Partitioning sparse matrices with eigenvectors of graphs}},
  volume = {11},
  year = {1990}
}

@article{Mohar92,
  address = {Amsterdam, The Netherlands, The Netherlands},
  author = {Mohar, Bojan},
  citeulike-article-id = {689776},
  journal = {Discrete Math.},
  keywords = {spectral},
  number = {1-3},
  pages = {171--183},
  posted-at = {2006-06-08 14:43:05},
  priority = {2},
  publisher = {Elsevier Science Publishers B. V.},
  title = {{Laplace eigenvalues of graphs: a survey}},
  volume = {109},
  year = {1992}
}

@article{Fiedler73,
  author = {Fiedler, M.},
  citeulike-article-id = {689761},
  journal = {Czechoslovak Mathematical Journal},
  keywords = {graphs},
  number = {98},
  pages = {298--305},
  posted-at = {2006-06-08 14:33:32},
  priority = {2},
  title = {{Algebraic connectivity of graphs}},
  volume = {23},
  year = {1973}
}

@inproceedings{Dhillon01,
  address = {New York, NY, USA},
  author = {Dhillon, Inderjit S.},
  booktitle = {KDD '01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
  citeulike-article-id = {688546},
  keywords = {spectral},
  pages = {269--274},
  posted-at = {2006-06-07 15:08:39},
  priority = {2},
  publisher = {ACM Press},
  title = {{Co-clustering documents and words using bipartite spectral graph partitioning}},
  year = {2001}
}

@misc{Belkin02,
  abstract = {{Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to...}},
  author = {Belkin, M. and Niyogi, P.},
  citeulike-article-id = {499876},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.9400},
  keywords = {spectral},
  posted-at = {2006-06-07 15:05:18},
  priority = {2},
  title = {{Laplacian eigenmaps and spectral techniques for embedding and clustering}},
  year = {2002}
}

@inproceedings{Kulis05,
  address = {New York, NY, USA},
  author = {Kulis, Brian and Basu, Sugato and Dhillon, Inderjit and Mooney, Raymond},
  booktitle = {ICML '05: Proceedings of the 22nd international conference on Machine learning},
  citeulike-article-id = {686723},
  keywords = {spectral},
  pages = {457--464},
  posted-at = {2006-06-06 16:34:16},
  priority = {2},
  publisher = {ACM Press},
  title = {{Semi-supervised graph clustering: a kernel approach}},
  year = {2005}
}

@inproceedings{Tan04,
  author = {Tan, Xiaoyang and Chen, Songcan and Zhou, Zhi H. and Zhang, Fuyan},
  booktitle = {ISNN (1)},
  citeulike-article-id = {686721},
  keywords = {kernel},
  pages = {858--863},
  posted-at = {2006-06-06 16:31:00},
  priority = {2},
  title = {{Robust Face Recognition from a Single Training Image per Person with Kernel-Based SOM-Face.}},
  year = {2004}
}

@inproceedings{Zhang03b,
  abstract = {{The 'kernel method' has attracted great attention with the development of support vector machine (SVM) and has been studied in a general way. In this paper, we present a kernel-based fuzzy clustering algorithm that exploits the spatial contextual information in image data. The algorithm is realized by modifying the objective function in the conventional fuzzy c-means algorithm using a kernel-induced distance metric and a spatial penalty term that takes into account the influence of the neighboring pixels on the centre pixel. Experimental results on both synthetic and real MR images show that the proposed algorithm is more robust to noise than the conventional fuzzy image segmentation algorithms.}},
  author = {Zhang, Dao-Qiang and Chen, Song-Can and Pan, Zhi-Song and Tan, Ke-Ren},
  booktitle = {International Conference on Machine Learning and Cybernetics},
  citeulike-article-id = {686717},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1259869},
  journal = {Machine Learning and Cybernetics, 2003 International Conference on},
  keywords = {clustering, kernel},
  pages = {2189--2192},
  posted-at = {2006-06-06 16:20:40},
  priority = {2},
  title = {{Kernel-based fuzzy clustering incorporating spatial constraints for image segmentation}},
  volume = {4},
  year = {2003}
}

@article{Sigillito89,
  author = {Sigillito, V. G. and Wing, S. P. and Hutton, L. V. and Baker, K. B.},
  citeulike-article-id = {686659},
  journal = {Johns Hopkins APL Technical Digest},
  keywords = {machine-learning},
  pages = {262--266},
  posted-at = {2006-06-06 14:13:20},
  priority = {2},
  title = {{Classification of radar returns from the ionosphere using neural networks}},
  volume = {10},
  year = {1989}
}

@inproceedings{MacDonald00,
  author = {Macdonald, D. and Fyfe, C.},
  booktitle = {Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies, 2000},
  citeulike-article-id = {671400},
  keywords = {clustering, kernel},
  pages = {317--320},
  posted-at = {2006-05-26 14:31:14},
  priority = {2},
  title = {{The kernel self-organising map}},
  volume = {1},
  year = {2000}
}

@inproceedings{Inokuchi04,
  abstract = {{This paper aims at discussing clustering algorithm based on learning vector quantization (LVQ) using a kernel function in support vector machines. Furthermore, self-organizing map (SOM) using a kernel function is considered. Examples of clustering using different techniques are shown and effects of the kernel function are discussed.}},
  author = {Inokuchi, R. and Miyamoto, S.},
  booktitle = {Proceedings of IEEE International Conference on Fuzzy Systems},
  citeulike-article-id = {669920},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1375395},
  journal = {FUZZ-IEEE 2004 Proceedings},
  keywords = {clustering, kernel},
  pages = {1497--1500},
  posted-at = {2006-05-25 14:27:20},
  priority = {2},
  title = {{LVQ} clustering and {SOM} using a kernel function},
  volume = {3},
  year = {2004}
}

@article{Leski04,
  author = {Leski, Jacek},
  citeulike-article-id = {665511},
  journal = {Fuzzy Sets and Systems},
  keywords = {clustering, kernel},
  number = {2},
  pages = {259--280},
  posted-at = {2006-05-22 13:37:01},
  priority = {3},
  title = {{Fuzzy c-varieties/elliptotypes clustering in reproducing kernel Hilbert space.}},
  volume = {141},
  year = {2004}
}

@misc{Mitra06,
  author = {Mitra, S. and Banka, H.},
  citeulike-article-id = {637843},
  howpublished = {to appear},
  journal = {Pattern Recognition},
  keywords = {biclustering},
  posted-at = {2006-05-17 13:20:56},
  priority = {0},
  title = {{Multi-objective evolutionary biclustering of gene expression data}},
  year = {2006}
}

@article{Peete03,
  author = {Peeters, R.},
  citeulike-article-id = {637348},
  journal = {Discrete Applied Mathematics},
  keywords = {biclustering},
  pages = {651--654},
  posted-at = {2006-05-16 16:27:30},
  priority = {2},
  title = {The maximum edge biclique problem is {NP}-{C}omplete},
  volume = {131},
  year = {2003}
}

@inproceedings{Bleuler04,
  address = {Piscataway, NJ},
  author = {Bleuler, Stefan and Preli\'c, Amela and Zitzler, Eckart},
  booktitle = {Congress on Evolutionary Computation (CEC-2004)},
  citeulike-article-id = {637250},
  keywords = {biclustering},
  pages = {166--173},
  posted-at = {2006-05-16 14:25:38},
  priority = {2},
  publisher = {IEEE},
  title = {An {EA} Framework for Biclustering of Gene Expression Data},
  year = {2004}
}

@inproceedings{Cho04,
  author = {Cho, H. and Dhilon, I. S. and Guan, Y. and Sra, S.},
  booktitle = {Proceedings of 4th SIAM International Conference on Data Mining},
  citeulike-article-id = {637247},
  keywords = {biclustering},
  posted-at = {2006-05-16 14:21:53},
  priority = {2},
  title = {{Minimum sum-squared residue co-clustering of Gene expression data}},
  year = {2004}
}

@misc{Aach00,
  author = {Aach, J. and Rindone, W. and Church, G.},
  citeulike-article-id = {637236},
  citeulike-linkout-0 = {\#},
  keywords = {biclustering},
  posted-at = {2006-05-16 13:19:46},
  priority = {2},
  title = {{Systematic management and analysis of yeast gene expression data}},
  year = {2000}
}

@article{Ball00,
  author = {Ball, Catherine A. and Dolinski, Kara and Dwight, Selina S. and Harris, Midori A. and Tarver, Laurie I. and Kasarskis, Andrew and Scafe, Charles R. and Sherlock, Gavin and Binkley, Gail and Jin, Heng and Kaloper, Miroslava and Orr, Sidney D. and Schroeder, Mark and Weng, Shuai and Zhu, Yan and Botstein, David and Cherry, Michael J.},
  citeulike-article-id = {637234},
  journal = {Nucleic Acids Research},
  keywords = {biclustering},
  number = {1},
  pages = {77--80},
  posted-at = {2006-05-16 13:15:34},
  priority = {2},
  title = {{Integrating functional genomic information into the Saccharomyces Genome Database.}},
  volume = {28},
  year = {2000}
}

@article{Tavazoie99,
  abstract = {{Technologies to measure whole-genome mRNA abundances1-3 and methods to organize and display such data4-10 are emerging as valuable tools for systems-level exploration of transcriptional regulatory networks. For instance, it has been shown that mRNA data from 118 genes, measured at several time points in the developing hindbrain of mice, can be hierarchically clustered into various patterns (or 'waves') whose members tend to participate in common processes5. We have previously shown that hierarchical clustering can group together genes whose cis-regulatory elements are bound by the same proteins in vivo6. Hierarchical clustering has also been used to organize genes into hierarchical dendograms on the basis of their expression across multiple growth conditions7. The application of Fourier analysis to synchronized yeast mRNA expression data has identified cell-cycle periodic genes, many of which have expected cis-regulatory elements8. Here we apply a systematic set of statistical algorithms, based on whole-genome mRNA data, partitional clustering and motif discovery, to identify transcriptional regulatory sub-networks in yeast?without any a priori knowledge of their structure or any assumptions about their dynamics. This approach uncovered new regulons (sets of co-regulated genes) and their putative cis-regulatory elements. We used statistical characterization of known regulons and motifs to derive criteria by which we infer the biological significance of newly discovered regulons and motifs. Our approach holds promise for the rapid elucidation of genetic network architecture in sequenced organisms in which little biology is known.}},
  author = {Tavazoie, Saeed and Hughes, Jason D. and Campbell, Michael J. and Cho, Raymond J. and Church, George M.},
  citeulike-article-id = {637232},
  journal = {Nature Genetics},
  keywords = {biclustering},
  number = {3},
  posted-at = {2006-05-16 13:13:18},
  priority = {2},
  title = {{Systematic determination of genetic network architecture}},
  volume = {22},
  year = {1999}
}

@inproceedings{Cheng00,
  author = {Cheng, Yizong and Church, George M.},
  booktitle = {Proceedings of the Eighth International Conference on Intelligent Systems for Molecular Biology},
  citeulike-article-id = {635793},
  keywords = {biclustering},
  pages = {93--103},
  posted-at = {2006-05-15 16:34:37},
  priority = {2},
  publisher = {AAAI Press},
  title = {{Biclustering of Expression Data}},
  year = {2000}
}

@inproceedings{Bryan05,
  author = {Bryan, K. and Cunningham, P. and Bolshakova, N.},
  booktitle = {18th IEEE Symposium on Computer-Based Medical Systems (CBMS 2005)},
  citeulike-article-id = {635792},
  keywords = {biclustering},
  pages = {383--388},
  posted-at = {2006-05-15 16:33:03},
  priority = {2},
  title = {{Biclustering of Expression Data using Simulated Annealing}},
  year = {2005}
}

@article{Madei04,
  author = {Madeira, S. C. and Oliveira, A. L.},
  citeulike-article-id = {635791},
  journal = {IEEE Transactions on Computational Biology and Bioinformatics},
  keywords = {biclustering},
  pages = {24--45},
  posted-at = {2006-05-15 16:32:02},
  priority = {2},
  title = {Biclustering Algorithms for Biological Data Analysis: {A} Survey},
  volume = {1},
  year = {2004}
}

@inproceedings{Zhang04b,
  author = {Zhang, Z. and Teo, A. and Ooi, B. C. and {-L}},
  booktitle = {Proceedings of the Fourth IEEE Symposium on Bioinformatics and Bioengineering (BIBE'04)},
  citeulike-article-id = {635790},
  keywords = {biclustering},
  pages = {283--292},
  posted-at = {2006-05-15 16:31:12},
  priority = {2},
  title = {{Mining Deterministic Biclusters in Gene Expression Data}},
  year = {2004}
}

@article{Getz03,
  author = {Getz, G. and Gal, H. and Kela, I. and Notterman, D. A. and Domany, E.},
  citeulike-article-id = {635788},
  journal = {Bioinformatics},
  keywords = {biclustering},
  pages = {1079--1089},
  posted-at = {2006-05-15 16:30:17},
  priority = {2},
  title = {{Coupled two-way clustering analysis of breast cancer and colon cancer gene expression data}},
  volume = {19},
  year = {2003}
}

@article{Tanay02,
  author = {Tanay, A. and Sharan, R. and Shamir, R.},
  citeulike-article-id = {635787},
  journal = {Bioinformatics},
  keywords = {biclustering},
  pages = {S136--S144},
  posted-at = {2006-05-15 16:29:26},
  priority = {2},
  title = {{Discovering statistically significant biclusters in gene expression data}},
  volume = {18},
  year = {2002}
}

@article{Lazzeroni02,
  author = {Lazzeroni, L. and Owen, A.},
  citeulike-article-id = {635761},
  journal = {Statistica Sinica},
  keywords = {biclustering},
  pages = {61--86},
  posted-at = {2006-05-15 16:28:43},
  priority = {2},
  title = {{Plaid models for gene expression data}},
  volume = {12},
  year = {2002}
}

@inproceedings{Yang03,
  author = {Yang, J. and Wang, H. and Wang, W. and Yu, P.},
  booktitle = {Proceedings of the Third IEEE Symposium on BioInformatics and Bioengineering (BIBE'03)},
  citeulike-article-id = {635760},
  keywords = {biclustering},
  pages = {1--7},
  posted-at = {2006-05-15 16:27:54},
  priority = {2},
  title = {{Enhanced Biclustering on Expression Data}},
  year = {2003}
}

@article{Turner05,
  author = {Turner, H. and Bailey, T. and Krzanowski, W.},
  citeulike-article-id = {635759},
  journal = {Computational Statistics and Data Analysis},
  keywords = {biclustering},
  number = {2},
  pages = {235--254},
  posted-at = {2006-05-15 16:26:51},
  priority = {2},
  title = {{Improved biclustering of microarray data demonstrated through systematic performance tests}},
  volume = {48},
  year = {2005}
}

@article{Kung05,
  author = {Kung, S. Y. and Mak, Man W. and Tagkopoulos, Iiias},
  citeulike-article-id = {635757},
  journal = {Proceedings of the 2005 IEEE Computational Systems Bioinformatics Conference (CSB'05)},
  keywords = {biclustering},
  posted-at = {2006-05-15 16:26:06},
  priority = {2},
  title = {{Multi-Metric and Multi-Substructure Biclustering Analysis for Gene Expression Data}},
  year = {2005}
}

@article{Hartigan72,
  author = {Hartigan, J. A.},
  citeulike-article-id = {635756},
  journal = {Journal of American Statistical Association},
  keywords = {clustering},
  pages = {123--129},
  posted-at = {2006-05-15 16:25:14},
  priority = {2},
  title = {{Direct clustering of a data matrix}},
  volume = {67(337)},
  year = {1972}
}

@inproceedings{Tewfik05,
  author = {Tewfik, A. H. and Tchagang, A. B.},
  booktitle = {Proceedings of ICASSP 2005},
  citeulike-article-id = {635753},
  keywords = {biclustering},
  pages = {V773--V776},
  posted-at = {2006-05-15 16:24:06},
  priority = {2},
  title = {Biclustering of {DNA} Microarray Data with Early Pruning},
  year = {2005}
}

@article{Altman01,
  author = {Altman, R. B. and Raychaudhuri, S.},
  citeulike-article-id = {635752},
  journal = {Current Opinion in Structural Biology},
  keywords = {biclustering},
  number = {3},
  pages = {340--347},
  posted-at = {2006-05-15 16:23:29},
  priority = {2},
  title = {{Whole-genome expression Analysis: challenges beyond clustering}},
  volume = {11},
  year = {2001}
}

@inproceedings{Nasraoui95,
  address = {Aachen, Germany},
  author = {Nasraoui, O. and Krishnapuram, R.},
  citeulike-article-id = {635750},
  journal = {EUFIT 95: 3rd European congress on intelligent techniques and soft computing},
  keywords = {clustering},
  month = aug,
  pages = {1312--1318},
  posted-at = {2006-05-15 16:19:50},
  priority = {2},
  title = {{Crisp interpretations of fuzzy and possibilistic clustering algorithms}},
  volume = {3},
  year = {1995}
}

@article{Masulli99w,
  author = {Masulli, Francesco and Schenone, Andrea},
  citeulike-article-id = {625054},
  journal = {Artificial Intelligence in Medicine},
  keywords = {clustering},
  number = {2},
  pages = {129--147},
  posted-at = {2006-05-12 13:45:20},
  priority = {3},
  title = {{A fuzzy clustering based segmentation system as support to diagnosis in medical imaging.}},
  volume = {16},
  year = {1999}
}

@article{Runkler99,
  abstract = {{Many clustering models define good clusters as extrema of objective functions. Optimization of these models is often done using an alternating optimization (AO) algorithm driven by necessary conditions for local extrema. We abandon the objective function model in favor of a generalized model called alternating cluster estimation (ACE). ACE uses an alternating iteration architecture, but membership and prototype functions are selected directly by the user. Virtually every clustering model can be realized as an instance of ACE. Out of a large variety of possible instances of non-AO models, we present two examples: 1) an algorithm with a dynamically changing prototype function that extracts representative data and 2) a computationally efficient algorithm with hyperconic membership functions that allows easy extraction of membership functions. We illustrate these non-AO instances on three problems: a) simple clustering of plane data where we show that creating an unmatched ACE algorithm overcomes some problems of fuzzy c-means (FCM-AO) and possibilistic c-means (PCM-AO); b) functional approximation by clustering on a simple artificial data set; and c) functional approximation on a 12 input 1 output real world data set. ACE models work pretty well in all three cases}},
  author = {Runkler, T. A. and Bezdek, J. C.},
  citeulike-article-id = {625051},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=784198},
  journal = {Fuzzy Systems, IEEE Transactions on},
  keywords = {clustering},
  number = {4},
  pages = {377--393},
  posted-at = {2006-05-12 13:42:55},
  priority = {3},
  title = {{Alternating cluster estimation: a new tool for clustering and function approximation}},
  volume = {7},
  year = {1999}
}

@article{Rose90,
  address = {New York, NY, USA},
  author = {Rose, K. and Gurewwitz, E. and Fox, G.},
  citeulike-article-id = {625023},
  journal = {Pattern Recogn. Lett.},
  keywords = {clustering},
  number = {9},
  pages = {589--594},
  posted-at = {2006-05-12 13:28:54},
  priority = {3},
  publisher = {Elsevier Science Inc.},
  title = {{A deterministic annealing approach to clustering}},
  volume = {11},
  year = {1990}
}

@book{Kohonen01,
  address = {Secaucus, NJ, USA},
  author = {Kohonen, Teuvo},
  citeulike-article-id = {625004},
  keywords = {clustering},
  posted-at = {2006-05-12 13:27:08},
  priority = {3},
  publisher = {Springer-Verlag New York, Inc.},
  title = {{Self-Organizing Maps}},
  year = {2001}
}

@article{Burges98,
  address = {Hingham, MA, USA},
  author = {Burges, Christopher J. C.},
  citeulike-article-id = {620518},
  journal = {Data Mining and Knowledge Discovery},
  keywords = {support-vector},
  number = {2},
  pages = {121--167},
  posted-at = {2006-05-09 16:02:01},
  priority = {4},
  publisher = {Kluwer Academic Publishers},
  title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
  volume = {2},
  year = {1998}
}

@article{Lee05,
  address = {Washington, DC, USA},
  author = {Lee, Daewon},
  citeulike-article-id = {620516},
  comment = {Member-Jaewook Lee},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {clustering, support-vector},
  number = {3},
  pages = {461--464},
  posted-at = {2006-05-09 15:58:16},
  priority = {0},
  publisher = {IEEE Computer Society},
  title = {{An Improved Cluster Labeling Method for Support Vector Clustering}},
  volume = {27},
  year = {2005}
}

@inproceedings{BenHur00,
  author = {Hur, Asa B. and Horn, David and Siegelmann, Hava T. and Vapnik, Vladimir},
  booktitle = {NIPS},
  citeulike-article-id = {620508},
  citeulike-linkout-0 = {\#},
  editor = {Todd},
  journal = {Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000, Denver, CO, USA},
  keywords = {clustering, support-vector},
  pages = {367--373},
  posted-at = {2006-05-09 15:48:04},
  priority = {0},
  title = {{A Support Vector Method for Clustering}},
  year = {2000}
}

@inproceedings{Yang02,
  abstract = {{Support vector machines (SVMs) have been widely adopted for classification, regression and novelty detection. Recent studies (A. Ben-Hur et al., 2001) proposed to employ them for cluster analysis too. The basis of this support vector clustering (SVC) is density estimation through SVM training. SVC is a boundary-based clustering method, where the support information is used to construct cluster boundaries. Despite its ability to deal with outliers, to handle high dimensional data and arbitrary boundaries in data space, there are two problems in the process of cluster labelling. The first problem is its low efficiency when the number of free support vectors increases. The other problem is that it sometimes produces false negatives. We propose a robust cluster assignment method that harvests clustering results efficiently. Our method uses proximity graphs to model the proximity structure of the data. We experimentally analyze and illustrate the performance of this new approach.}},
  author = {Yang, Jianhua and Estivill- and Chalup, S. K.},
  booktitle = {Proceedings of the 9th International Conference on Neural Information Processing},
  citeulike-article-id = {620507},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1198191},
  journal = {ICONIP 02},
  keywords = {clustering, support-vector},
  pages = {898--903},
  posted-at = {2006-05-09 15:43:59},
  priority = {2},
  title = {{Support vector clustering through proximity graph modelling}},
  volume = {2},
  year = {2002}
}

@article{Chiang03,
  abstract = {{In this paper, the support vector clustering is extended to an adaptive cell growing model which maps data points to a high dimensional feature space through a desired kernel function. This generalized model is called multiple spheres support vector clustering, which essentially identifies dense regions in the original space by finding their corresponding spheres with minimal radius in the feature space. A multisphere clustering algorithm based on adaptive cluster cell growing method is developed, whereby it is possible to obtain the grade of memberships, as well as cluster prototypes in partition. The effectiveness of the proposed algorithm is demonstrated for the problem of arbitrary cluster shapes and for prototype identification in an actual application to a handwritten digit data set.}},
  author = {Chiang, Jung-Hsien and Hao, Pei-Yi},
  citeulike-article-id = {620505},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/tfuzz.2003.814839},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1220297},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {clustering, support-vector},
  number = {4},
  pages = {518--527},
  posted-at = {2006-05-09 15:43:09},
  priority = {4},
  title = {{A new kernel-based fuzzy clustering approach: support vector clustering with cell growing}},
  volume = {11},
  year = {2003}
}

@article{BenHur01,
  author = {Hur, Asa B. and Horn, David and Siegelmann, Hava T. and Vapnik, Vladimir},
  citeulike-article-id = {620502},
  journal = {Journal of Machine Learning Research},
  keywords = {clustering, support-vector},
  pages = {125--137},
  posted-at = {2006-05-09 15:37:48},
  priority = {0},
  title = {{Support Vector Clustering.}},
  volume = {2},
  year = {2001}
}

@inproceedings{Joshi98,
  author = {Joshi, A. and Krishnapuram, R.},
  booktitle = {Workshop in Data Mining and knowledge Discovery},
  citeulike-article-id = {615246},
  journal = {SIGMOD},
  keywords = {clustering},
  posted-at = {2006-05-05 16:29:20},
  priority = {2},
  title = {{Robust Fuzzy Clustering Methods to Support Web Mining}},
  year = {1998}
}

@book{Bollobas98,
  abstract = {{The time has now come when graph theory should be part of the education of every serious student of mathematics and computer science, both for its own sake and to enhance the appreciation of mathematics as a whole. This book is an in-depth account of graph theory, written with such a student in mind; it reflects the current state of the subject and emphasizes connections with other branches of pure mathematics. The volume grew out of the author's earlier book, Graph Theory -- An Introductory Course, but its length is well over twice that of its predecessor, allowing it to reveal many exciting new developments in the subject. Recognizing that graph theory is one of several courses competing for the attention of a student, the book contains extensive descriptive passages designed to convey the flavor of the subject and to arouse interest. In addition to a modern treatment of the classical areas of graph theory such as coloring, matching, extremal theory, and algebraic graph theory, the book presents a detailed account of newer topics, including Szemer\'{e}di's Regularity Lemma and its use, Shelah's extension of the Hales-Jewett Theorem, the precise nature of the phase transition in a random graph process, the connection between electrical networks and random walks on graphs, and the Tutte polynomial and its cousins in knot theory. In no other branch of mathematics is it as vital to tackle and solve challenging exercises in order to master the subject. To this end, the book contains an unusually large number of well thought-out exercises: over 600 in total. Although some are straightforward, most of them are substantial, and others will stretch even the most able reader.}},
  author = {Bollobas, Bela},
  citeulike-article-id = {150268},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0387984887},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0387984887},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0387984887},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0387984887},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0387984887/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387984887},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0387984887},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0387984887},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0387984887\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0387984887},
  day = {01},
  edition = {Corrected},
  howpublished = {Paperback},
  isbn = {0387984887},
  keywords = {graphs},
  month = jul,
  posted-at = {2006-05-05 11:47:30},
  priority = {2},
  publisher = {Springer},
  title = {{Modern Graph Theory}},
  year = {1998}
}

@book{Chung97,
  abstract = {{Beautifully written and elegantly presented, this book is based on 10 lectures given at the CBMS workshop on spectral graph theory in June 1994 at Fresno State University. Chung's well-written exposition can be likened to a conversation with a good teacher--one who not only gives you the facts, but tells you what is really going on, why it is worth doing, and how it is related to familiar ideas in other areas. The monograph is accessible to the nonexpert who is interested in reading about this evolving area of mathematics.}},
  author = {Chung, Fan R. K.},
  citeulike-article-id = {352523},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0821803158},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0821803158},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0821803158},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0821803158},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0821803158/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0821803158},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0821803158},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0821803158},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0821803158\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0821803158},
  day = {03},
  howpublished = {Paperback},
  isbn = {0821803158},
  keywords = {graph, spectral},
  month = feb,
  posted-at = {2006-03-10 09:58:10},
  priority = {0},
  publisher = {American Mathematical Society},
  title = {{Spectral Graph Theory (CBMS Regional Conference Series in Mathematics, No. 92)}},
  year = {1996}
}

@inproceedings{Cristianini01b,
  author = {Cristianini, Nello and Taylor, John S. and Elisseeff, Andr{\'e} and Kandola, Jaz S.},
  booktitle = {NIPS},
  citeulike-article-id = {486972},
  keywords = {kernel},
  pages = {367--373},
  posted-at = {2006-01-31 07:08:13},
  priority = {3},
  title = {{On Kernel-Target Alignment.}},
  year = {2001}
}

@techreport{Bengio03b,
  author = {Bengio, Yoshua and Vincent, Pascal and Paiement, Jean F.},
  citeulike-article-id = {486971},
  institution = {CIRANO},
  keywords = {kernel, spectral},
  number = {2003s-19},
  posted-at = {2006-01-31 06:59:51},
  priority = {3},
  title = {Spectral Clustering and Kernel {PCA} are Learning Eigenfunctions},
  year = {2003}
}

@techreport{Dhillon05,
  author = {Dhillon, Inderjit and Guan, Yuqiang and Kulis, Brian},
  citeulike-article-id = {486970},
  institution = {UTCS},
  keywords = {clustering, spectral},
  number = {Technical Report TR-04-25},
  posted-at = {2006-01-31 06:56:19},
  priority = {3},
  title = {{A Unified View of Kernel k-means, Spectral Clustering and Graph Partitioning}},
  year = {2005}
}

@techreport{Fischer05,
  author = {Fischer, Igor and Poland, Ian},
  citeulike-article-id = {470811},
  institution = {IDSIA},
  keywords = {clustering, spectral},
  number = {IDSIA-03-05},
  posted-at = {2006-01-19 14:56:41},
  priority = {2},
  title = {{Amplifying the Block Matrix Structure for Spectral Clustering}},
  year = {2005}
}

@inproceedings{Satish04,
  author = {Satish, D. S. and Sekhar, C. C.},
  booktitle = {Proceedings of the 2004 14th IEEE Signal Processing Society Workshop},
  citeulike-article-id = {470776},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1422989},
  journal = {Machine Learning for Signal Processing, 2004.},
  keywords = {clustering, kernel},
  pages = {315--324},
  posted-at = {2006-01-19 13:51:08},
  priority = {2},
  title = {{Kernel based clustering and vector quantization for speech recognition}},
  year = {2004}
}

@inproceedings{Zhang03,
  author = {Zhang, Dao Q. and Chen, Song C.},
  booktitle = {Proceedings of the International Conference Artificial Neural Network},
  citeulike-article-id = {470762},
  keywords = {clustering, kernel},
  pages = {122--125},
  posted-at = {2006-01-19 13:27:59},
  priority = {0},
  publisher = {Turkey},
  title = {{Kernel based fuzzy and possibilistic c-means clustering}},
  year = {2003}
}

@article{Donath73,
  author = {Donath, W. E. and Hoffman, A. J.},
  citeulike-article-id = {470452},
  journal = {IBM Journal of Research and Development},
  keywords = {graphs},
  pages = {420--425},
  posted-at = {2006-01-19 10:26:23},
  priority = {2},
  title = {{Lower bounds for the partitioning of graphs}},
  volume = {17},
  year = {1973}
}

@techreport{Fischer04,
  author = {Fischer, Igor and Poland, Ian},
  citeulike-article-id = {470442},
  institution = {IDSIA},
  keywords = {clustering, spectral},
  number = {IDSIA-12-04},
  posted-at = {2006-01-19 10:17:27},
  priority = {2},
  title = {{New Methods for Spectral Clustering}},
  year = {2004}
}

@article{Rahimi04,
  author = {Rahimi, Ali and Recht, Ben},
  citeulike-article-id = {470439},
  citeulike-linkout-0 = {http://people.csail.mit.edu/rahimi/Research.html},
  journal = {Statistical Learning in Computer Vision},
  keywords = {clustering, spectral},
  posted-at = {2006-01-19 10:00:02},
  priority = {2},
  title = {{Clustering with Normalized Cuts is Clustering with a Hyperplane}},
  year = {2004}
}

@inproceedings{Cristianini01,
  author = {Cristianini, Nello and Taylor, John S. and Kandola, Jaz S.},
  booktitle = {NIPS},
  citeulike-article-id = {470431},
  keywords = {clustering, spectral},
  pages = {649--655},
  posted-at = {2006-01-19 09:30:08},
  priority = {2},
  title = {{Spectral Kernel Methods for Clustering.}},
  year = {2001}
}

@techreport{Spielman96,
  author = {Spielman, Daniel A. and Teng, Shang H.},
  citeulike-article-id = {470417},
  institution = {EECS Department, University of California, Berkeley},
  keywords = {clustering, spectral},
  number = {UCB/CSD-96-898},
  posted-at = {2006-01-19 09:21:56},
  priority = {2},
  title = {{Spectral Partitioning Works: Planar Graphs and Finite Element Meshes}},
  year = {1996}
}

@article{Krishnapuram96,
  abstract = {{Recently, the possibilistic C-means algorithm (PCM) was proposed to address the drawbacks associated with the constrained memberships used in algorithms such as the fuzzy C-means (FCM). In this issue, Barni et al. (1996) report a difficulty they faced while applying the PCM, and note that it exhibits an undesirable tendency to converge to coincidental clusters. The purpose of this paper is not just to address the issues raised by Barni et al., but to go further and analytically examines the underlying principles of the PCM and the possibilistic approach, in general. We analyze the data sets used by Barni et al. and interpret the results reported by them in the light of our findings}},
  author = {Krishnapuram, R. and Keller, J. M.},
  citeulike-article-id = {470383},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=531779},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {clustering},
  number = {3},
  pages = {385--393},
  posted-at = {2006-01-19 08:53:56},
  priority = {0},
  title = {{The possibilistic C-means algorithm: insights and recommendations}},
  volume = {4},
  year = {1996}
}

@book{Golub96,
  abstract = {{<P>Revised and updated, the third edition of Golub and Van Loan's classic text in computer science provides essential information about the mathematical background and algorithmic skills required for the production of numerical software. This new edition includes thoroughly revised chapters on matrix multiplication problems and parallel matrix computations, expanded treatment of CS decomposition, an updated overview of floating point arithmetic, a more accurate rendition of the modified Gram-Schmidt process, and new material devoted to GMRES, QMR, and other methods designed to handle the sparse unsymmetric linear system problem.</P>}},
  author = {Golub, Gene H. and Van Loan, Charles F.},
  citeulike-article-id = {252315},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0801854148},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0801854148},
  citeulike-linkout-10 = {http://www.worldcat.org/oclc/34515797},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0801854148},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0801854148},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0801854148/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0801854148},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0801854148},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0801854148},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0801854148\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0801854148},
  day = {15},
  edition = {3rd},
  howpublished = {Paperback},
  isbn = {080185413},
  keywords = {algebra},
  month = oct,
  posted-at = {2006-01-19 08:30:30},
  priority = {2},
  publisher = {The Johns Hopkins University Press},
  title = {{Matrix computations}},
  year = {1996}
}

@article{Xu05survey,
  abstract = {{Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.}},
  author = {Xu, Rui and Wunsch, Donald I. I.},
  booktitle = {Neural Networks, IEEE Transactions on},
  citeulike-article-id = {469342},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/tnn.2005.845141},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1427769},
  institution = {Dept. of Electr. \& Comput. Eng., Univ. of Missouri-Rolla, Rolla, MO, USA},
  issn = {1045-9227},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {clustering},
  month = may,
  number = {3},
  pages = {645--678},
  posted-at = {2006-01-18 15:26:05},
  priority = {4},
  publisher = {IEEE},
  title = {{Survey of clustering algorithms}},
  volume = {16},
  year = {2005}
}

@inproceedings{Smyth98,
  address = {Cambridge, MA, USA},
  author = {Smyth, Padhraic and Wolpert, David},
  booktitle = {NIPS '97: Proceedings of the 1997 conference on Advances in neural information processing systems 10},
  citeulike-article-id = {469340},
  keywords = {clustering},
  pages = {668--674},
  posted-at = {2006-01-18 15:22:51},
  priority = {2},
  publisher = {MIT Press},
  title = {{Stacked density estimation}},
  year = {1998}
}

@techreport{Luxburg04,
  author = {von Luxburg, U. and Belkin, M. and Bousquet, O.},
  citeulike-article-id = {469337},
  comment = {(private-note)Submitted.},
  institution = {Max Planck Institute for Biological Cybernetics},
  keywords = {clustering, spectral},
  number = {134},
  posted-at = {2006-01-18 15:19:56},
  priority = {2},
  title = {{Consistency of Spectral Clustering}},
  year = {2004}
}

@inproceedings{Brand03,
  author = {Brand, M. and Huang, K.},
  booktitle = {Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics},
  citeulike-article-id = {469335},
  citeulike-linkout-0 = {\#},
  editor = {Bishop, Christopher M. and Frey, Brendan J.},
  keywords = {clustering, spectral},
  posted-at = {2006-01-18 15:17:43},
  priority = {2},
  title = {{A unifying theorem for spectral embedding and clustering}},
  year = {2003}
}

@incollection{Luxburg05,
  address = {Cambridge, MA},
  author = {von Luxburg, U. and Bousquet, O. and Belkin, M.},
  booktitle = {Advances in Neural Information Processing Systems (NIPS) 17},
  citeulike-article-id = {469332},
  editor = {Saul, Lawrence K. and Weiss, Yair and Bottou, L\'{e}on},
  keywords = {clustering, spectral},
  posted-at = {2006-01-18 15:13:46},
  priority = {2},
  publisher = {MIT Press},
  title = {{Limits of Spectral Clustering}},
  year = {2005}
}

@article{Chen04,
  abstract = {{Fuzzy c-means clustering (FCM) with spatial constraints (FCM/spl I.bar/S) is an effective algorithm suitable for image segmentation. Its effectiveness contributes not only to the introduction of fuzziness for belongingness of each pixel but also to exploitation of spatial contextual information. Although the contextual information can raise its insensitivity to noise to some extent, FCM/spl I.bar/S still lacks enough robustness to noise and outliers and is not suitable for revealing non-Euclidean structure of the input data due to the use of Euclidean distance (L/sub 2/ norm). In this paper, to overcome the above problems, we first propose two variants, FCM/spl I.bar/S/sub 1/ and FCM/spl I.bar/S/sub 2/, of FCM/spl I.bar/S to aim at simplifying its computation and then extend them, including FCM/spl I.bar/S, to corresponding robust kernelized versions KFCM/spl I.bar/S, KFCM/spl I.bar/S/sub 1/ and KFCM/spl I.bar/S/sub 2/ by the kernel methods. Our main motives of using the kernel methods consist in: inducing a class of robust non-Euclidean distance measures for the original data space to derive new objective functions and thus clustering the non-Euclidean structures in data; enhancing robustness of the original clustering algorithms to noise and outliers, and still retaining computational simplicity. The experiments on the artificial and real-world datasets show that our proposed algorithms, especially with spatial constraints, are more effective.}},
  author = {Chen, Songcan and Zhang, Daoqiang},
  citeulike-article-id = {469328},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1315771},
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B},
  keywords = {clustering, kernel},
  number = {4},
  pages = {1907--1916},
  posted-at = {2006-01-18 15:05:16},
  priority = {0},
  title = {Robust image segmentation using {FCM} with spatial constraints based on new kernel-induced distance measure},
  volume = {34},
  year = {2004}
}

@article{Krishnapuram93,
  abstract = {{The clustering problem is cast in the framework of possibility
theory. The approach differs from the existing clustering methods in
that the resulting partition of the data can be interpreted as a
possibilistic partition, and the membership values can be interpreted as
degrees of possibility of the points belonging to the classes, i.e., the
compatibilities of the points with the class prototypes. An appropriate
objective function whose minimum will characterize a good possibilistic
partition of the data is constructed, and the membership and prototype
update equations are derived from necessary conditions for minimization
of the criterion function. The advantages of the resulting family of
possibilistic algorithms are illustrated by several examples}},
  author = {Krishnapuram, R. and Keller, J. M.},
  citeulike-article-id = {469326},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/91.227387},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=227387},
  issn = {10636706},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {clustering},
  month = may,
  number = {2},
  pages = {98--110},
  posted-at = {2006-01-18 14:57:53},
  priority = {0},
  publisher = {IEEE},
  title = {{A possibilistic approach to clustering}},
  volume = {1},
  year = {1993}
}

@inproceedings{Dhillon04,
  abstract = {{Kernel k-means and spectral clustering have both been used to identify clusters that are non-linearly separable in input space. Despite significant research, these methods have remained only loosely related. In this paper, we give an explicit theoretical connection between them. We show the generality of the weighted kernel k-means objective function, and derive the spectral clustering objective of normalized cut as a special case. Given a positive definite similarity matrix, our results lead to a novel weighted kernel k-means algorithm that monotonically decreases the normalized cut. This has important implications: a) eigenvector-based algorithms, which can be computationally prohibitive, are not essential for minimizing normalized cuts, b) various techniques, such as local search and acceleration schemes, may be used to improve the quality as well as speed of kernel k-means. Finally, we present results on several interesting data sets, including diametrical clustering of large gene-expression matrices and a handwriting recognition data set.}},
  address = {New York, NY, USA},
  author = {Dhillon, Inderjit S. and Guan, Yuqiang and Kulis, Brian},
  booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  citeulike-article-id = {464442},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1014118},
  citeulike-linkout-1 = {http://dx.doi.org/10.1145/1014052.1014118},
  isbn = {1-58113-888-1},
  keywords = {clustering, spectral},
  location = {Seattle, WA, USA},
  pages = {551--556},
  posted-at = {2006-01-13 15:02:54},
  priority = {3},
  publisher = {ACM},
  series = {KDD '04},
  title = {{Kernel K-means: Spectral Clustering and Normalized Cuts}},
  year = {2004}
}

@article{Horn01,
  author = {Horn, D.},
  citeulike-article-id = {464440},
  journal = {Physica A Statistical Mechanics and its Applications},
  keywords = {clustering, kernel},
  month = dec,
  pages = {70--79},
  posted-at = {2006-01-13 15:00:15},
  priority = {3},
  title = {{Clustering via Hilbert space}},
  volume = {302},
  year = {2001}
}

@article{Friess99,
  author = {Frie{\ss}, Thilo T. and Harrison, Robert F.},
  citeulike-article-id = {464438},
  journal = {Intell. Data Anal.},
  keywords = {kernel},
  number = {4},
  pages = {307--313},
  posted-at = {2006-01-13 14:56:41},
  priority = {3},
  title = {{A kernel-based Adaline for function approximation.}},
  volume = {3},
  year = {1999}
}

@article{Tsuda03,
  address = {Cambridge, MA, USA},
  author = {Tsuda, Koji and Akaho, Shotaro and Asai, Kiyoshi},
  citeulike-article-id = {464437},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=945365.945369},
  issn = {1533-7928},
  journal = {J. Mach. Learn. Res.},
  keywords = {kernel},
  pages = {67--81},
  posted-at = {2006-01-13 14:54:03},
  priority = {3},
  publisher = {MIT Press},
  title = {{The em algorithm for kernel matrix completion with auxiliary data}},
  volume = {4},
  year = {2003}
}

@article{Have06,
  author = {Have, A. S. and Girolami, M. A. and Larsen, J.},
  citeulike-article-id = {464435},
  journal = {{IEEE} Transactions on Neural Networks},
  keywords = {clustering, kernel},
  posted-at = {2006-01-13 14:51:19},
  priority = {3},
  title = {{Clustering via Kernel Decomposition}},
  year = {2006}
}

@inproceedings{Zha01,
  author = {Zha, Hongyuan and He, Xiaofeng and Ding, Chris H. Q. and Gu, Ming and Simon, Horst D.},
  booktitle = {NIPS},
  citeulike-article-id = {464433},
  keywords = {clustering, spectral},
  pages = {1057--1064},
  posted-at = {2006-01-13 14:46:20},
  priority = {3},
  title = {{Spectral Relaxation for K-means Clustering.}},
  year = {2001}
}

@inproceedings{Cutrona02,
  address = {Annecy, France},
  author = {Cutrona, J. and Bonnet, N. and Herbin, M.},
  booktitle = {Information Processing and Management of Uncertainty},
  citeulike-article-id = {464430},
  citeulike-linkout-0 = {http://crestic.univ-reims.fr/php/pdf.php?id=25},
  keywords = {clustering},
  month = jul,
  pages = {225--232},
  posted-at = {2006-01-13 14:43:08},
  priority = {3},
  title = {{A new fuzzy clustering technique based on pdf estimation}},
  year = {2002}
}

@incollection{Bengio03,
  address = {Cambridge, MA},
  author = {Bengio, Yoshua and Paiement, {jean-Fran\c{c}ois} and Vincent, Pascal and Delalleau, Olivier and Le Roux, Nicolas and Ouimet, Marie},
  booktitle = {Advances in Neural Information Processing Systems 16},
  citeulike-article-id = {464428},
  editor = {Thrun, Sebastian and Saul, Lawrence and {sch\"{o}lkopf}, Bernhard},
  keywords = {clustering, spectral},
  posted-at = {2006-01-13 14:39:07},
  priority = {3},
  publisher = {MIT Press},
  title = {Out-of-Sample Extensions for {LLE}, Isomap, {MDS}, Eigenmaps, and Spectral Clustering},
  year = {2004}
}

@inproceedings{Zhang02,
  author = {Zhang, Dao Q. and Chen, Song C.},
  booktitle = {The 2002 International Conference on Control and Automation, 2002. ICCA},
  citeulike-article-id = {464413},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1229535},
  journal = {Control and Automation, 2002. ICCA. Final Program and Book of Abstracts. The 2002 International Conference on},
  keywords = {clustering, kernel},
  pages = {162--163},
  posted-at = {2006-01-13 14:03:06},
  priority = {0},
  title = {{Fuzzy clustering using kernel method}},
  year = {2002}
}

@techreport{Verma05,
  author = {Verma, Deepak and Meila, Marina},
  citeulike-article-id = {464411},
  citeulike-linkout-0 = {\#},
  institution = {Department of CSE University of Washington Seattle, WA 98195-2350},
  keywords = {clustering, spectral},
  posted-at = {2006-01-13 13:54:29},
  priority = {3},
  title = {{A Comparison of Spectral Clustering Algorithms}},
  year = {2005}
}

@inproceedings{Srivastava04,
  author = {Srivastava, Ashok N.},
  booktitle = {SDM},
  citeulike-article-id = {464406},
  keywords = {kernel},
  posted-at = {2006-01-13 13:36:42},
  priority = {3},
  title = {Mixture Density {M}ercer Kernels: A Method to Learn Kernels Directly from Data.},
  year = {2004}
}

@inproceedings{Graepel98,
  author = {Graepel, T. and Obermayer, K.},
  booktitle = {Proceedings of the 5th GI Workshop Fuzzy Neuro Systems '98},
  citeulike-article-id = {464405},
  editor = {Brauer, W.},
  keywords = {clustering},
  pages = {90--97},
  posted-at = {2006-01-13 13:33:52},
  priority = {3},
  title = {{Fuzzy Topographic Kernel Clustering}},
  year = {1998}
}

@incollection{Xu05,
  address = {Cambridge, MA},
  author = {Xu, Linli and Neufeld, James and Larson, Bryce and Schuurmans, Dale},
  booktitle = {Advances in Neural Information Processing Systems 17},
  citeulike-article-id = {464404},
  editor = {Saul, Lawrence K. and Weiss, Yair and Bottou, {l\'{e}on}},
  keywords = {clustering},
  pages = {1537--1544},
  posted-at = {2006-01-13 13:30:31},
  priority = {4},
  publisher = {MIT Press},
  title = {{Maximum Margin Clustering}},
  year = {2005}
}

@inproceedings{kannan00,
  author = {Kannan, Ravi and Vempala, Santosh and Vetta, Adrian},
  booktitle = {Proceedings of the 41st Annual Symposium on the Foundation of Computer Science},
  citeulike-article-id = {464303},
  keywords = {clustering, spectral},
  month = nov,
  pages = {367--380},
  posted-at = {2006-01-13 09:39:47},
  priority = {3},
  publisher = {IEEE Computer Society},
  title = {{On Clusterings: Good, Bad, and Spectral}},
  year = {2000}
}

@inproceedings{Meila00,
  author = {Meila, Marina and Shi, Jianbo},
  booktitle = {NIPS},
  citeulike-article-id = {464251},
  keywords = {clustering, spectral},
  pages = {873--879},
  posted-at = {2006-01-13 09:34:23},
  priority = {3},
  title = {{Learning Segmentation by Random Walks.}},
  year = {2000}
}

@inproceedings{Ng02,
  address = {Cambridge, MA},
  author = {Ng, A. Y. and Jordan, M. I. and Weiss, Y.},
  booktitle = {Advances in Neural Information Processing Systems 14},
  citeulike-article-id = {464238},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  keywords = {clustering, spectral},
  posted-at = {2006-01-13 09:26:46},
  priority = {0},
  publisher = {MIT Press},
  title = {{On Spectral Clustering: Analysis and an algorithm}},
  year = {2002}
}

@article{Golub99,
  author = {Golub, Todd R. and Slonim, Donna K. and Tamayo, Pablo and Huard, Christine and Gaasenbeek, Michelle and Mesirov, Jill P. and Coller, Hilary and Loh, Mignon and Downing, James R. and Caligiuri, Mark A. and Bloomfield, Clara D. and Lander, Eric S.},
  citeulike-article-id = {464208},
  journal = {Science},
  keywords = {clustering},
  pages = {531--537},
  posted-at = {2006-01-13 08:55:19},
  priority = {0},
  title = {{Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring}},
  volume = {286},
  year = {1999}
}

@article{laserExp,
  author = {H{\"u}bner, U. and Abraham, N. B. and Weiss, C. O.},
  citeulike-article-id = {464207},
  journal = {Phisical Review A},
  keywords = {timeseries},
  number = {11},
  pages = {6354--6365},
  posted-at = {2006-01-13 08:48:12},
  priority = {0},
  title = {Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared {NH}\$\_3\$ laser},
  volume = {40},
  year = {1989}
}

@article{laser2,
  author = {Weiss, C. O. and Brock, J.},
  citeulike-article-id = {464206},
  journal = {Phisical Review Letters},
  keywords = {timeseries},
  number = {22},
  pages = {2804--2806},
  posted-at = {2006-01-13 08:48:12},
  priority = {0},
  title = {Evidence for {Lorenz}-Type Chaos in a Laser},
  volume = {57},
  year = {1986}
}

@article{laser1,
  author = {Hogenboom, E. H. M. and Klische, W. and Weiss, C. O. and Godone, A.},
  citeulike-article-id = {464205},
  journal = {Phisical Review Letters},
  keywords = {timeseries},
  number = {23},
  pages = {2571--2574},
  posted-at = {2006-01-13 08:48:12},
  priority = {0},
  title = {{Instabilities of a Homogeneously Broadened Laser}},
  volume = {55},
  year = {1985}
}

@proceedings{Santafe,
  booktitle = {Time Series Prediction: Forecasting the Future and Understanding the Past},
  citeulike-article-id = {464204},
  editor = {Weigend, A. S. and Gershenfeld, N. A.},
  keywords = {timeseries},
  posted-at = {2006-01-13 08:48:12},
  priority = {0},
  publisher = {Addison Wesley Publishing Company},
  series = {Proceedings Volume, Santa Fe Institute Studies in the Sciences of Complexity},
  title = {{Proceedings of the NATO Advanced Research Workshop on Comparative Time Serie Analysis held in Santa Fe, New Messico, May 14-17, 1992}},
  volume = {XV},
  year = {1993}
}

@techreport{Buhlmann01,
  author = {B{\"u}lhmann, P. and Yu, B.},
  citeulike-article-id = {464203},
  institution = {Eidgen{\"o}ssische Technische Hochschule},
  keywords = {ensemble},
  month = aug,
  number = {98},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{Boosting with the \$L\_2\$-Loss: Regression and Classification}},
  year = {2001}
}

@article{DuffyML02,
  author = {Duffy, N. and Helmbold, D.},
  citeulike-article-id = {464202},
  journal = {Machine Learning},
  keywords = {ensemble},
  pages = {153--200},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{Boosting Methods for Regression}},
  volume = {47},
  year = {2002}
}

@article{SchapireML00,
  author = {Schapire, R. E. and Singer, Y.},
  citeulike-article-id = {464201},
  journal = {Machine Learning},
  keywords = {ensemble},
  pages = {135--168},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{BoosTexter: A Boosting-based System for Text Categorization}},
  volume = {39},
  year = {2000}
}

@inproceedings{SchapireMSRI02,
  author = {Schapire, R. E.},
  booktitle = {MSRI Workshop on Nonlinear Estimation and Classification},
  citeulike-article-id = {464200},
  keywords = {ensemble},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{The Boosting Approach to Machine Learning: An Overview}},
  year = {2002}
}

@unpublished{valentiniPhd,
  author = {Dietterich},
  citeulike-article-id = {464199},
  keywords = {ensemble},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {Support Vector Machines for the development of {SVM}-based ensemble methods}
}

@article{freundPrize,
  author = {Freund, Y. and Schapire, R. E.},
  citeulike-article-id = {464198},
  journal = {Journal of Computer and System Sciences},
  keywords = {ensemble},
  pages = {119--139},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting}},
  volume = {55},
  year = {1997}
}

@article{AllweinJMLR00,
  author = {Allwein, E. L. and Schapire, R. E. and Singer, Y.},
  citeulike-article-id = {464197},
  journal = {Journal of Machine Learning Research},
  keywords = {ensemble},
  pages = {113--141},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{Reducing multiclass to binary: a unifying approach for margin classifiers}},
  volume = {1},
  year = {2000}
}

@inproceedings{FreundICML96,
  author = {Freund, Y. and Schapire, R. E.},
  booktitle = {Proceedings of the 13th International Conference on Machine Learning},
  citeulike-article-id = {464196},
  keywords = {ensemble},
  pages = {148--156},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  publisher = {Morgan Kauffman},
  title = {{Experiments with a new boosting algorithm}},
  year = {1996}
}

@article{BauerML99,
  author = {Kohavi},
  citeulike-article-id = {464195},
  journal = {Machine Learning},
  keywords = {ensemble},
  pages = {105--142},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.}},
  volume = {36},
  year = {1999}
}

@article{DietterichML01,
  author = {Dietterich, T. G.},
  citeulike-article-id = {464194},
  journal = {Machine Learning},
  keywords = {ensemble},
  number = {2},
  pages = {139--158},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{An experimental comparison of three methods for constructing ensembles of decision tress: Bagging, Boosting and Randomization}},
  volume = {40},
  year = {2001}
}

@inproceedings{ValentiniWirn02,
  address = {Heidelberg (Germany)},
  author = {Valentini, G. and Masulli, F.},
  booktitle = {Neural Nets Wirn Vietri-02},
  citeulike-article-id = {464193},
  editor = {Tagliaferri, R. and Marinaro, M.},
  keywords = {ensemble},
  pages = {3--19},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  publisher = {Series {\it Lecture Notes in Computer Sciences}, Springer-Verlag},
  title = {{Ensembles of Learning Machines}},
  volume = {LNCS 2486},
  year = {2002}
}

@inproceedings{DietterichMCS00,
  author = {Dietterich, T. G.},
  booktitle = {Multiple Classifier Systems. First International Workshop, MCS2000, Cagliari, Italy},
  citeulike-article-id = {464192},
  keywords = {ensemble},
  pages = {1--15},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  publisher = {Springer-Verlag},
  title = {{Ensemble Methods in Machine Learning}},
  year = {2000}
}

@article{Breiman96,
  author = {Breiman, L.},
  citeulike-article-id = {464191},
  journal = {Machine Learning},
  keywords = {ensemble},
  pages = {123--140},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{Bagging Predictors}},
  volume = {24},
  year = {1996}
}

@inproceedings{Alpaydin03,
  author = {Barat\c{c}ou\v{g}lu, Z. and Alpaydin, E.},
  booktitle = {ICANN},
  citeulike-article-id = {464190},
  editor = {Kaynak, Okyay and Alpaydin, Ethem and Oja, Erkki and Xu, Lei},
  keywords = {ensemble},
  pages = {76--83},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {{A Comparison of Model Aggregation Methods for Regression}},
  volume = {2714},
  year = {2003}
}

@article{Avnimelech99,
  author = {Avnimelech, R. and Intrator, N.},
  citeulike-article-id = {464189},
  journal = {Neural Computation},
  keywords = {ensemble},
  pages = {491--513},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  title = {{Boosting Regression Estimators}},
  volume = {11},
  year = {1999}
}

@inproceedings{Drucker97,
  author = {Drucker, H.},
  booktitle = {ICML},
  citeulike-article-id = {464188},
  editor = {Fisher, Douglas H.},
  keywords = {ensemble},
  pages = {107--115},
  posted-at = {2006-01-13 08:46:13},
  priority = {0},
  publisher = {Morgan Kaufmann},
  title = {{Improving Regressors using Boosting Techniques}},
  year = {1997}
}

@article{Vautard89,
  author = {Vautard, R. and Ghil, M.},
  citeulike-article-id = {464174},
  journal = {Physica D},
  keywords = {timeseries},
  pages = {395--424},
  posted-at = {2006-01-13 08:45:21},
  priority = {0},
  title = {{Singular-spectrum analysis in nonlinear dynamics, with applications to paleoclimatic time series}},
  volume = {35},
  year = {1989}
}

@article{Vautard92,
  author = {Vautard, R. and You, P. and Ghil, M.},
  citeulike-article-id = {464173},
  journal = {Physica D},
  keywords = {timeseries},
  pages = {95--126},
  posted-at = {2006-01-13 08:45:21},
  priority = {0},
  title = {Singular-spectrum analysis: {A} toolkit for short, noisy chaotic signals},
  volume = {58},
  year = {1992}
}

@book{Elsner96,
  address = {New York},
  author = {Elsner, J. B. and Tsonis, A. A.},
  citeulike-article-id = {464172},
  keywords = {timeseries},
  posted-at = {2006-01-13 08:45:20},
  priority = {0},
  publisher = {Plenum},
  title = {{Singular Spectrum Analysis: A New Tool in Time Series Analysis}},
  year = {1996}
}

@inproceedings{Mane81,
  address = {Warwick 1980},
  author = {{n}\'{e}, Ma\ R.},
  booktitle = {Dynamical Systems and Turbulence},
  citeulike-article-id = {464171},
  editor = {Rand, D. A. and Young, L. S.},
  keywords = {dynamical-systems},
  pages = {230--242},
  posted-at = {2006-01-13 08:44:22},
  priority = {0},
  publisher = {Springer-Verlag, Berlin},
  series = {Lecture Notes in Mathematics},
  title = {{On the dimension of the compact invariant sets of certain non-linear maps}},
  volume = {898},
  year = {1981}
}

@inproceedings{Takens81,
  address = {Warwick},
  author = {Takens, F.},
  booktitle = {Dynamical Systems and Turbulence},
  citeulike-article-id = {464170},
  editor = {Rand, D. A. and Young, L. S.},
  keywords = {dynamical-systems},
  pages = {366--381},
  posted-at = {2006-01-13 08:44:22},
  priority = {0},
  publisher = {Springer-Verlag, Berlin},
  series = {Lecture Notes in Mathematics},
  title = {{Detecting Strange Attractors In Turbulence}},
  volume = {898},
  year = {1981}
}

@book{Kecman01,
  author = {Kecman, V.},
  citeulike-article-id = {464169},
  keywords = {machine-learning},
  posted-at = {2006-01-13 08:43:18},
  priority = {0},
  publisher = {MIT Press, Cambridge},
  title = {{Learning and Soft Computing}},
  year = {2001}
}

@book{Cherkassky98,
  address = {New York},
  author = {Cherkassky, V. N. and Mulier, F.},
  citeulike-article-id = {464168},
  keywords = {machine-learning},
  posted-at = {2006-01-13 08:43:18},
  priority = {0},
  publisher = {Wiley \& Sons},
  title = {{Learning from data: Concepts, Theory and Methods}},
  year = {1998}
}

@book{Vapnik95,
  address = {New York, NY, USA},
  author = {Vapnik, Vladimir N.},
  citeulike-article-id = {464167},
  keywords = {machine-learning},
  posted-at = {2006-01-13 08:43:18},
  priority = {0},
  publisher = {Springer-Verlag New York, Inc.},
  title = {{The nature of statistical learning theory}},
  year = {1995}
}

@book{Sutton98,
  address = {Cambridge, MA},
  author = {Sutton, R. S. and Barto, A. G.},
  citeulike-article-id = {464166},
  keywords = {machine-learning},
  posted-at = {2006-01-13 08:43:18},
  priority = {0},
  publisher = {MIT Press},
  title = {{Reinforcement Learning: An Introduction}},
  year = {1998}
}

@book{Bishop96,
  address = {Oxford, UK},
  author = {Bishop, C. M.},
  citeulike-article-id = {464165},
  keywords = {machine-learning},
  posted-at = {2006-01-13 08:43:18},
  priority = {0},
  publisher = {Oxford University Press},
  title = {{Neural networks for pattern recognition}},
  year = {1996}
}

@book{Kravtsov,
  author = {Kravtsov, Y. A.},
  citeulike-article-id = {464164},
  keywords = {dynamical-systems},
  posted-at = {2006-01-13 08:41:17},
  priority = {0},
  publisher = {Springer-Verlag, Berlin Heildeberg},
  title = {{Limits of Predictability}},
  year = {1993}
}

@book{Abarbanel96,
  author = {Abarbanel, H. D. I.},
  citeulike-article-id = {464163},
  keywords = {dynamical-systems},
  posted-at = {2006-01-13 08:41:17},
  priority = {0},
  publisher = {Springer, New York},
  title = {{Analysis of Observed Chaotic Data}},
  year = {1996}
}

@book{Arrowsmith90,
  author = {Arrowsmith, D. K. and Place, C. M.},
  citeulike-article-id = {464162},
  keywords = {dynamical-systems},
  posted-at = {2006-01-13 08:41:17},
  priority = {0},
  publisher = {Cambridge University Press},
  title = {{An introduction to Dynamical Systems}},
  year = {1990}
}

@book{Casti77,
  address = {New York},
  author = {Casti, J. L.},
  citeulike-article-id = {464161},
  keywords = {dynamical-systems},
  posted-at = {2006-01-13 08:41:17},
  priority = {0},
  publisher = {Academic Press},
  title = {{Dynamical Systems and Their Applications: Linear Theory}},
  year = {1977}
}

@article{Rose98,
  author = {Rose, Kenneth},
  citeulike-article-id = {464157},
  journal = {Proceedings of IEEE},
  keywords = {optimization},
  month = nov,
  number = {11},
  pages = {2210--2239},
  posted-at = {2006-01-13 08:39:31},
  priority = {3},
  title = {{Deterministic annealing for clustering, compression, classification, regression, and related optimization problems}},
  volume = {86},
  year = {1998}
}

@article{Scholkopf98,
  address = {Cambridge, MA, USA},
  author = {Sch\"olkopf, B. and Smola, A. J. and M\"uller, K. R.},
  citeulike-article-id = {464156},
  journal = {Neural Computation},
  keywords = {kernel},
  number = {5},
  pages = {1299--1319},
  posted-at = {2006-01-13 08:38:01},
  priority = {0},
  publisher = {MIT Press},
  title = {{Nonlinear component analysis as a kernel eigenvalue problem}},
  volume = {10},
  year = {1998}
}

@article{Camastra05,
  abstract = {{Kernel Methods are algorithms that, by replacing the inner product with an appropriate positive definite function, implicitly perform a nonlinear mapping of the input data into a high-dimensional feature space. In this paper, we present a kernel method for clustering inspired by the classical K-Means algorithm in which each cluster is iteratively refined using a one-class Support Vector Machine. Our method, which can be easily implemented, compares favorably with respect to popular clustering algorithms, like K-Means, Neural Gas, and Self-Organizing Maps, on a synthetic data set and three UCI real data benchmarks (IRIS data, Wisconsin breast cancer database, Spam database).}},
  author = {Camastra, F. and Verri, A.},
  citeulike-article-id = {162182},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1407882},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {clustering, kernel},
  number = {5},
  pages = {801--804},
  posted-at = {2006-01-13 08:36:01},
  priority = {4},
  title = {{A Novel Kernel Method for Clustering}},
  volume = {27},
  year = {2005}
}

@article{Ghil02,
  abstract = {{The analysis of univariate or multivariate time series provides crucial information to describe, understand, and predict climatic variability. The discovery and implementation of a number of novel methods for extracting useful information from time series has recently revitalized this classical field of study. Considerable progress has also been made in interpreting the information so obtained in terms of dynamical systems theory. In this review we describe the connections between time series analysis and nonlinear dynamics, discuss signal-to-noise enhancement, and present some of the novel methods for spectral analysis. The various steps, as well as the advantages and disadvantages of these methods, are illustrated by their application to an important climatic time series, the Southern Oscillation Index. This index captures major features of interannual climate variability and is used extensively in its prediction. Regional and global sea surface temperature data sets are used to illustrate multivariate spectral methods. Open questions and further prospects conclude the review.}},
  author = {Ghil, M. and Allen, M. R. and Dettinger, M. D. and Ide, K. and Kondrashov, D. and Mann, M. E. and Robertson, A. W. and Saunders, A. and Tian, Y. and Varadi, F. and Yiou, P.},
  citeulike-article-id = {464155},
  citeulike-linkout-0 = {http://www.agu.org/pubs/crossref/2002/2000RG000092.shtml},
  citeulike-linkout-1 = {http://dx.doi.org/10.1029/2000rg000092},
  citeulike-linkout-2 = {http://adsabs.harvard.edu/cgi-bin/nph-bib\_query?bibcode=2002RvGeo..40a...3G},
  day = {1},
  issn = {8755-1209},
  journal = {Rev. Geophys.},
  keywords = {timeseries},
  month = feb,
  number = {1},
  pages = {1--41},
  posted-at = {2006-01-13 08:32:07},
  priority = {0},
  title = {ADVANCED SPECTRAL METHODS FOR CLIMATIC TIME SERIES},
  volume = {40},
  year = {2002}
}

@article{kirkpatrick83,
  abstract = {{this article we briefly review the central constructs in combinatorial opti- mization and in statistical mechanics and then develop the similarities between the two fields. We show how the Metropolis algorithm for approximate numerical simulation of the behavior of a manybody system at a finite temperature provides a natural tool for bringing the techniques of statistical mechanics to bear on optimization. We have applied this point of view to a number of problems arising in optimal design of...}},
  author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  citeulike-article-id = {464154},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.4175},
  journal = {Science, Number 4598, 13 May 1983},
  keywords = {optimization},
  pages = {671--680},
  posted-at = {2006-01-13 08:25:06},
  priority = {0},
  title = {{Optimization by Simulated Annealing}},
  volume = {220, 4598},
  year = {1983}
}

@article{Girolami02,
  author = {Girolami, M.},
  citeulike-article-id = {464152},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {kernel},
  number = {3},
  pages = {780--784},
  posted-at = {2006-01-13 08:22:47},
  priority = {0},
  title = {{Mercer Kernel Based Clustering in Feature Space}},
  volume = {13},
  year = {2002}
}

@article{Aronszajn50,
  author = {Aronszajn, N.},
  citeulike-article-id = {464151},
  journal = {Transactions of the American Mathematical Society},
  keywords = {kernel},
  number = {3},
  pages = {337--404},
  posted-at = {2006-01-13 08:22:47},
  priority = {0},
  title = {{Theory of Reproducing Kernels}},
  volume = {68},
  year = {1950}
}

@book{Saitoh88,
  address = {Harlow, England},
  author = {Saitoh, S.},
  citeulike-article-id = {464150},
  keywords = {kernel},
  posted-at = {2006-01-13 08:22:47},
  priority = {0},
  publisher = {Longman Scientific \& Technical},
  title = {{Theory of Reproducing Kernels and its Applications}},
  year = {1988}
}

@article{Zhang04,
  author = {Zhang, Dao Q. and Chen, Song C.},
  citeulike-article-id = {464149},
  journal = {Artificial Intelligence in Medicine},
  keywords = {kernel},
  number = {1},
  pages = {37--50},
  posted-at = {2006-01-13 08:22:47},
  priority = {0},
  title = {{A novel kernelized fuzzy C-means algorithm with application in medical image segmentation.}},
  volume = {32},
  year = {2004}
}

@article{Aizerman64,
  author = {Aizerman, M. and Braverman, E. and Rozonoer, L.},
  citeulike-article-id = {464148},
  journal = {Automation and Remote Control},
  keywords = {kernel},
  pages = {821--837},
  posted-at = {2006-01-13 08:22:47},
  priority = {0},
  title = {{Theoretical foundations of the potential function method in pattern recognition learning.}},
  volume = {25},
  year = {1964}
}

@article{Cortes95,
  author = {Cortes, C. and Vapnik, V.},
  citeulike-article-id = {464147},
  journal = {Machine Learning},
  keywords = {kernel},
  pages = {273--297},
  posted-at = {2006-01-13 08:22:47},
  priority = {0},
  title = {{Support Vector Networks}},
  volume = {20},
  year = {1995}
}

@article{Tax99,
  address = {New York, NY, USA},
  author = {Tax, David M. J. and Duin, Robert P. W.},
  citeulike-article-id = {464146},
  journal = {Pattern Recognition Letters},
  keywords = {kernel},
  number = {11-13},
  pages = {1191--1199},
  posted-at = {2006-01-13 08:22:47},
  priority = {0},
  publisher = {Elsevier Science Inc.},
  title = {{Support vector domain description}},
  volume = {20},
  year = {1999}
}

@article{Martinetz93,
  author = {Martinetz, T. M. and Berkovich, S. G. and Schulten, K. J.},
  citeulike-article-id = {464144},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {clustering},
  number = {4},
  pages = {558--569},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  title = {`{Neural gas}' network for vector quantization and its application to time-series prediction},
  volume = {4},
  year = {1993}
}

@book{Gersho92,
  address = {Boston},
  author = {Gersho, A. and Gray, R. M.},
  citeulike-article-id = {464143},
  keywords = {clustering},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  publisher = {Kluwer},
  title = {{Vector quantization and signal compression}},
  year = {1992}
}

@article{Ward63,
  author = {Ward, J. H.},
  citeulike-article-id = {464142},
  journal = {Journal of the American Statistical Association},
  keywords = {clustering},
  pages = {236--244},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  title = {{Hierarchical grouping to optimize an objective function}},
  volume = {58},
  year = {1963}
}

@book{Sneath73,
  address = {San Francisco},
  author = {Sneath, P. H. A. and Sokal, R. R.},
  citeulike-article-id = {464141},
  keywords = {clustering},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  publisher = {W.H.\~{}Freeman},
  title = {{Numerical Taxonomy: The Principles and Practice of Numerical Classification}},
  year = {1973}
}

@article{Linde80,
  author = {Linde, Y. and Buzo, A. and Gray, R.},
  citeulike-article-id = {464140},
  journal = {IEEE Transactions on Communications},
  keywords = {clustering},
  pages = {84--95},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  title = {{An algorithm for vector quantizer design}},
  volume = {1},
  year = {1980}
}

@article{Jain99,
  author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
  citeulike-article-id = {464139},
  journal = {ACM Computing Surveys},
  keywords = {clustering},
  number = {3},
  pages = {264--323},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  title = {{Data clustering: a review}},
  volume = {31},
  year = {1999}
}

@book{Jain88,
  address = {Upper Saddle River, NJ, USA},
  author = {Jain, A. K. and Dubes, R. C.},
  citeulike-article-id = {464138},
  keywords = {clustering},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  publisher = {Prentice-Hall, Inc.},
  title = {{Algorithms for clustering data}},
  year = {1988}
}

@book{DudaHart73,
  author = {Duda, R. O. and Hart, P. E.},
  citeulike-article-id = {464137},
  keywords = {clustering},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  publisher = {Wiley},
  title = {{Pattern Classification and Scene Analysis}},
  year = {1973}
}

@book{Bezdek81,
  address = {Norwell, MA, USA},
  author = {Bezdek, J. C.},
  citeulike-article-id = {464136},
  keywords = {clustering},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  publisher = {Kluwer Academic Publishers},
  title = {{Pattern Recognition with Fuzzy Objective Function Algorithms}},
  year = {1981}
}

@article{Kohonen82,
  author = {Kohonen, Teuvo},
  citeulike-article-id = {464135},
  journal = {Biological Cybernetics},
  keywords = {clustering},
  number = {1},
  pages = {59--69},
  posted-at = {2006-01-13 08:22:24},
  priority = {0},
  title = {{Self-organized formation of topologically correct feature maps.}},
  volume = {43},
  year = {1982}
}

